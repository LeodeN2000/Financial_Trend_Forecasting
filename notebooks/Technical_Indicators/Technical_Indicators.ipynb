{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b315f3c-ae60-475d-8233-edcbeeaa69ee",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2278adb-9214-417e-a02e-54cdd24a208e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0. Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3af210-5758-4885-ba83-c77aad92cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f12ca-e0f3-4334-88eb-2b820fed1326",
   "metadata": {},
   "source": [
    "### a. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073982d4-2de4-4010-a31c-804b3fe3374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cleaned_merged_data-sample-with-sentiment-v02.csv\")\n",
    "columns = ['Datetime', 'score_int', 'total_tweets', 'share_of_positive', 'share_of_negative']\n",
    "sentimental_data = data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7fc367-be19-48bd-aec4-fe22d5ca9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "sent_df = sentimental_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ee423-24f9-427a-98f4-afe16556e6fa",
   "metadata": {},
   "source": [
    "### c. Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac21e04-02ba-4348-aae2-7c5c3b71cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_formating(formated_df, date_column, open_column, high_column, low_column, adj_close_column, volume_column):\n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame by renaming columns, setting columns to float64,\n",
    "    dropping unnecessary columns, setting the 'date' column to datetime type,\n",
    "    and setting the 'date' column as the index.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - define which columns of df refere to which price data\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Step 1: Rename columns\n",
    "    formated_df = formated_df.rename(columns={\n",
    "        date_column: 'Date',\n",
    "        open_column: 'Open',\n",
    "        high_column: 'High',\n",
    "        low_column: 'Low',\n",
    "        adj_close_column: 'Adj_Close',\n",
    "        volume_column: 'Volume'\n",
    "    })\n",
    "\n",
    "    # Step 2: Set columns to float64\n",
    "    formated_df = formated_df.astype({'Open': 'float32', 'High': 'float32', 'Low': 'float32', 'Adj_Close': 'float32', 'Volume': 'float32'})\n",
    "\n",
    "    # Step 3: Drop all other columns\n",
    "    columns_to_keep = ['Date', 'Open', 'High', 'Low', 'Adj_Close', 'Volume']\n",
    "    formated_df = formated_df[columns_to_keep]\n",
    "\n",
    "    # Step 4: Set 'date' column to datetime type\n",
    "    formated_df['Date'] = pd.to_datetime(formated_df['Date'], format='mixed')\n",
    "\n",
    "    # Step 5: Set 'date' column as the index\n",
    "    formated_df.set_index('Date', inplace=True)\n",
    "\n",
    "    return formated_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89162f6-0524-468a-a469-647985f7175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_df_formating(sent_df, date_column, score_int, total_tweets, share_of_positive, share_of_negative):\n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame by renaming columns, setting columns to float64,\n",
    "    dropping unnecessary columns, setting the 'date' column to datetime type,\n",
    "    and setting the 'date' column as the index.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - define which columns of df refere to which price data\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Step 1: Rename columns\n",
    "    sent_df = sent_df.rename(columns={\n",
    "        date_column: 'Date',\n",
    "        score_int: 'Score',\n",
    "        total_tweets: 'Total',\n",
    "        share_of_positive: 'Positive',\n",
    "        share_of_negative: 'Negative'\n",
    "    })\n",
    "\n",
    "    # Step 2: Set columns to float64\n",
    "    sent_df = sent_df.astype({'Score': 'float32', 'Total': 'float32', 'Positive': 'float32', 'Negative': 'float32'})\n",
    "\n",
    "    # Step 3: Set 'date' column to datetime type\n",
    "    sent_df['Date'] = pd.to_datetime(sent_df['Date'], format='mixed')\n",
    "\n",
    "    # Step 4: Set 'date' column as the index\n",
    "    sent_df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Step 4: Drop Nan rows\n",
    "    sent_df = sent_df.dropna()\n",
    "    \n",
    "    return sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d1a7e0-f04a-4534-9ca7-10ba1eb6d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_df(labeled_df):\n",
    "    # Create a new column 'Label' and initialize with 0 (constant)\n",
    "    labeled_df['Label'] = 0\n",
    "    \n",
    "    # Label -1 (down) where 'Open' is higher than 'Adj Close'\n",
    "    labeled_df.loc[labeled_df['Open'] > labeled_df['Adj_Close'], 'Label'] = 0\n",
    "\n",
    "    # Label +1 (up) where 'Open' is lower than 'Adj Close'\n",
    "    labeled_df.loc[labeled_df['Open'] < labeled_df['Adj_Close'], 'Label'] = 1\n",
    "\n",
    "    return labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7647a47d-8a53-4702-910b-bec1200d3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(df, sent_df):\n",
    "    \n",
    "    # Merge two df on their indexes\n",
    "    merged_df = pd.merge(df, sent_df, left_index=True, right_index=True)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b226a9d7-e889-4e09-b516-95b8e3529598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  \n",
       "Date                                                                     \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  \n",
       "...                        ...    ...    ...    ...       ...       ...  \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  \n",
       "\n",
       "[1260 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formated_df = df_formating(df, 'Datetime', 'Open-TSLA', 'High-TSLA', 'Low-TSLA', 'Adj Close-TSLA', 'Volume-TSLA')\n",
    "formated_sent_df = sent_df_formating(sent_df, 'Datetime', 'score_int', 'total_tweets', 'share_of_positive', 'share_of_negative')\n",
    "labeled_formated_df = labeling_df(formated_df)\n",
    "merged_df = merge_df(labeled_formated_df, formated_sent_df)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776cf68-9808-4b4a-a534-4f61a071d141",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329c167-9809-4772-926e-8a27426e497c",
   "metadata": {},
   "source": [
    "### A. Moving Average (MA(5) & MA(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9b324e-befc-495d-9053-172b782005c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_averages(df, column_name='Adj_Close', window_sizes=[5, 20]):\n",
    "    \"\"\"\n",
    "    Add Moving Averages (MA) columns to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which moving averages are calculated.\n",
    "    - window_sizes (list): List of window sizes for moving averages. Default is [5, 20].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with added MA columns.\n",
    "    \"\"\"\n",
    "    for window_size in window_sizes:\n",
    "        ma_column_name = f'MA_{window_size}'\n",
    "        df[ma_column_name] = df[column_name].rolling(window=window_size).mean()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d005c4-835c-4314-9fd7-fc54d994b337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>MA_5</th>\n",
       "      <th>MA_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>379.360010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.346704</td>\n",
       "      <td>280.185275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.257007</td>\n",
       "      <td>279.713780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>273.297003</td>\n",
       "      <td>279.282774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>269.319000</td>\n",
       "      <td>278.823724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>268.416998</td>\n",
       "      <td>278.349730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  \\\n",
       "Date                                                                      \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000   \n",
       "...                        ...    ...    ...    ...       ...       ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111   \n",
       "\n",
       "                           MA_5       MA_20  \n",
       "Date                                         \n",
       "2021-11-30 15:00:00         NaN         NaN  \n",
       "2021-11-30 16:00:00         NaN         NaN  \n",
       "2021-11-30 17:00:00         NaN         NaN  \n",
       "2021-11-30 18:00:00         NaN         NaN  \n",
       "2021-11-30 19:00:00  379.360010         NaN  \n",
       "...                         ...         ...  \n",
       "2022-09-29 16:00:00  280.346704  280.185275  \n",
       "2022-09-29 17:00:00  277.257007  279.713780  \n",
       "2022-09-29 18:00:00  273.297003  279.282774  \n",
       "2022-09-29 19:00:00  269.319000  278.823724  \n",
       "2022-09-29 20:00:00  268.416998  278.349730  \n",
       "\n",
       "[1260 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving_averages(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603044c-819f-4be7-8a8f-558da114da44",
   "metadata": {},
   "source": [
    "### B. Bollinger Band (BB up & BB down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbbabf05-d41d-471f-b62d-84bab447f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bollinger_bands(df, column_name='Adj_Close', window_size=20, num_std_dev=2):\n",
    "    \"\"\"\n",
    "    Calculate Bollinger Bands for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which Bollinger Bands are calculated.\n",
    "    - window_size (int): Window size for the moving average. Default is 20.\n",
    "    - num_std_dev (int): Number of standard deviations for the upper and lower bands. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with added columns for Bollinger Bands (BB up, BB down).\n",
    "    \"\"\"\n",
    "    # Calculate the rolling mean (middle band)\n",
    "    df['MiddleBand'] = df[column_name].rolling(window=window_size).mean()\n",
    "\n",
    "    # Calculate the rolling standard deviation\n",
    "    df['StdDev'] = df[column_name].rolling(window=window_size).std()\n",
    "\n",
    "    # Calculate Bollinger Bands\n",
    "    df['BB_Up'] = df['MiddleBand'] + num_std_dev * df['StdDev']\n",
    "    df['BB_Down'] = df['MiddleBand'] - num_std_dev * df['StdDev']\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df.drop(['MiddleBand', 'StdDev'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6855a51-f074-45a5-aa0d-403388cdb44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>MA_5</th>\n",
       "      <th>MA_20</th>\n",
       "      <th>BB_Up</th>\n",
       "      <th>BB_Down</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>379.360010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.346704</td>\n",
       "      <td>280.185275</td>\n",
       "      <td>289.308075</td>\n",
       "      <td>271.062476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.257007</td>\n",
       "      <td>279.713780</td>\n",
       "      <td>290.294435</td>\n",
       "      <td>269.133126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>273.297003</td>\n",
       "      <td>279.282774</td>\n",
       "      <td>291.101124</td>\n",
       "      <td>267.464425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>269.319000</td>\n",
       "      <td>278.823724</td>\n",
       "      <td>291.675140</td>\n",
       "      <td>265.972309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>268.416998</td>\n",
       "      <td>278.349730</td>\n",
       "      <td>292.038371</td>\n",
       "      <td>264.661089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  \\\n",
       "Date                                                                      \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000   \n",
       "...                        ...    ...    ...    ...       ...       ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111   \n",
       "\n",
       "                           MA_5       MA_20       BB_Up     BB_Down  \n",
       "Date                                                                 \n",
       "2021-11-30 15:00:00         NaN         NaN         NaN         NaN  \n",
       "2021-11-30 16:00:00         NaN         NaN         NaN         NaN  \n",
       "2021-11-30 17:00:00         NaN         NaN         NaN         NaN  \n",
       "2021-11-30 18:00:00         NaN         NaN         NaN         NaN  \n",
       "2021-11-30 19:00:00  379.360010         NaN         NaN         NaN  \n",
       "...                         ...         ...         ...         ...  \n",
       "2022-09-29 16:00:00  280.346704  280.185275  289.308075  271.062476  \n",
       "2022-09-29 17:00:00  277.257007  279.713780  290.294435  269.133126  \n",
       "2022-09-29 18:00:00  273.297003  279.282774  291.101124  267.464425  \n",
       "2022-09-29 19:00:00  269.319000  278.823724  291.675140  265.972309  \n",
       "2022-09-29 20:00:00  268.416998  278.349730  292.038371  264.661089  \n",
       "\n",
       "[1260 rows x 14 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bollinger_bands(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3e935-62cb-4412-8edf-01dd55ed47c3",
   "metadata": {},
   "source": [
    "### C. Relative Difference in the Percentage of the price (RDP(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d77891ea-ba0c-45ab-84e7-e578b9f8e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdp(df, column_name='Adj_Close'):\n",
    "    \"\"\"\n",
    "    Calculate Relative Difference in the Percentage of the price (RDP(1)) for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which RDP(1) is calculated.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an added column for RDP(1).\n",
    "    \"\"\"\n",
    "    # Calculate RDP(1)\n",
    "    df['RDP_1'] = df[column_name].pct_change() * 100\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e934f74-99b2-43f1-ae30-d5394a03f97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>MA_5</th>\n",
       "      <th>MA_20</th>\n",
       "      <th>BB_Up</th>\n",
       "      <th>BB_Down</th>\n",
       "      <th>RDP_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.723438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.426024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>379.360010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.067448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.346704</td>\n",
       "      <td>280.185275</td>\n",
       "      <td>289.308075</td>\n",
       "      <td>271.062476</td>\n",
       "      <td>-0.881630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.257007</td>\n",
       "      <td>279.713780</td>\n",
       "      <td>290.294435</td>\n",
       "      <td>269.133126</td>\n",
       "      <td>-0.882077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>273.297003</td>\n",
       "      <td>279.282774</td>\n",
       "      <td>291.101124</td>\n",
       "      <td>267.464425</td>\n",
       "      <td>-0.167912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>269.319000</td>\n",
       "      <td>278.823724</td>\n",
       "      <td>291.675140</td>\n",
       "      <td>265.972309</td>\n",
       "      <td>0.119603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>268.416998</td>\n",
       "      <td>278.349730</td>\n",
       "      <td>292.038371</td>\n",
       "      <td>264.661089</td>\n",
       "      <td>0.153065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  \\\n",
       "Date                                                                      \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000   \n",
       "...                        ...    ...    ...    ...       ...       ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111   \n",
       "\n",
       "                           MA_5       MA_20       BB_Up     BB_Down     RDP_1  \n",
       "Date                                                                           \n",
       "2021-11-30 15:00:00         NaN         NaN         NaN         NaN       NaN  \n",
       "2021-11-30 16:00:00         NaN         NaN         NaN         NaN -2.723438  \n",
       "2021-11-30 17:00:00         NaN         NaN         NaN         NaN  0.010169  \n",
       "2021-11-30 18:00:00         NaN         NaN         NaN         NaN -0.426024  \n",
       "2021-11-30 19:00:00  379.360010         NaN         NaN         NaN  1.067448  \n",
       "...                         ...         ...         ...         ...       ...  \n",
       "2022-09-29 16:00:00  280.346704  280.185275  289.308075  271.062476 -0.881630  \n",
       "2022-09-29 17:00:00  277.257007  279.713780  290.294435  269.133126 -0.882077  \n",
       "2022-09-29 18:00:00  273.297003  279.282774  291.101124  267.464425 -0.167912  \n",
       "2022-09-29 19:00:00  269.319000  278.823724  291.675140  265.972309  0.119603  \n",
       "2022-09-29 20:00:00  268.416998  278.349730  292.038371  264.661089  0.153065  \n",
       "\n",
       "[1260 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdp(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534da867-c707-4617-b4e1-149ad39e8429",
   "metadata": {},
   "source": [
    "### D. Bias Ratio (BIAS(6), BIAS(12) & BIAS(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f187853b-4176-4eed-93c1-98b342c226d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(df, column_name='Adj_Close', ma_windows=[6, 12, 24]):\n",
    "    \"\"\"\n",
    "    Calculate Bias Ratios (BIAS) for specified moving average windows for a column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which BIAS is calculated.\n",
    "    - ma_windows (list): List of moving average window sizes. Default is [6, 12, 24].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with added columns for BIAS(6), BIAS(12), and BIAS(24).\n",
    "    \"\"\"\n",
    "    for window_size in ma_windows:\n",
    "        ma_column_name = f'MA_{window_size}'\n",
    "        bias_column_name = f'BIAS_{window_size}'\n",
    "\n",
    "        # Calculate the moving average\n",
    "        df[ma_column_name] = df[column_name].rolling(window=window_size).mean()\n",
    "\n",
    "        # Calculate BIAS\n",
    "        df[bias_column_name] = ((df[column_name] - df[ma_column_name]) / df[ma_column_name]) * 100\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        df.drop(ma_column_name, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22ec06fe-d1b4-4463-ac6f-c1dd84dc4ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>MA_5</th>\n",
       "      <th>MA_20</th>\n",
       "      <th>BB_Up</th>\n",
       "      <th>BB_Down</th>\n",
       "      <th>RDP_1</th>\n",
       "      <th>BIAS_6</th>\n",
       "      <th>BIAS_12</th>\n",
       "      <th>BIAS_24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.723438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.426024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>379.360010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.067448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.346704</td>\n",
       "      <td>280.185275</td>\n",
       "      <td>289.308075</td>\n",
       "      <td>271.062476</td>\n",
       "      <td>-0.881630</td>\n",
       "      <td>-3.733241</td>\n",
       "      <td>-3.866743</td>\n",
       "      <td>-3.200636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.257007</td>\n",
       "      <td>279.713780</td>\n",
       "      <td>290.294435</td>\n",
       "      <td>269.133126</td>\n",
       "      <td>-0.882077</td>\n",
       "      <td>-3.697208</td>\n",
       "      <td>-4.398604</td>\n",
       "      <td>-3.953059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>273.297003</td>\n",
       "      <td>279.282774</td>\n",
       "      <td>291.101124</td>\n",
       "      <td>267.464425</td>\n",
       "      <td>-0.167912</td>\n",
       "      <td>-2.934700</td>\n",
       "      <td>-4.229376</td>\n",
       "      <td>-4.009411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>269.319000</td>\n",
       "      <td>278.823724</td>\n",
       "      <td>291.675140</td>\n",
       "      <td>265.972309</td>\n",
       "      <td>0.119603</td>\n",
       "      <td>-1.660290</td>\n",
       "      <td>-3.713547</td>\n",
       "      <td>-3.794647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>268.416998</td>\n",
       "      <td>278.349730</td>\n",
       "      <td>292.038371</td>\n",
       "      <td>264.661089</td>\n",
       "      <td>0.153065</td>\n",
       "      <td>-0.321697</td>\n",
       "      <td>-3.138500</td>\n",
       "      <td>-3.545635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  \\\n",
       "Date                                                                      \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000   \n",
       "...                        ...    ...    ...    ...       ...       ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111   \n",
       "\n",
       "                           MA_5       MA_20       BB_Up     BB_Down     RDP_1  \\\n",
       "Date                                                                            \n",
       "2021-11-30 15:00:00         NaN         NaN         NaN         NaN       NaN   \n",
       "2021-11-30 16:00:00         NaN         NaN         NaN         NaN -2.723438   \n",
       "2021-11-30 17:00:00         NaN         NaN         NaN         NaN  0.010169   \n",
       "2021-11-30 18:00:00         NaN         NaN         NaN         NaN -0.426024   \n",
       "2021-11-30 19:00:00  379.360010         NaN         NaN         NaN  1.067448   \n",
       "...                         ...         ...         ...         ...       ...   \n",
       "2022-09-29 16:00:00  280.346704  280.185275  289.308075  271.062476 -0.881630   \n",
       "2022-09-29 17:00:00  277.257007  279.713780  290.294435  269.133126 -0.882077   \n",
       "2022-09-29 18:00:00  273.297003  279.282774  291.101124  267.464425 -0.167912   \n",
       "2022-09-29 19:00:00  269.319000  278.823724  291.675140  265.972309  0.119603   \n",
       "2022-09-29 20:00:00  268.416998  278.349730  292.038371  264.661089  0.153065   \n",
       "\n",
       "                       BIAS_6   BIAS_12   BIAS_24  \n",
       "Date                                               \n",
       "2021-11-30 15:00:00       NaN       NaN       NaN  \n",
       "2021-11-30 16:00:00       NaN       NaN       NaN  \n",
       "2021-11-30 17:00:00       NaN       NaN       NaN  \n",
       "2021-11-30 18:00:00       NaN       NaN       NaN  \n",
       "2021-11-30 19:00:00       NaN       NaN       NaN  \n",
       "...                       ...       ...       ...  \n",
       "2022-09-29 16:00:00 -3.733241 -3.866743 -3.200636  \n",
       "2022-09-29 17:00:00 -3.697208 -4.398604 -3.953059  \n",
       "2022-09-29 18:00:00 -2.934700 -4.229376 -4.009411  \n",
       "2022-09-29 19:00:00 -1.660290 -3.713547 -3.794647  \n",
       "2022-09-29 20:00:00 -0.321697 -3.138500 -3.545635  \n",
       "\n",
       "[1260 rows x 18 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463acc7-c88c-4f55-b214-e7035395d938",
   "metadata": {},
   "source": [
    "### E. Relative Strength Index (RSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e26827fd-fa8a-4b4f-9cd6-ebfb8e8266ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsi(df, column_name='Adj_Close', window=14):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which RSI is calculated. Default is 'Close'.\n",
    "    - window (int): Window size for RSI calculation. Default is 14.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an added column for RSI.\n",
    "    \"\"\"\n",
    "    # Calculate daily price changes\n",
    "    df['PriceChange'] = df[column_name].diff()\n",
    "\n",
    "    # Calculate the average gain and average loss over the specified window\n",
    "    df['Gain'] = df['PriceChange'].apply(lambda x: x if x > 0 else 0).rolling(window=window, min_periods=1).mean()\n",
    "    df['Loss'] = -df['PriceChange'].apply(lambda x: x if x < 0 else 0).rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "    # Calculate relative strength (RS)\n",
    "    df['RS'] = df['Gain'] / df['Loss']\n",
    "\n",
    "    # Calculate RSI\n",
    "    df['RSI'] = 100 - (100 / (1 + df['RS']))\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df.drop(['PriceChange', 'Gain', 'Loss', 'RS'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b376bb70-334e-4cca-87f0-a63aa3abc7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>MA_5</th>\n",
       "      <th>MA_20</th>\n",
       "      <th>BB_Up</th>\n",
       "      <th>BB_Down</th>\n",
       "      <th>RDP_1</th>\n",
       "      <th>BIAS_6</th>\n",
       "      <th>BIAS_12</th>\n",
       "      <th>BIAS_24</th>\n",
       "      <th>RSI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.723438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.361775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.426024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>379.360010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.067448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.963853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.346704</td>\n",
       "      <td>280.185275</td>\n",
       "      <td>289.308075</td>\n",
       "      <td>271.062476</td>\n",
       "      <td>-0.881630</td>\n",
       "      <td>-3.733241</td>\n",
       "      <td>-3.866743</td>\n",
       "      <td>-3.200636</td>\n",
       "      <td>43.548223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.257007</td>\n",
       "      <td>279.713780</td>\n",
       "      <td>290.294435</td>\n",
       "      <td>269.133126</td>\n",
       "      <td>-0.882077</td>\n",
       "      <td>-3.697208</td>\n",
       "      <td>-4.398604</td>\n",
       "      <td>-3.953059</td>\n",
       "      <td>27.409629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>273.297003</td>\n",
       "      <td>279.282774</td>\n",
       "      <td>291.101124</td>\n",
       "      <td>267.464425</td>\n",
       "      <td>-0.167912</td>\n",
       "      <td>-2.934700</td>\n",
       "      <td>-4.229376</td>\n",
       "      <td>-4.009411</td>\n",
       "      <td>31.167846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>269.319000</td>\n",
       "      <td>278.823724</td>\n",
       "      <td>291.675140</td>\n",
       "      <td>265.972309</td>\n",
       "      <td>0.119603</td>\n",
       "      <td>-1.660290</td>\n",
       "      <td>-3.713547</td>\n",
       "      <td>-3.794647</td>\n",
       "      <td>32.602642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>268.416998</td>\n",
       "      <td>278.349730</td>\n",
       "      <td>292.038371</td>\n",
       "      <td>264.661089</td>\n",
       "      <td>0.153065</td>\n",
       "      <td>-0.321697</td>\n",
       "      <td>-3.138500</td>\n",
       "      <td>-3.545635</td>\n",
       "      <td>33.471341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  \\\n",
       "Date                                                                      \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000   \n",
       "...                        ...    ...    ...    ...       ...       ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111   \n",
       "\n",
       "                           MA_5       MA_20       BB_Up     BB_Down     RDP_1  \\\n",
       "Date                                                                            \n",
       "2021-11-30 15:00:00         NaN         NaN         NaN         NaN       NaN   \n",
       "2021-11-30 16:00:00         NaN         NaN         NaN         NaN -2.723438   \n",
       "2021-11-30 17:00:00         NaN         NaN         NaN         NaN  0.010169   \n",
       "2021-11-30 18:00:00         NaN         NaN         NaN         NaN -0.426024   \n",
       "2021-11-30 19:00:00  379.360010         NaN         NaN         NaN  1.067448   \n",
       "...                         ...         ...         ...         ...       ...   \n",
       "2022-09-29 16:00:00  280.346704  280.185275  289.308075  271.062476 -0.881630   \n",
       "2022-09-29 17:00:00  277.257007  279.713780  290.294435  269.133126 -0.882077   \n",
       "2022-09-29 18:00:00  273.297003  279.282774  291.101124  267.464425 -0.167912   \n",
       "2022-09-29 19:00:00  269.319000  278.823724  291.675140  265.972309  0.119603   \n",
       "2022-09-29 20:00:00  268.416998  278.349730  292.038371  264.661089  0.153065   \n",
       "\n",
       "                       BIAS_6   BIAS_12   BIAS_24        RSI  \n",
       "Date                                                          \n",
       "2021-11-30 15:00:00       NaN       NaN       NaN        NaN  \n",
       "2021-11-30 16:00:00       NaN       NaN       NaN   0.000000  \n",
       "2021-11-30 17:00:00       NaN       NaN       NaN   0.361775  \n",
       "2021-11-30 18:00:00       NaN       NaN       NaN   0.314141  \n",
       "2021-11-30 19:00:00       NaN       NaN       NaN  24.963853  \n",
       "...                       ...       ...       ...        ...  \n",
       "2022-09-29 16:00:00 -3.733241 -3.866743 -3.200636  43.548223  \n",
       "2022-09-29 17:00:00 -3.697208 -4.398604 -3.953059  27.409629  \n",
       "2022-09-29 18:00:00 -2.934700 -4.229376 -4.009411  31.167846  \n",
       "2022-09-29 19:00:00 -1.660290 -3.713547 -3.794647  32.602642  \n",
       "2022-09-29 20:00:00 -0.321697 -3.138500 -3.545635  33.471341  \n",
       "\n",
       "[1260 rows x 19 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsi(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a5d5be-103e-479d-b0fa-702fcd15fce1",
   "metadata": {},
   "source": [
    "### F. Exponential Moving Average (EMA(12) & EMA(26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4594634-272d-4b7e-a260-2a0de467a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ema(df, column_name='Adj_Close', ema_short=12, ema_long=26):\n",
    "    \"\"\"\n",
    "    Calculate Exponential Moving Averages (EMA) for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which EMA is calculated. Default is 'Close'.\n",
    "    - ema_short (int): Short-term EMA window size. Default is 12.\n",
    "    - ema_long (int): Long-term EMA window size. Default is 26.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with added columns for EMA(12) and EMA(26).\n",
    "    \"\"\"\n",
    "    # Calculate EMA(12)\n",
    "    df['EMA_12'] = df[column_name].ewm(span=ema_short, adjust=False).mean()\n",
    "\n",
    "    # Calculate EMA(26)\n",
    "    df['EMA_26'] = df[column_name].ewm(span=ema_long, adjust=False).mean()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e395dfd-9854-44c7-988d-da4567784e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>MA_20</th>\n",
       "      <th>BB_Up</th>\n",
       "      <th>BB_Down</th>\n",
       "      <th>RDP_1</th>\n",
       "      <th>BIAS_6</th>\n",
       "      <th>BIAS_12</th>\n",
       "      <th>BIAS_24</th>\n",
       "      <th>RSI</th>\n",
       "      <th>EMA_12</th>\n",
       "      <th>EMA_26</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>387.623322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.723438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>385.999218</td>\n",
       "      <td>386.841346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.361775</td>\n",
       "      <td>384.630874</td>\n",
       "      <td>386.120134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.426024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314141</td>\n",
       "      <td>383.225880</td>\n",
       "      <td>385.333340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.067448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.963853</td>\n",
       "      <td>382.653696</td>\n",
       "      <td>384.901735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>280.185275</td>\n",
       "      <td>289.308075</td>\n",
       "      <td>271.062476</td>\n",
       "      <td>-0.881630</td>\n",
       "      <td>-3.733241</td>\n",
       "      <td>-3.866743</td>\n",
       "      <td>-3.200636</td>\n",
       "      <td>43.548223</td>\n",
       "      <td>280.129088</td>\n",
       "      <td>282.584802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>279.713780</td>\n",
       "      <td>290.294435</td>\n",
       "      <td>269.133126</td>\n",
       "      <td>-0.882077</td>\n",
       "      <td>-3.697208</td>\n",
       "      <td>-4.398604</td>\n",
       "      <td>-3.953059</td>\n",
       "      <td>27.409629</td>\n",
       "      <td>278.263074</td>\n",
       "      <td>281.504446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>279.282774</td>\n",
       "      <td>291.101124</td>\n",
       "      <td>267.464425</td>\n",
       "      <td>-0.167912</td>\n",
       "      <td>-2.934700</td>\n",
       "      <td>-4.229376</td>\n",
       "      <td>-4.009411</td>\n",
       "      <td>31.167846</td>\n",
       "      <td>276.614907</td>\n",
       "      <td>280.470783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>278.823724</td>\n",
       "      <td>291.675140</td>\n",
       "      <td>265.972309</td>\n",
       "      <td>0.119603</td>\n",
       "      <td>-1.660290</td>\n",
       "      <td>-3.713547</td>\n",
       "      <td>-3.794647</td>\n",
       "      <td>32.602642</td>\n",
       "      <td>275.269536</td>\n",
       "      <td>279.537391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>278.349730</td>\n",
       "      <td>292.038371</td>\n",
       "      <td>264.661089</td>\n",
       "      <td>0.153065</td>\n",
       "      <td>-0.321697</td>\n",
       "      <td>-3.138500</td>\n",
       "      <td>-3.545635</td>\n",
       "      <td>33.471341</td>\n",
       "      <td>274.194223</td>\n",
       "      <td>278.703510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  ...  \\\n",
       "Date                                                                     ...   \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  ...   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  ...   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  ...   \n",
       "...                        ...    ...    ...    ...       ...       ...  ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  ...   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  ...   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  ...   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  ...   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  ...   \n",
       "\n",
       "                          MA_20       BB_Up     BB_Down     RDP_1    BIAS_6  \\\n",
       "Date                                                                          \n",
       "2021-11-30 15:00:00         NaN         NaN         NaN       NaN       NaN   \n",
       "2021-11-30 16:00:00         NaN         NaN         NaN -2.723438       NaN   \n",
       "2021-11-30 17:00:00         NaN         NaN         NaN  0.010169       NaN   \n",
       "2021-11-30 18:00:00         NaN         NaN         NaN -0.426024       NaN   \n",
       "2021-11-30 19:00:00         NaN         NaN         NaN  1.067448       NaN   \n",
       "...                         ...         ...         ...       ...       ...   \n",
       "2022-09-29 16:00:00  280.185275  289.308075  271.062476 -0.881630 -3.733241   \n",
       "2022-09-29 17:00:00  279.713780  290.294435  269.133126 -0.882077 -3.697208   \n",
       "2022-09-29 18:00:00  279.282774  291.101124  267.464425 -0.167912 -2.934700   \n",
       "2022-09-29 19:00:00  278.823724  291.675140  265.972309  0.119603 -1.660290   \n",
       "2022-09-29 20:00:00  278.349730  292.038371  264.661089  0.153065 -0.321697   \n",
       "\n",
       "                      BIAS_12   BIAS_24        RSI      EMA_12      EMA_26  \n",
       "Date                                                                        \n",
       "2021-11-30 15:00:00       NaN       NaN        NaN  387.623322  387.623322  \n",
       "2021-11-30 16:00:00       NaN       NaN   0.000000  385.999218  386.841346  \n",
       "2021-11-30 17:00:00       NaN       NaN   0.361775  384.630874  386.120134  \n",
       "2021-11-30 18:00:00       NaN       NaN   0.314141  383.225880  385.333340  \n",
       "2021-11-30 19:00:00       NaN       NaN  24.963853  382.653696  384.901735  \n",
       "...                       ...       ...        ...         ...         ...  \n",
       "2022-09-29 16:00:00 -3.866743 -3.200636  43.548223  280.129088  282.584802  \n",
       "2022-09-29 17:00:00 -4.398604 -3.953059  27.409629  278.263074  281.504446  \n",
       "2022-09-29 18:00:00 -4.229376 -4.009411  31.167846  276.614907  280.470783  \n",
       "2022-09-29 19:00:00 -3.713547 -3.794647  32.602642  275.269536  279.537391  \n",
       "2022-09-29 20:00:00 -3.138500 -3.545635  33.471341  274.194223  278.703510  \n",
       "\n",
       "[1260 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ema(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfeb2e-3f58-43da-b170-293c411467a7",
   "metadata": {},
   "source": [
    "### G. Moving Average Convergence/Divergence (MACD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0619166-d66f-4709-bfef-d9d0f103a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macd(df, column_name='Adj_Close', ema_short=12, ema_long=26, signal_period=9):\n",
    "    \"\"\"\n",
    "    Calculate Moving Average Convergence Divergence (MACD) and its signal line for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which MACD is calculated. Default is 'Close'.\n",
    "    - ema_short (int): Short-term EMA window size. Default is 12.\n",
    "    - ema_long (int): Long-term EMA window size. Default is 26.\n",
    "    - signal_period (int): Signal line EMA window size. Default is 9.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with added columns for MACD, Signal Line, and MACD Histogram.\n",
    "    \"\"\"\n",
    "    # Calculate short-term EMA\n",
    "    df['EMA_short'] = df[column_name].ewm(span=ema_short, adjust=False).mean()\n",
    "\n",
    "    # Calculate long-term EMA\n",
    "    df['EMA_long'] = df[column_name].ewm(span=ema_long, adjust=False).mean()\n",
    "\n",
    "    # Calculate MACD Line\n",
    "    df['DIF'] = df['EMA_short'] - df['EMA_long']\n",
    "\n",
    "    # Calculate Signal Line\n",
    "    df['Signal_Line'] = df['DIF'].ewm(span=signal_period, adjust=False).mean()\n",
    "\n",
    "    # Calculate MACD Histogram\n",
    "    df['OSC'] = df['DIF'] - df['Signal_Line']\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df.drop(['EMA_short', 'EMA_long'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2986501a-fc84-4117-b84c-5b5c7c9c7ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>RDP_1</th>\n",
       "      <th>BIAS_6</th>\n",
       "      <th>BIAS_12</th>\n",
       "      <th>BIAS_24</th>\n",
       "      <th>RSI</th>\n",
       "      <th>EMA_12</th>\n",
       "      <th>EMA_26</th>\n",
       "      <th>DIF</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>OSC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.723438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>385.999218</td>\n",
       "      <td>386.841346</td>\n",
       "      <td>-0.842128</td>\n",
       "      <td>-0.168426</td>\n",
       "      <td>-0.673702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.361775</td>\n",
       "      <td>384.630874</td>\n",
       "      <td>386.120134</td>\n",
       "      <td>-1.489260</td>\n",
       "      <td>-0.432592</td>\n",
       "      <td>-1.056667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314141</td>\n",
       "      <td>383.225880</td>\n",
       "      <td>385.333340</td>\n",
       "      <td>-2.107460</td>\n",
       "      <td>-0.767566</td>\n",
       "      <td>-1.339894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.067448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.963853</td>\n",
       "      <td>382.653696</td>\n",
       "      <td>384.901735</td>\n",
       "      <td>-2.248039</td>\n",
       "      <td>-1.063661</td>\n",
       "      <td>-1.184379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.881630</td>\n",
       "      <td>-3.733241</td>\n",
       "      <td>-3.866743</td>\n",
       "      <td>-3.200636</td>\n",
       "      <td>43.548223</td>\n",
       "      <td>280.129088</td>\n",
       "      <td>282.584802</td>\n",
       "      <td>-2.455714</td>\n",
       "      <td>-2.395471</td>\n",
       "      <td>-0.060243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.882077</td>\n",
       "      <td>-3.697208</td>\n",
       "      <td>-4.398604</td>\n",
       "      <td>-3.953059</td>\n",
       "      <td>27.409629</td>\n",
       "      <td>278.263074</td>\n",
       "      <td>281.504446</td>\n",
       "      <td>-3.241372</td>\n",
       "      <td>-2.564651</td>\n",
       "      <td>-0.676721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167912</td>\n",
       "      <td>-2.934700</td>\n",
       "      <td>-4.229376</td>\n",
       "      <td>-4.009411</td>\n",
       "      <td>31.167846</td>\n",
       "      <td>276.614907</td>\n",
       "      <td>280.470783</td>\n",
       "      <td>-3.855876</td>\n",
       "      <td>-2.822896</td>\n",
       "      <td>-1.032979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119603</td>\n",
       "      <td>-1.660290</td>\n",
       "      <td>-3.713547</td>\n",
       "      <td>-3.794647</td>\n",
       "      <td>32.602642</td>\n",
       "      <td>275.269536</td>\n",
       "      <td>279.537391</td>\n",
       "      <td>-4.267855</td>\n",
       "      <td>-3.111888</td>\n",
       "      <td>-1.155967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153065</td>\n",
       "      <td>-0.321697</td>\n",
       "      <td>-3.138500</td>\n",
       "      <td>-3.545635</td>\n",
       "      <td>33.471341</td>\n",
       "      <td>274.194223</td>\n",
       "      <td>278.703510</td>\n",
       "      <td>-4.509287</td>\n",
       "      <td>-3.391368</td>\n",
       "      <td>-1.117920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  ...  \\\n",
       "Date                                                                     ...   \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  ...   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  ...   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  ...   \n",
       "...                        ...    ...    ...    ...       ...       ...  ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  ...   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  ...   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  ...   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  ...   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  ...   \n",
       "\n",
       "                        RDP_1    BIAS_6   BIAS_12   BIAS_24        RSI  \\\n",
       "Date                                                                     \n",
       "2021-11-30 15:00:00       NaN       NaN       NaN       NaN        NaN   \n",
       "2021-11-30 16:00:00 -2.723438       NaN       NaN       NaN   0.000000   \n",
       "2021-11-30 17:00:00  0.010169       NaN       NaN       NaN   0.361775   \n",
       "2021-11-30 18:00:00 -0.426024       NaN       NaN       NaN   0.314141   \n",
       "2021-11-30 19:00:00  1.067448       NaN       NaN       NaN  24.963853   \n",
       "...                       ...       ...       ...       ...        ...   \n",
       "2022-09-29 16:00:00 -0.881630 -3.733241 -3.866743 -3.200636  43.548223   \n",
       "2022-09-29 17:00:00 -0.882077 -3.697208 -4.398604 -3.953059  27.409629   \n",
       "2022-09-29 18:00:00 -0.167912 -2.934700 -4.229376 -4.009411  31.167846   \n",
       "2022-09-29 19:00:00  0.119603 -1.660290 -3.713547 -3.794647  32.602642   \n",
       "2022-09-29 20:00:00  0.153065 -0.321697 -3.138500 -3.545635  33.471341   \n",
       "\n",
       "                         EMA_12      EMA_26       DIF  Signal_Line       OSC  \n",
       "Date                                                                          \n",
       "2021-11-30 15:00:00  387.623322  387.623322  0.000000     0.000000  0.000000  \n",
       "2021-11-30 16:00:00  385.999218  386.841346 -0.842128    -0.168426 -0.673702  \n",
       "2021-11-30 17:00:00  384.630874  386.120134 -1.489260    -0.432592 -1.056667  \n",
       "2021-11-30 18:00:00  383.225880  385.333340 -2.107460    -0.767566 -1.339894  \n",
       "2021-11-30 19:00:00  382.653696  384.901735 -2.248039    -1.063661 -1.184379  \n",
       "...                         ...         ...       ...          ...       ...  \n",
       "2022-09-29 16:00:00  280.129088  282.584802 -2.455714    -2.395471 -0.060243  \n",
       "2022-09-29 17:00:00  278.263074  281.504446 -3.241372    -2.564651 -0.676721  \n",
       "2022-09-29 18:00:00  276.614907  280.470783 -3.855876    -2.822896 -1.032979  \n",
       "2022-09-29 19:00:00  275.269536  279.537391 -4.267855    -3.111888 -1.155967  \n",
       "2022-09-29 20:00:00  274.194223  278.703510 -4.509287    -3.391368 -1.117920  \n",
       "\n",
       "[1260 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macd(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169c3ac-ca21-4a66-b338-35332a7c50fa",
   "metadata": {},
   "source": [
    "### H. Psychological Line (PSY(12) & PSY(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "101c496e-8763-4f17-8f33-272e353c845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psy(df, column_name='Adj_Close', psy_short=12, psy_long=24):\n",
    "    \"\"\"\n",
    "    Calculate Psychological Line (PSY) for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which PSY is calculated. Default is 'Close'.\n",
    "    - psy_short (int): Short-term PSY window size. Default is 12.\n",
    "    - psy_long (int): Long-term PSY window size. Default is 24.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with added columns for PSY(12) and PSY(24).\n",
    "    \"\"\"\n",
    "    # Calculate the percentage of days where the closing price is higher than the previous day's closing price\n",
    "    df['PriceUp'] = df[column_name].diff() > 0\n",
    "\n",
    "    # Calculate PSY(12)\n",
    "    df['PSY_12'] = df['PriceUp'].rolling(window=psy_short).mean() * 100\n",
    "\n",
    "    # Calculate PSY(24)\n",
    "    df['PSY_24'] = df['PriceUp'].rolling(window=psy_long).mean() * 100\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df.drop(['PriceUp'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2419b09-8f9b-497f-b035-6b3cc4bf4b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>BIAS_12</th>\n",
       "      <th>BIAS_24</th>\n",
       "      <th>RSI</th>\n",
       "      <th>EMA_12</th>\n",
       "      <th>EMA_26</th>\n",
       "      <th>DIF</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>OSC</th>\n",
       "      <th>PSY_12</th>\n",
       "      <th>PSY_24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>385.999218</td>\n",
       "      <td>386.841346</td>\n",
       "      <td>-0.842128</td>\n",
       "      <td>-0.168426</td>\n",
       "      <td>-0.673702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.361775</td>\n",
       "      <td>384.630874</td>\n",
       "      <td>386.120134</td>\n",
       "      <td>-1.489260</td>\n",
       "      <td>-0.432592</td>\n",
       "      <td>-1.056667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314141</td>\n",
       "      <td>383.225880</td>\n",
       "      <td>385.333340</td>\n",
       "      <td>-2.107460</td>\n",
       "      <td>-0.767566</td>\n",
       "      <td>-1.339894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.963853</td>\n",
       "      <td>382.653696</td>\n",
       "      <td>384.901735</td>\n",
       "      <td>-2.248039</td>\n",
       "      <td>-1.063661</td>\n",
       "      <td>-1.184379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.866743</td>\n",
       "      <td>-3.200636</td>\n",
       "      <td>43.548223</td>\n",
       "      <td>280.129088</td>\n",
       "      <td>282.584802</td>\n",
       "      <td>-2.455714</td>\n",
       "      <td>-2.395471</td>\n",
       "      <td>-0.060243</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.398604</td>\n",
       "      <td>-3.953059</td>\n",
       "      <td>27.409629</td>\n",
       "      <td>278.263074</td>\n",
       "      <td>281.504446</td>\n",
       "      <td>-3.241372</td>\n",
       "      <td>-2.564651</td>\n",
       "      <td>-0.676721</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.229376</td>\n",
       "      <td>-4.009411</td>\n",
       "      <td>31.167846</td>\n",
       "      <td>276.614907</td>\n",
       "      <td>280.470783</td>\n",
       "      <td>-3.855876</td>\n",
       "      <td>-2.822896</td>\n",
       "      <td>-1.032979</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.713547</td>\n",
       "      <td>-3.794647</td>\n",
       "      <td>32.602642</td>\n",
       "      <td>275.269536</td>\n",
       "      <td>279.537391</td>\n",
       "      <td>-4.267855</td>\n",
       "      <td>-3.111888</td>\n",
       "      <td>-1.155967</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.138500</td>\n",
       "      <td>-3.545635</td>\n",
       "      <td>33.471341</td>\n",
       "      <td>274.194223</td>\n",
       "      <td>278.703510</td>\n",
       "      <td>-4.509287</td>\n",
       "      <td>-3.391368</td>\n",
       "      <td>-1.117920</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  ...  \\\n",
       "Date                                                                     ...   \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  ...   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  ...   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  ...   \n",
       "...                        ...    ...    ...    ...       ...       ...  ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  ...   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  ...   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  ...   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  ...   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  ...   \n",
       "\n",
       "                      BIAS_12   BIAS_24        RSI      EMA_12      EMA_26  \\\n",
       "Date                                                                         \n",
       "2021-11-30 15:00:00       NaN       NaN        NaN  387.623322  387.623322   \n",
       "2021-11-30 16:00:00       NaN       NaN   0.000000  385.999218  386.841346   \n",
       "2021-11-30 17:00:00       NaN       NaN   0.361775  384.630874  386.120134   \n",
       "2021-11-30 18:00:00       NaN       NaN   0.314141  383.225880  385.333340   \n",
       "2021-11-30 19:00:00       NaN       NaN  24.963853  382.653696  384.901735   \n",
       "...                       ...       ...        ...         ...         ...   \n",
       "2022-09-29 16:00:00 -3.866743 -3.200636  43.548223  280.129088  282.584802   \n",
       "2022-09-29 17:00:00 -4.398604 -3.953059  27.409629  278.263074  281.504446   \n",
       "2022-09-29 18:00:00 -4.229376 -4.009411  31.167846  276.614907  280.470783   \n",
       "2022-09-29 19:00:00 -3.713547 -3.794647  32.602642  275.269536  279.537391   \n",
       "2022-09-29 20:00:00 -3.138500 -3.545635  33.471341  274.194223  278.703510   \n",
       "\n",
       "                          DIF  Signal_Line       OSC  PSY_12     PSY_24  \n",
       "Date                                                                     \n",
       "2021-11-30 15:00:00  0.000000     0.000000  0.000000     NaN        NaN  \n",
       "2021-11-30 16:00:00 -0.842128    -0.168426 -0.673702     NaN        NaN  \n",
       "2021-11-30 17:00:00 -1.489260    -0.432592 -1.056667     NaN        NaN  \n",
       "2021-11-30 18:00:00 -2.107460    -0.767566 -1.339894     NaN        NaN  \n",
       "2021-11-30 19:00:00 -2.248039    -1.063661 -1.184379     NaN        NaN  \n",
       "...                       ...          ...       ...     ...        ...  \n",
       "2022-09-29 16:00:00 -2.455714    -2.395471 -0.060243    50.0  54.166667  \n",
       "2022-09-29 17:00:00 -3.241372    -2.564651 -0.676721    50.0  50.000000  \n",
       "2022-09-29 18:00:00 -3.855876    -2.822896 -1.032979    50.0  50.000000  \n",
       "2022-09-29 19:00:00 -4.267855    -3.111888 -1.155967    50.0  54.166667  \n",
       "2022-09-29 20:00:00 -4.509287    -3.391368 -1.117920    50.0  54.166667  \n",
       "\n",
       "[1260 rows x 26 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psy(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd2fc5-4c0c-43d1-bec3-fa6185e1ab74",
   "metadata": {},
   "source": [
    "### I. Williams %R (WMS%R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54569384-663b-47b4-ac6e-822fee40d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def williams_percent_r(df, high_column='High', low_column='Low', adj_close_column='Adj_Close', window=14):\n",
    "    \"\"\"\n",
    "    Calculate Williams %R for a specified high, low, and close columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - high_column (str): Name of the column containing high prices. Default is 'High'.\n",
    "    - low_column (str): Name of the column containing low prices. Default is 'Low'.\n",
    "    - adj_close_column (str): Name of the column containing close prices. Default is 'Close'.\n",
    "    - window (int): Window size for Williams %R calculation. Default is 14.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an added column for Williams %R.\n",
    "    \"\"\"\n",
    "    # Calculate highest high and lowest low over the specified window\n",
    "    df['HH'] = df[high_column].rolling(window=window).max()\n",
    "    df['LL'] = df[low_column].rolling(window=window).min()\n",
    "\n",
    "    # Calculate Williams %R\n",
    "    df['Williams_%R'] = (df['HH'] - df[adj_close_column]) / (df['HH'] - df['LL']) * -100\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df.drop(['HH', 'LL'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c8e90ae-36eb-4525-9408-5a133a1fefcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>BIAS_24</th>\n",
       "      <th>RSI</th>\n",
       "      <th>EMA_12</th>\n",
       "      <th>EMA_26</th>\n",
       "      <th>DIF</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>OSC</th>\n",
       "      <th>PSY_12</th>\n",
       "      <th>PSY_24</th>\n",
       "      <th>Williams_%R</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>385.999218</td>\n",
       "      <td>386.841346</td>\n",
       "      <td>-0.842128</td>\n",
       "      <td>-0.168426</td>\n",
       "      <td>-0.673702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.361775</td>\n",
       "      <td>384.630874</td>\n",
       "      <td>386.120134</td>\n",
       "      <td>-1.489260</td>\n",
       "      <td>-0.432592</td>\n",
       "      <td>-1.056667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314141</td>\n",
       "      <td>383.225880</td>\n",
       "      <td>385.333340</td>\n",
       "      <td>-2.107460</td>\n",
       "      <td>-0.767566</td>\n",
       "      <td>-1.339894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.963853</td>\n",
       "      <td>382.653696</td>\n",
       "      <td>384.901735</td>\n",
       "      <td>-2.248039</td>\n",
       "      <td>-1.063661</td>\n",
       "      <td>-1.184379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.200636</td>\n",
       "      <td>43.548223</td>\n",
       "      <td>280.129088</td>\n",
       "      <td>282.584802</td>\n",
       "      <td>-2.455714</td>\n",
       "      <td>-2.395471</td>\n",
       "      <td>-0.060243</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-93.171910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.953059</td>\n",
       "      <td>27.409629</td>\n",
       "      <td>278.263074</td>\n",
       "      <td>281.504446</td>\n",
       "      <td>-3.241372</td>\n",
       "      <td>-2.564651</td>\n",
       "      <td>-0.676721</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-95.541354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.009411</td>\n",
       "      <td>31.167846</td>\n",
       "      <td>276.614907</td>\n",
       "      <td>280.470783</td>\n",
       "      <td>-3.855876</td>\n",
       "      <td>-2.822896</td>\n",
       "      <td>-1.032979</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-94.243543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.794647</td>\n",
       "      <td>32.602642</td>\n",
       "      <td>275.269536</td>\n",
       "      <td>279.537391</td>\n",
       "      <td>-4.267855</td>\n",
       "      <td>-3.111888</td>\n",
       "      <td>-1.155967</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-90.999155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.545635</td>\n",
       "      <td>33.471341</td>\n",
       "      <td>274.194223</td>\n",
       "      <td>278.703510</td>\n",
       "      <td>-4.509287</td>\n",
       "      <td>-3.391368</td>\n",
       "      <td>-1.117920</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-89.233420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  ...  \\\n",
       "Date                                                                     ...   \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  ...   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  ...   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  ...   \n",
       "...                        ...    ...    ...    ...       ...       ...  ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  ...   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  ...   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  ...   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  ...   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  ...   \n",
       "\n",
       "                      BIAS_24        RSI      EMA_12      EMA_26       DIF  \\\n",
       "Date                                                                         \n",
       "2021-11-30 15:00:00       NaN        NaN  387.623322  387.623322  0.000000   \n",
       "2021-11-30 16:00:00       NaN   0.000000  385.999218  386.841346 -0.842128   \n",
       "2021-11-30 17:00:00       NaN   0.361775  384.630874  386.120134 -1.489260   \n",
       "2021-11-30 18:00:00       NaN   0.314141  383.225880  385.333340 -2.107460   \n",
       "2021-11-30 19:00:00       NaN  24.963853  382.653696  384.901735 -2.248039   \n",
       "...                       ...        ...         ...         ...       ...   \n",
       "2022-09-29 16:00:00 -3.200636  43.548223  280.129088  282.584802 -2.455714   \n",
       "2022-09-29 17:00:00 -3.953059  27.409629  278.263074  281.504446 -3.241372   \n",
       "2022-09-29 18:00:00 -4.009411  31.167846  276.614907  280.470783 -3.855876   \n",
       "2022-09-29 19:00:00 -3.794647  32.602642  275.269536  279.537391 -4.267855   \n",
       "2022-09-29 20:00:00 -3.545635  33.471341  274.194223  278.703510 -4.509287   \n",
       "\n",
       "                     Signal_Line       OSC  PSY_12     PSY_24  Williams_%R  \n",
       "Date                                                                        \n",
       "2021-11-30 15:00:00     0.000000  0.000000     NaN        NaN          NaN  \n",
       "2021-11-30 16:00:00    -0.168426 -0.673702     NaN        NaN          NaN  \n",
       "2021-11-30 17:00:00    -0.432592 -1.056667     NaN        NaN          NaN  \n",
       "2021-11-30 18:00:00    -0.767566 -1.339894     NaN        NaN          NaN  \n",
       "2021-11-30 19:00:00    -1.063661 -1.184379     NaN        NaN          NaN  \n",
       "...                          ...       ...     ...        ...          ...  \n",
       "2022-09-29 16:00:00    -2.395471 -0.060243    50.0  54.166667   -93.171910  \n",
       "2022-09-29 17:00:00    -2.564651 -0.676721    50.0  50.000000   -95.541354  \n",
       "2022-09-29 18:00:00    -2.822896 -1.032979    50.0  50.000000   -94.243543  \n",
       "2022-09-29 19:00:00    -3.111888 -1.155967    50.0  54.166667   -90.999155  \n",
       "2022-09-29 20:00:00    -3.391368 -1.117920    50.0  54.166667   -89.233420  \n",
       "\n",
       "[1260 rows x 27 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "williams_percent_r(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320471bc-2ca7-441c-8f1c-63cb753e42f1",
   "metadata": {},
   "source": [
    "### J. Stochastic Oscillator (Stochastic%K & Stochastic%D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deef3936-e6e2-4ca0-862f-47b1fa153512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_oscillator(df, high_column='High', low_column='Low', adj_close_column='Adj_Close', k_window=14, d_window=3):\n",
    "    \"\"\"\n",
    "    Calculate Stochastic Oscillator (%K and %D) for specified high, low, and close columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - high_column (str): Name of the column containing high prices. Default is 'High'.\n",
    "    - low_column (str): Name of the column containing low prices. Default is 'Low'.\n",
    "    - close_column (str): Name of the column containing close prices. Default is 'Close'.\n",
    "    - k_window (int): Window size for %K calculation. Default is 14.\n",
    "    - d_window (int): Window size for %D calculation. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with added columns for Stochastic %K and %D.\n",
    "    \"\"\"\n",
    "    # Calculate lowest low and highest high over the specified window\n",
    "    df['LL'] = df[low_column].rolling(window=k_window).min()\n",
    "    df['HH'] = df[high_column].rolling(window=k_window).max()\n",
    "\n",
    "    # Calculate Stochastic %K\n",
    "    df['Stochastic_%K'] = ((df[adj_close_column] - df['LL']) / (df['HH'] - df['LL'])) * 100\n",
    "\n",
    "    # Calculate Stochastic %D (3-day simple moving average of %K)\n",
    "    df['Stochastic_%D'] = df['Stochastic_%K'].rolling(window=d_window).mean()\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df.drop(['LL', 'HH'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d4c0014-f461-4304-9ca8-1039a9b215b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>EMA_12</th>\n",
       "      <th>EMA_26</th>\n",
       "      <th>DIF</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>OSC</th>\n",
       "      <th>PSY_12</th>\n",
       "      <th>PSY_24</th>\n",
       "      <th>Williams_%R</th>\n",
       "      <th>Stochastic_%K</th>\n",
       "      <th>Stochastic_%D</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>385.999218</td>\n",
       "      <td>386.841346</td>\n",
       "      <td>-0.842128</td>\n",
       "      <td>-0.168426</td>\n",
       "      <td>-0.673702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>384.630874</td>\n",
       "      <td>386.120134</td>\n",
       "      <td>-1.489260</td>\n",
       "      <td>-0.432592</td>\n",
       "      <td>-1.056667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>383.225880</td>\n",
       "      <td>385.333340</td>\n",
       "      <td>-2.107460</td>\n",
       "      <td>-0.767566</td>\n",
       "      <td>-1.339894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>382.653696</td>\n",
       "      <td>384.901735</td>\n",
       "      <td>-2.248039</td>\n",
       "      <td>-1.063661</td>\n",
       "      <td>-1.184379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>280.129088</td>\n",
       "      <td>282.584802</td>\n",
       "      <td>-2.455714</td>\n",
       "      <td>-2.395471</td>\n",
       "      <td>-0.060243</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-93.171910</td>\n",
       "      <td>6.828090</td>\n",
       "      <td>38.773714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>278.263074</td>\n",
       "      <td>281.504446</td>\n",
       "      <td>-3.241372</td>\n",
       "      <td>-2.564651</td>\n",
       "      <td>-0.676721</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-95.541354</td>\n",
       "      <td>4.458646</td>\n",
       "      <td>10.050782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>276.614907</td>\n",
       "      <td>280.470783</td>\n",
       "      <td>-3.855876</td>\n",
       "      <td>-2.822896</td>\n",
       "      <td>-1.032979</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-94.243543</td>\n",
       "      <td>5.756457</td>\n",
       "      <td>5.681064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>275.269536</td>\n",
       "      <td>279.537391</td>\n",
       "      <td>-4.267855</td>\n",
       "      <td>-3.111888</td>\n",
       "      <td>-1.155967</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-90.999155</td>\n",
       "      <td>9.000845</td>\n",
       "      <td>6.405316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>274.194223</td>\n",
       "      <td>278.703510</td>\n",
       "      <td>-4.509287</td>\n",
       "      <td>-3.391368</td>\n",
       "      <td>-1.117920</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-89.233420</td>\n",
       "      <td>10.766580</td>\n",
       "      <td>8.507961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  ...  \\\n",
       "Date                                                                     ...   \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  ...   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  ...   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  ...   \n",
       "...                        ...    ...    ...    ...       ...       ...  ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  ...   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  ...   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  ...   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  ...   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  ...   \n",
       "\n",
       "                         EMA_12      EMA_26       DIF  Signal_Line       OSC  \\\n",
       "Date                                                                           \n",
       "2021-11-30 15:00:00  387.623322  387.623322  0.000000     0.000000  0.000000   \n",
       "2021-11-30 16:00:00  385.999218  386.841346 -0.842128    -0.168426 -0.673702   \n",
       "2021-11-30 17:00:00  384.630874  386.120134 -1.489260    -0.432592 -1.056667   \n",
       "2021-11-30 18:00:00  383.225880  385.333340 -2.107460    -0.767566 -1.339894   \n",
       "2021-11-30 19:00:00  382.653696  384.901735 -2.248039    -1.063661 -1.184379   \n",
       "...                         ...         ...       ...          ...       ...   \n",
       "2022-09-29 16:00:00  280.129088  282.584802 -2.455714    -2.395471 -0.060243   \n",
       "2022-09-29 17:00:00  278.263074  281.504446 -3.241372    -2.564651 -0.676721   \n",
       "2022-09-29 18:00:00  276.614907  280.470783 -3.855876    -2.822896 -1.032979   \n",
       "2022-09-29 19:00:00  275.269536  279.537391 -4.267855    -3.111888 -1.155967   \n",
       "2022-09-29 20:00:00  274.194223  278.703510 -4.509287    -3.391368 -1.117920   \n",
       "\n",
       "                     PSY_12     PSY_24  Williams_%R  Stochastic_%K  \\\n",
       "Date                                                                 \n",
       "2021-11-30 15:00:00     NaN        NaN          NaN            NaN   \n",
       "2021-11-30 16:00:00     NaN        NaN          NaN            NaN   \n",
       "2021-11-30 17:00:00     NaN        NaN          NaN            NaN   \n",
       "2021-11-30 18:00:00     NaN        NaN          NaN            NaN   \n",
       "2021-11-30 19:00:00     NaN        NaN          NaN            NaN   \n",
       "...                     ...        ...          ...            ...   \n",
       "2022-09-29 16:00:00    50.0  54.166667   -93.171910       6.828090   \n",
       "2022-09-29 17:00:00    50.0  50.000000   -95.541354       4.458646   \n",
       "2022-09-29 18:00:00    50.0  50.000000   -94.243543       5.756457   \n",
       "2022-09-29 19:00:00    50.0  54.166667   -90.999155       9.000845   \n",
       "2022-09-29 20:00:00    50.0  54.166667   -89.233420      10.766580   \n",
       "\n",
       "                     Stochastic_%D  \n",
       "Date                                \n",
       "2021-11-30 15:00:00            NaN  \n",
       "2021-11-30 16:00:00            NaN  \n",
       "2021-11-30 17:00:00            NaN  \n",
       "2021-11-30 18:00:00            NaN  \n",
       "2021-11-30 19:00:00            NaN  \n",
       "...                            ...  \n",
       "2022-09-29 16:00:00      38.773714  \n",
       "2022-09-29 17:00:00      10.050782  \n",
       "2022-09-29 18:00:00       5.681064  \n",
       "2022-09-29 19:00:00       6.405316  \n",
       "2022-09-29 20:00:00       8.507961  \n",
       "\n",
       "[1260 rows x 29 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_oscillator(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b34ad4-a675-4431-8e0a-48de2eb22dc1",
   "metadata": {},
   "source": [
    "### K. Percentage of Price Change (PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b244a954-0e2b-46b6-a5ae-4ddbda1dd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(df, column_name='Adj_Close', window=1):\n",
    "    \"\"\"\n",
    "    Calculate Percentage of Price Change (PROC) for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which PROC is calculated. Default is 'Close'.\n",
    "    - window (int): Window size for PROC calculation. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an added column for PROC.\n",
    "    \"\"\"\n",
    "    # Calculate the percentage change in price using rolling window\n",
    "    df['PROC'] = df[column_name].pct_change().rolling(window=window).mean() * 100\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16483c2c-e486-4452-b697-a3d5916cc4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>EMA_26</th>\n",
       "      <th>DIF</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>OSC</th>\n",
       "      <th>PSY_12</th>\n",
       "      <th>PSY_24</th>\n",
       "      <th>Williams_%R</th>\n",
       "      <th>Stochastic_%K</th>\n",
       "      <th>Stochastic_%D</th>\n",
       "      <th>PROC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>386.841346</td>\n",
       "      <td>-0.842128</td>\n",
       "      <td>-0.168426</td>\n",
       "      <td>-0.673702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.723438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>386.120134</td>\n",
       "      <td>-1.489260</td>\n",
       "      <td>-0.432592</td>\n",
       "      <td>-1.056667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>385.333340</td>\n",
       "      <td>-2.107460</td>\n",
       "      <td>-0.767566</td>\n",
       "      <td>-1.339894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.426024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>384.901735</td>\n",
       "      <td>-2.248039</td>\n",
       "      <td>-1.063661</td>\n",
       "      <td>-1.184379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.067448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>282.584802</td>\n",
       "      <td>-2.455714</td>\n",
       "      <td>-2.395471</td>\n",
       "      <td>-0.060243</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-93.171910</td>\n",
       "      <td>6.828090</td>\n",
       "      <td>38.773714</td>\n",
       "      <td>-0.881630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>281.504446</td>\n",
       "      <td>-3.241372</td>\n",
       "      <td>-2.564651</td>\n",
       "      <td>-0.676721</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-95.541354</td>\n",
       "      <td>4.458646</td>\n",
       "      <td>10.050782</td>\n",
       "      <td>-0.882077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>280.470783</td>\n",
       "      <td>-3.855876</td>\n",
       "      <td>-2.822896</td>\n",
       "      <td>-1.032979</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-94.243543</td>\n",
       "      <td>5.756457</td>\n",
       "      <td>5.681064</td>\n",
       "      <td>-0.167912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>279.537391</td>\n",
       "      <td>-4.267855</td>\n",
       "      <td>-3.111888</td>\n",
       "      <td>-1.155967</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-90.999155</td>\n",
       "      <td>9.000845</td>\n",
       "      <td>6.405316</td>\n",
       "      <td>0.119603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>278.703510</td>\n",
       "      <td>-4.509287</td>\n",
       "      <td>-3.391368</td>\n",
       "      <td>-1.117920</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-89.233420</td>\n",
       "      <td>10.766580</td>\n",
       "      <td>8.507961</td>\n",
       "      <td>0.153065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  ...  \\\n",
       "Date                                                                     ...   \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  ...   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  ...   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  ...   \n",
       "...                        ...    ...    ...    ...       ...       ...  ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  ...   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  ...   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  ...   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  ...   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  ...   \n",
       "\n",
       "                         EMA_26       DIF  Signal_Line       OSC  PSY_12  \\\n",
       "Date                                                                       \n",
       "2021-11-30 15:00:00  387.623322  0.000000     0.000000  0.000000     NaN   \n",
       "2021-11-30 16:00:00  386.841346 -0.842128    -0.168426 -0.673702     NaN   \n",
       "2021-11-30 17:00:00  386.120134 -1.489260    -0.432592 -1.056667     NaN   \n",
       "2021-11-30 18:00:00  385.333340 -2.107460    -0.767566 -1.339894     NaN   \n",
       "2021-11-30 19:00:00  384.901735 -2.248039    -1.063661 -1.184379     NaN   \n",
       "...                         ...       ...          ...       ...     ...   \n",
       "2022-09-29 16:00:00  282.584802 -2.455714    -2.395471 -0.060243    50.0   \n",
       "2022-09-29 17:00:00  281.504446 -3.241372    -2.564651 -0.676721    50.0   \n",
       "2022-09-29 18:00:00  280.470783 -3.855876    -2.822896 -1.032979    50.0   \n",
       "2022-09-29 19:00:00  279.537391 -4.267855    -3.111888 -1.155967    50.0   \n",
       "2022-09-29 20:00:00  278.703510 -4.509287    -3.391368 -1.117920    50.0   \n",
       "\n",
       "                        PSY_24  Williams_%R  Stochastic_%K  Stochastic_%D  \\\n",
       "Date                                                                        \n",
       "2021-11-30 15:00:00        NaN          NaN            NaN            NaN   \n",
       "2021-11-30 16:00:00        NaN          NaN            NaN            NaN   \n",
       "2021-11-30 17:00:00        NaN          NaN            NaN            NaN   \n",
       "2021-11-30 18:00:00        NaN          NaN            NaN            NaN   \n",
       "2021-11-30 19:00:00        NaN          NaN            NaN            NaN   \n",
       "...                        ...          ...            ...            ...   \n",
       "2022-09-29 16:00:00  54.166667   -93.171910       6.828090      38.773714   \n",
       "2022-09-29 17:00:00  50.000000   -95.541354       4.458646      10.050782   \n",
       "2022-09-29 18:00:00  50.000000   -94.243543       5.756457       5.681064   \n",
       "2022-09-29 19:00:00  54.166667   -90.999155       9.000845       6.405316   \n",
       "2022-09-29 20:00:00  54.166667   -89.233420      10.766580       8.507961   \n",
       "\n",
       "                         PROC  \n",
       "Date                           \n",
       "2021-11-30 15:00:00       NaN  \n",
       "2021-11-30 16:00:00 -2.723438  \n",
       "2021-11-30 17:00:00  0.010169  \n",
       "2021-11-30 18:00:00 -0.426024  \n",
       "2021-11-30 19:00:00  1.067448  \n",
       "...                       ...  \n",
       "2022-09-29 16:00:00 -0.881630  \n",
       "2022-09-29 17:00:00 -0.882077  \n",
       "2022-09-29 18:00:00 -0.167912  \n",
       "2022-09-29 19:00:00  0.119603  \n",
       "2022-09-29 20:00:00  0.153065  \n",
       "\n",
       "[1260 rows x 30 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee3aac-2183-4402-9547-ebbd1602bebd",
   "metadata": {},
   "source": [
    "### L. Momentum (MO(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b681bfb-ee2c-4c43-aef3-e121a603be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(df, column_name='Adj_Close', window=1):\n",
    "    \"\"\"\n",
    "    Calculate Momentum (MO) for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which Momentum is calculated. Default is 'Close'.\n",
    "    - window (int): Window size for Momentum calculation. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an added column for Momentum.\n",
    "    \"\"\"\n",
    "    # Calculate the difference in price over the specified window\n",
    "    df['Momentum'] = df[column_name].diff(window)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19f286c7-3202-4bcc-bbb4-dcf81ada085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>DIF</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>OSC</th>\n",
       "      <th>PSY_12</th>\n",
       "      <th>PSY_24</th>\n",
       "      <th>Williams_%R</th>\n",
       "      <th>Stochastic_%K</th>\n",
       "      <th>Stochastic_%D</th>\n",
       "      <th>PROC</th>\n",
       "      <th>Momentum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.842128</td>\n",
       "      <td>-0.168426</td>\n",
       "      <td>-0.673702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.723438</td>\n",
       "      <td>-10.556671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.489260</td>\n",
       "      <td>-0.432592</td>\n",
       "      <td>-1.056667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>0.038330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.107460</td>\n",
       "      <td>-0.767566</td>\n",
       "      <td>-1.339894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.426024</td>\n",
       "      <td>-1.606567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.248039</td>\n",
       "      <td>-1.063661</td>\n",
       "      <td>-1.184379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.067448</td>\n",
       "      <td>4.008270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.455714</td>\n",
       "      <td>-2.395471</td>\n",
       "      <td>-0.060243</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-93.171910</td>\n",
       "      <td>6.828090</td>\n",
       "      <td>38.773714</td>\n",
       "      <td>-0.881630</td>\n",
       "      <td>-2.404999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.241372</td>\n",
       "      <td>-2.564651</td>\n",
       "      <td>-0.676721</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-95.541354</td>\n",
       "      <td>4.458646</td>\n",
       "      <td>10.050782</td>\n",
       "      <td>-0.882077</td>\n",
       "      <td>-2.385010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.855876</td>\n",
       "      <td>-2.822896</td>\n",
       "      <td>-1.032979</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-94.243543</td>\n",
       "      <td>5.756457</td>\n",
       "      <td>5.681064</td>\n",
       "      <td>-0.167912</td>\n",
       "      <td>-0.450012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.267855</td>\n",
       "      <td>-3.111888</td>\n",
       "      <td>-1.155967</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-90.999155</td>\n",
       "      <td>9.000845</td>\n",
       "      <td>6.405316</td>\n",
       "      <td>0.119603</td>\n",
       "      <td>0.320007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.509287</td>\n",
       "      <td>-3.391368</td>\n",
       "      <td>-1.117920</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-89.233420</td>\n",
       "      <td>10.766580</td>\n",
       "      <td>8.507961</td>\n",
       "      <td>0.153065</td>\n",
       "      <td>0.410004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  ...  \\\n",
       "Date                                                                     ...   \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  ...   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  ...   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  ...   \n",
       "...                        ...    ...    ...    ...       ...       ...  ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  ...   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  ...   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  ...   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  ...   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  ...   \n",
       "\n",
       "                          DIF  Signal_Line       OSC  PSY_12     PSY_24  \\\n",
       "Date                                                                      \n",
       "2021-11-30 15:00:00  0.000000     0.000000  0.000000     NaN        NaN   \n",
       "2021-11-30 16:00:00 -0.842128    -0.168426 -0.673702     NaN        NaN   \n",
       "2021-11-30 17:00:00 -1.489260    -0.432592 -1.056667     NaN        NaN   \n",
       "2021-11-30 18:00:00 -2.107460    -0.767566 -1.339894     NaN        NaN   \n",
       "2021-11-30 19:00:00 -2.248039    -1.063661 -1.184379     NaN        NaN   \n",
       "...                       ...          ...       ...     ...        ...   \n",
       "2022-09-29 16:00:00 -2.455714    -2.395471 -0.060243    50.0  54.166667   \n",
       "2022-09-29 17:00:00 -3.241372    -2.564651 -0.676721    50.0  50.000000   \n",
       "2022-09-29 18:00:00 -3.855876    -2.822896 -1.032979    50.0  50.000000   \n",
       "2022-09-29 19:00:00 -4.267855    -3.111888 -1.155967    50.0  54.166667   \n",
       "2022-09-29 20:00:00 -4.509287    -3.391368 -1.117920    50.0  54.166667   \n",
       "\n",
       "                     Williams_%R  Stochastic_%K  Stochastic_%D      PROC  \\\n",
       "Date                                                                       \n",
       "2021-11-30 15:00:00          NaN            NaN            NaN       NaN   \n",
       "2021-11-30 16:00:00          NaN            NaN            NaN -2.723438   \n",
       "2021-11-30 17:00:00          NaN            NaN            NaN  0.010169   \n",
       "2021-11-30 18:00:00          NaN            NaN            NaN -0.426024   \n",
       "2021-11-30 19:00:00          NaN            NaN            NaN  1.067448   \n",
       "...                          ...            ...            ...       ...   \n",
       "2022-09-29 16:00:00   -93.171910       6.828090      38.773714 -0.881630   \n",
       "2022-09-29 17:00:00   -95.541354       4.458646      10.050782 -0.882077   \n",
       "2022-09-29 18:00:00   -94.243543       5.756457       5.681064 -0.167912   \n",
       "2022-09-29 19:00:00   -90.999155       9.000845       6.405316  0.119603   \n",
       "2022-09-29 20:00:00   -89.233420      10.766580       8.507961  0.153065   \n",
       "\n",
       "                      Momentum  \n",
       "Date                            \n",
       "2021-11-30 15:00:00        NaN  \n",
       "2021-11-30 16:00:00 -10.556671  \n",
       "2021-11-30 17:00:00   0.038330  \n",
       "2021-11-30 18:00:00  -1.606567  \n",
       "2021-11-30 19:00:00   4.008270  \n",
       "...                        ...  \n",
       "2022-09-29 16:00:00  -2.404999  \n",
       "2022-09-29 17:00:00  -2.385010  \n",
       "2022-09-29 18:00:00  -0.450012  \n",
       "2022-09-29 19:00:00   0.320007  \n",
       "2022-09-29 20:00:00   0.410004  \n",
       "\n",
       "[1260 rows x 31 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "momentum(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1327d00-c61d-4fa7-b144-fa077790c250",
   "metadata": {},
   "source": [
    "### M. First-Order Lag (LAG(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf2336b9-6545-4d62-87cc-441b58683cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_lag(df, column_name='Adj_Close', lag=1):\n",
    "    \"\"\"\n",
    "    Calculate First-Order Lag (LAG(1)) for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - column_name (str): Name of the column for which the lag is calculated. Default is 'Close'.\n",
    "    - lag (int): Number of periods to lag. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an added column for the First-Order Lag.\n",
    "    \"\"\"\n",
    "    # Calculate the First-Order Lag using the shift() method\n",
    "    df[f'LAG_{lag}'] = df[column_name].shift(lag)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8d89fc2-f074-485d-8c31-7db9bc9c5cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>OSC</th>\n",
       "      <th>PSY_12</th>\n",
       "      <th>PSY_24</th>\n",
       "      <th>Williams_%R</th>\n",
       "      <th>Stochastic_%K</th>\n",
       "      <th>Stochastic_%D</th>\n",
       "      <th>PROC</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>LAG_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168426</td>\n",
       "      <td>-0.673702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.723438</td>\n",
       "      <td>-10.556671</td>\n",
       "      <td>387.623322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.432592</td>\n",
       "      <td>-1.056667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>0.038330</td>\n",
       "      <td>377.066650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.767566</td>\n",
       "      <td>-1.339894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.426024</td>\n",
       "      <td>-1.606567</td>\n",
       "      <td>377.104980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.063661</td>\n",
       "      <td>-1.184379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.067448</td>\n",
       "      <td>4.008270</td>\n",
       "      <td>375.498413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.395471</td>\n",
       "      <td>-0.060243</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-93.171910</td>\n",
       "      <td>6.828090</td>\n",
       "      <td>38.773714</td>\n",
       "      <td>-0.881630</td>\n",
       "      <td>-2.404999</td>\n",
       "      <td>272.790009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.564651</td>\n",
       "      <td>-0.676721</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-95.541354</td>\n",
       "      <td>4.458646</td>\n",
       "      <td>10.050782</td>\n",
       "      <td>-0.882077</td>\n",
       "      <td>-2.385010</td>\n",
       "      <td>270.385010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.822896</td>\n",
       "      <td>-1.032979</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-94.243543</td>\n",
       "      <td>5.756457</td>\n",
       "      <td>5.681064</td>\n",
       "      <td>-0.167912</td>\n",
       "      <td>-0.450012</td>\n",
       "      <td>268.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.111888</td>\n",
       "      <td>-1.155967</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-90.999155</td>\n",
       "      <td>9.000845</td>\n",
       "      <td>6.405316</td>\n",
       "      <td>0.119603</td>\n",
       "      <td>0.320007</td>\n",
       "      <td>267.549988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.391368</td>\n",
       "      <td>-1.117920</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-89.233420</td>\n",
       "      <td>10.766580</td>\n",
       "      <td>8.507961</td>\n",
       "      <td>0.153065</td>\n",
       "      <td>0.410004</td>\n",
       "      <td>267.869995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  ...  \\\n",
       "Date                                                                     ...   \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  ...   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  ...   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  ...   \n",
       "...                        ...    ...    ...    ...       ...       ...  ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  ...   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  ...   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  ...   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  ...   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  ...   \n",
       "\n",
       "                     Signal_Line       OSC  PSY_12     PSY_24  Williams_%R  \\\n",
       "Date                                                                         \n",
       "2021-11-30 15:00:00     0.000000  0.000000     NaN        NaN          NaN   \n",
       "2021-11-30 16:00:00    -0.168426 -0.673702     NaN        NaN          NaN   \n",
       "2021-11-30 17:00:00    -0.432592 -1.056667     NaN        NaN          NaN   \n",
       "2021-11-30 18:00:00    -0.767566 -1.339894     NaN        NaN          NaN   \n",
       "2021-11-30 19:00:00    -1.063661 -1.184379     NaN        NaN          NaN   \n",
       "...                          ...       ...     ...        ...          ...   \n",
       "2022-09-29 16:00:00    -2.395471 -0.060243    50.0  54.166667   -93.171910   \n",
       "2022-09-29 17:00:00    -2.564651 -0.676721    50.0  50.000000   -95.541354   \n",
       "2022-09-29 18:00:00    -2.822896 -1.032979    50.0  50.000000   -94.243543   \n",
       "2022-09-29 19:00:00    -3.111888 -1.155967    50.0  54.166667   -90.999155   \n",
       "2022-09-29 20:00:00    -3.391368 -1.117920    50.0  54.166667   -89.233420   \n",
       "\n",
       "                     Stochastic_%K  Stochastic_%D      PROC   Momentum  \\\n",
       "Date                                                                     \n",
       "2021-11-30 15:00:00            NaN            NaN       NaN        NaN   \n",
       "2021-11-30 16:00:00            NaN            NaN -2.723438 -10.556671   \n",
       "2021-11-30 17:00:00            NaN            NaN  0.010169   0.038330   \n",
       "2021-11-30 18:00:00            NaN            NaN -0.426024  -1.606567   \n",
       "2021-11-30 19:00:00            NaN            NaN  1.067448   4.008270   \n",
       "...                            ...            ...       ...        ...   \n",
       "2022-09-29 16:00:00       6.828090      38.773714 -0.881630  -2.404999   \n",
       "2022-09-29 17:00:00       4.458646      10.050782 -0.882077  -2.385010   \n",
       "2022-09-29 18:00:00       5.756457       5.681064 -0.167912  -0.450012   \n",
       "2022-09-29 19:00:00       9.000845       6.405316  0.119603   0.320007   \n",
       "2022-09-29 20:00:00      10.766580       8.507961  0.153065   0.410004   \n",
       "\n",
       "                          LAG_1  \n",
       "Date                             \n",
       "2021-11-30 15:00:00         NaN  \n",
       "2021-11-30 16:00:00  387.623322  \n",
       "2021-11-30 17:00:00  377.066650  \n",
       "2021-11-30 18:00:00  377.104980  \n",
       "2021-11-30 19:00:00  375.498413  \n",
       "...                         ...  \n",
       "2022-09-29 16:00:00  272.790009  \n",
       "2022-09-29 17:00:00  270.385010  \n",
       "2022-09-29 18:00:00  268.000000  \n",
       "2022-09-29 19:00:00  267.549988  \n",
       "2022-09-29 20:00:00  267.869995  \n",
       "\n",
       "[1260 rows x 32 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_order_lag(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3dc06-f6e6-4022-9cef-a19c8516bd4a",
   "metadata": {},
   "source": [
    "### N. Trading Volume (VOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b1cb210-531e-4b23-89b5-8d1750598016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_volume(df, volume_column='Volume'):\n",
    "    \"\"\"\n",
    "    Calculate Trading Volume (VOL) for a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - volume_column (str): Name of the column containing trading volume. Default is 'Volume'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an added column for Trading Volume.\n",
    "    \"\"\"\n",
    "    df['VOL'] = df[volume_column]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d635c1d-1bbf-4de9-b3da-acea1163d1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>OSC</th>\n",
       "      <th>PSY_12</th>\n",
       "      <th>PSY_24</th>\n",
       "      <th>Williams_%R</th>\n",
       "      <th>Stochastic_%K</th>\n",
       "      <th>Stochastic_%D</th>\n",
       "      <th>PROC</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>LAG_1</th>\n",
       "      <th>VOL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-30 15:00:00</th>\n",
       "      <td>381.456665</td>\n",
       "      <td>389.333344</td>\n",
       "      <td>381.334717</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>6562747.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6562747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 16:00:00</th>\n",
       "      <td>387.766693</td>\n",
       "      <td>388.833344</td>\n",
       "      <td>375.333344</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>5269359.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.723438</td>\n",
       "      <td>-10.556671</td>\n",
       "      <td>387.623322</td>\n",
       "      <td>5269359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 17:00:00</th>\n",
       "      <td>376.839996</td>\n",
       "      <td>378.619995</td>\n",
       "      <td>372.666656</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>3247689.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.056667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>0.038330</td>\n",
       "      <td>377.066650</td>\n",
       "      <td>3247689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 18:00:00</th>\n",
       "      <td>376.963348</td>\n",
       "      <td>380.649994</td>\n",
       "      <td>375.066650</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>2089177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.339894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.426024</td>\n",
       "      <td>-1.606567</td>\n",
       "      <td>377.104980</td>\n",
       "      <td>2089177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30 19:00:00</th>\n",
       "      <td>375.510010</td>\n",
       "      <td>381.189972</td>\n",
       "      <td>375.176666</td>\n",
       "      <td>379.506683</td>\n",
       "      <td>1840965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.184379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.067448</td>\n",
       "      <td>4.008270</td>\n",
       "      <td>375.498413</td>\n",
       "      <td>1840965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 16:00:00</th>\n",
       "      <td>272.820007</td>\n",
       "      <td>273.760010</td>\n",
       "      <td>269.649994</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>7964963.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060243</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-93.171910</td>\n",
       "      <td>6.828090</td>\n",
       "      <td>38.773714</td>\n",
       "      <td>-0.881630</td>\n",
       "      <td>-2.404999</td>\n",
       "      <td>272.790009</td>\n",
       "      <td>7964963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 17:00:00</th>\n",
       "      <td>270.329987</td>\n",
       "      <td>271.149994</td>\n",
       "      <td>267.019989</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8794413.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.676721</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-95.541354</td>\n",
       "      <td>4.458646</td>\n",
       "      <td>10.050782</td>\n",
       "      <td>-0.882077</td>\n",
       "      <td>-2.385010</td>\n",
       "      <td>270.385010</td>\n",
       "      <td>8794413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 18:00:00</th>\n",
       "      <td>268.000214</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>266.239807</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8637437.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.032979</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-94.243543</td>\n",
       "      <td>5.756457</td>\n",
       "      <td>5.681064</td>\n",
       "      <td>-0.167912</td>\n",
       "      <td>-0.450012</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>8637437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 19:00:00</th>\n",
       "      <td>267.540009</td>\n",
       "      <td>269.160004</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>8950333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.155967</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-90.999155</td>\n",
       "      <td>9.000845</td>\n",
       "      <td>6.405316</td>\n",
       "      <td>0.119603</td>\n",
       "      <td>0.320007</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>8950333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29 20:00:00</th>\n",
       "      <td>267.850006</td>\n",
       "      <td>269.399994</td>\n",
       "      <td>267.549988</td>\n",
       "      <td>268.279999</td>\n",
       "      <td>6499787.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.117920</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>-89.233420</td>\n",
       "      <td>10.766580</td>\n",
       "      <td>8.507961</td>\n",
       "      <td>0.153065</td>\n",
       "      <td>0.410004</td>\n",
       "      <td>267.869995</td>\n",
       "      <td>6499787.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low   Adj_Close  \\\n",
       "Date                                                                  \n",
       "2021-11-30 15:00:00  381.456665  389.333344  381.334717  387.623322   \n",
       "2021-11-30 16:00:00  387.766693  388.833344  375.333344  377.066650   \n",
       "2021-11-30 17:00:00  376.839996  378.619995  372.666656  377.104980   \n",
       "2021-11-30 18:00:00  376.963348  380.649994  375.066650  375.498413   \n",
       "2021-11-30 19:00:00  375.510010  381.189972  375.176666  379.506683   \n",
       "...                         ...         ...         ...         ...   \n",
       "2022-09-29 16:00:00  272.820007  273.760010  269.649994  270.385010   \n",
       "2022-09-29 17:00:00  270.329987  271.149994  267.019989  268.000000   \n",
       "2022-09-29 18:00:00  268.000214  268.869995  266.239807  267.549988   \n",
       "2022-09-29 19:00:00  267.540009  269.160004  265.779999  267.869995   \n",
       "2022-09-29 20:00:00  267.850006  269.399994  267.549988  268.279999   \n",
       "\n",
       "                        Volume  Label  Score  Total  Positive  Negative  ...  \\\n",
       "Date                                                                     ...   \n",
       "2021-11-30 15:00:00  6562747.0      1    0.0   11.0  0.000000  0.000000  ...   \n",
       "2021-11-30 16:00:00  5269359.0      0   -1.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 17:00:00  3247689.0      1   -2.0    7.0  0.000000  0.000000  ...   \n",
       "2021-11-30 18:00:00  2089177.0      0    0.0    7.0  0.142857 -0.142857  ...   \n",
       "2021-11-30 19:00:00  1840965.0      1   -2.0    5.0  0.000000  0.000000  ...   \n",
       "...                        ...    ...    ...    ...       ...       ...  ...   \n",
       "2022-09-29 16:00:00  7964963.0      0    0.0    6.0  0.000000  0.000000  ...   \n",
       "2022-09-29 17:00:00  8794413.0      0   -1.0    8.0  0.000000  0.000000  ...   \n",
       "2022-09-29 18:00:00  8637437.0      0   -2.0   11.0  0.090909 -0.090909  ...   \n",
       "2022-09-29 19:00:00  8950333.0      1    0.0    9.0  0.111111 -0.111111  ...   \n",
       "2022-09-29 20:00:00  6499787.0      1    1.0    9.0  0.111111 -0.111111  ...   \n",
       "\n",
       "                          OSC  PSY_12     PSY_24  Williams_%R  Stochastic_%K  \\\n",
       "Date                                                                           \n",
       "2021-11-30 15:00:00  0.000000     NaN        NaN          NaN            NaN   \n",
       "2021-11-30 16:00:00 -0.673702     NaN        NaN          NaN            NaN   \n",
       "2021-11-30 17:00:00 -1.056667     NaN        NaN          NaN            NaN   \n",
       "2021-11-30 18:00:00 -1.339894     NaN        NaN          NaN            NaN   \n",
       "2021-11-30 19:00:00 -1.184379     NaN        NaN          NaN            NaN   \n",
       "...                       ...     ...        ...          ...            ...   \n",
       "2022-09-29 16:00:00 -0.060243    50.0  54.166667   -93.171910       6.828090   \n",
       "2022-09-29 17:00:00 -0.676721    50.0  50.000000   -95.541354       4.458646   \n",
       "2022-09-29 18:00:00 -1.032979    50.0  50.000000   -94.243543       5.756457   \n",
       "2022-09-29 19:00:00 -1.155967    50.0  54.166667   -90.999155       9.000845   \n",
       "2022-09-29 20:00:00 -1.117920    50.0  54.166667   -89.233420      10.766580   \n",
       "\n",
       "                     Stochastic_%D      PROC   Momentum       LAG_1        VOL  \n",
       "Date                                                                            \n",
       "2021-11-30 15:00:00            NaN       NaN        NaN         NaN  6562747.0  \n",
       "2021-11-30 16:00:00            NaN -2.723438 -10.556671  387.623322  5269359.0  \n",
       "2021-11-30 17:00:00            NaN  0.010169   0.038330  377.066650  3247689.0  \n",
       "2021-11-30 18:00:00            NaN -0.426024  -1.606567  377.104980  2089177.0  \n",
       "2021-11-30 19:00:00            NaN  1.067448   4.008270  375.498413  1840965.0  \n",
       "...                            ...       ...        ...         ...        ...  \n",
       "2022-09-29 16:00:00      38.773714 -0.881630  -2.404999  272.790009  7964963.0  \n",
       "2022-09-29 17:00:00      10.050782 -0.882077  -2.385010  270.385010  8794413.0  \n",
       "2022-09-29 18:00:00       5.681064 -0.167912  -0.450012  268.000000  8637437.0  \n",
       "2022-09-29 19:00:00       6.405316  0.119603   0.320007  267.549988  8950333.0  \n",
       "2022-09-29 20:00:00       8.507961  0.153065   0.410004  267.869995  6499787.0  \n",
       "\n",
       "[1260 rows x 33 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_volume(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb30e62-be2d-4850-824d-b81707b82606",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8577e4-d93e-4919-9c59-29f7fe9c1f50",
   "metadata": {},
   "source": [
    "### a. Removing columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0720001a-a6bc-4020-b581-96dfb5a124dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns_to_drop=['Open', 'High', 'Low', 'Adj_Close', 'Volume']):\n",
    "    \"\"\"\n",
    "    Drop specified columns from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - columns_to_drop (list): List of column names to drop. Default is ['Open', 'High', 'Low', 'Adj_Close', 'Volume'].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with specified columns dropped.\n",
    "    \"\"\"\n",
    "    # Drop specified columns\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13cbf4b6-e107-4baa-b31f-df5b0cfa1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df):\n",
    "    \"\"\"\n",
    "    Drop all rows with NaN values from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with NaN rows dropped.\n",
    "    - list: List of indices corresponding to dropped rows.\n",
    "    \"\"\"\n",
    "    # Drop rows with NaN values\n",
    "    cleaned_df = df.dropna()\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9237c64c-750b-40bd-8e53-fad6fe1c60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_merged_df = drop_columns(merged_df)\n",
    "clean_merged_df = drop_rows(clean_merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83214a7-776d-4175-a7fb-7d51d2803da1",
   "metadata": {},
   "source": [
    "### b. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e83d8fbd-faf8-4f74-80c3-7332bb6a4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataframe(df):\n",
    "    \"\"\"\n",
    "    Scale a DataFrame using Standard scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Scaled DataFrame.\n",
    "    \"\"\"\n",
    "    # Scale the selected columns\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    index_column = df.index\n",
    "    \n",
    "    date_column = df['Label']\n",
    "    int_df = df.drop(columns=['Label'])\n",
    "    \n",
    "    columns_to_scale = int_df.columns\n",
    "    \n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(int_df), columns=columns_to_scale)\n",
    "    scaled_df.index = index_column\n",
    "    scaled_df['Label'] = date_column\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6067908a-b251-4952-9cc4-e74d386466fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_clean_merged_df = scale_dataframe(clean_merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fec9b-e75c-4506-ba16-889cb9530c06",
   "metadata": {},
   "source": [
    "### c. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "003458b2-8156-43d8-972f-d653db94c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split a time series dataset into training, testing, and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "    - df: NumPy array or matrix, the input time series dataset.\n",
    "    - test_size: Float, the proportion of the dataset to include in the test split.\n",
    "    - val_size: Float, the proportion of the dataset to include in the validation split.\n",
    "\n",
    "    Returns:\n",
    "    - df_train, df_test: Pandas arrays, representing features and target values for each set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract index number of splitting points\n",
    "    len_df = len(df)\n",
    "    index_1 = round(len_df*(1-(test_size)))\n",
    "    index_2 = index_1 +1\n",
    "\n",
    "    # Extract values at previously calculated splitting points\n",
    "    date_1 = df.index[index_1]\n",
    "    date_2 = df.index[index_2]\n",
    "\n",
    "    # Construct train_df, val_df and test_df\n",
    "    df_train = df[:date_1]\n",
    "    df_test = df[date_2:]\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "610aa3d8-5e39-432f-ab1e-c63787246e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_scaled_clean_merged_df = train_test_split(scaled_clean_merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564e8e7-fb89-48f7-85aa-57c8d9a66aaa",
   "metadata": {},
   "source": [
    "### d. Reshape Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9524537-09e0-4957-b2c1-7e978255f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_matrix_split_X_y(df, window_size=5):\n",
    "    \"\"\"\n",
    "    Reshape a DataFrame into a 3D NumPy arrays (num_observations, window_size, num_features)\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with a list of time series data\n",
    "    - sequence_length: the number of time steps to consider for each observation\n",
    "\n",
    "    Returns:\n",
    "    - X, y: a 3D NumPy arrays, one for the features and one for the lables\n",
    "    \"\"\"\n",
    "    df_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    df_X = df.drop('Label', axis=1)\n",
    "    df_y = df['Label']\n",
    "\n",
    "    for i in range(len(df_np)-(window_size)):\n",
    "        row = df_X[i:i+window_size]\n",
    "        X.append(row)\n",
    "        label = df_y[i+(window_size)]\n",
    "        y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    y = np.expand_dims(y, axis=-1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5bb75c2-cac5-413b-bff0-cfde39476b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/q_79lrcx3ps_z7hltvr9nl4c0000gn/T/ipykernel_62963/118946909.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = df_y[i+(window_size)]\n",
      "/var/folders/l4/q_79lrcx3ps_z7hltvr9nl4c0000gn/T/ipykernel_62963/118946909.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = df_y[i+(window_size)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((986, 5, 27), (986, 1))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = input_matrix_split_X_y(split_scaled_clean_merged_df[0], 5)\n",
    "X_test, y_test = input_matrix_split_X_y(split_scaled_clean_merged_df[1], 5)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80ca8b-3dc4-40e7-93b1-fd67773d0109",
   "metadata": {},
   "source": [
    "## 3. Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a93f188f-4dda-4e8f-a3af-cdd7bddaa653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Flatten, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0edf0-1caa-4e16-b6f4-a417429b8ec9",
   "metadata": {},
   "source": [
    "### a. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "009e5152-86d1-4dd3-a8ce-1f90ab17ec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 5, 64)             23552     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5, 64)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5, 1)              65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23617 (92.25 KB)\n",
      "Trainable params: 23617 (92.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(window_size=5, optimizer_name='adam'):\n",
    "    \n",
    "    #############################\n",
    "    #  1 - Model architecture   #\n",
    "    ############################# \n",
    "    \n",
    "    # [1, 5] layers\n",
    "    # nodes per layer [30, 70]\n",
    "    # activation function: relu\n",
    "    # optimizer: Adam\n",
    "    # learning rate = [0.001, 0.1]\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(window_size, X_train.shape[-1])))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    # $CHALLENGIFY_END\n",
    "    \n",
    "    #############################\n",
    "    #  2 - Optimization Method  #\n",
    "    #############################\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer_name, \n",
    "                  metrics=['accuracy']) \n",
    "\n",
    "    return model \n",
    "\n",
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e0495-4b7f-43dc-bb07-7ca685933248",
   "metadata": {},
   "source": [
    "### b. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07849ce9-0ab5-4245-907c-10137064f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 1s 18ms/step - loss: 0.7011 - accuracy: 0.4963 - val_loss: 0.6904 - val_accuracy: 0.5778\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6961 - accuracy: 0.5125 - val_loss: 0.6971 - val_accuracy: 0.4707\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5333 - val_loss: 0.6986 - val_accuracy: 0.4566\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5443 - val_loss: 0.7006 - val_accuracy: 0.4444\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6846 - accuracy: 0.5493 - val_loss: 0.6986 - val_accuracy: 0.4869\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5421 - val_loss: 0.7076 - val_accuracy: 0.4162\n"
     ]
    }
   ],
   "source": [
    "# batch size = [64, 128]\n",
    "# epochs = [50, 80]\n",
    "\n",
    "def train_model():\n",
    "    \n",
    "    es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_split=0.1,\n",
    "                        batch_size=64,\n",
    "                        epochs=100, \n",
    "                        callbacks=[es],\n",
    "                        verbose=1)\n",
    "\n",
    "    return history\n",
    "\n",
    "history = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b38dfacb-3d51-413f-838b-571d0ff562c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f884579-8d1d-4876-af65-ad41b206e430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB0z0lEQVR4nO3de3zO9f/H8ce1M7MDxmbMHHM+bgyVUgoVUUoSkvTtG5VW/UrfotM3fTv6Vkr5ftFB8Y1IB4pREsJmksOczWmbYUd2sOvz++Nj1yymzQ6fbdfzfrtdN9f1uT7X53pdS67n3kebYRgGIiIiIk7ExeoCRERERCqaApCIiIg4HQUgERERcToKQCIiIuJ0FIBERETE6SgAiYiIiNNRABIRERGnowAkIiIiTkcBSERERJyOApCIVCibzcbzzz9f4tcdOHAAm83GnDlzyrwmEXE+CkAiTmjOnDnYbDZsNhtr1qy54HnDMAgJCcFms3HLLbdYUGHZ+P7777HZbAQHB2O3260uR0QqEQUgESfm5eXF559/fsHxn3/+mcOHD+Pp6WlBVWVn7ty5NGnShGPHjrFy5UqryxGRSkQBSMSJ3XTTTXz55ZecPXu20PHPP/+csLAwgoKCLKqs9DIzM/n666+JjIykS5cuzJ071+qSipSZmWl1CSJORwFIxIkNHz6cEydOsHz5csexnJwcFixYwN13333R12RmZvL4448TEhKCp6cnrVq14o033sAwjELnZWdn89hjj1GvXj18fHwYNGgQhw8fvug1jxw5wn333UdgYCCenp60a9eOWbNmleqzLVq0iDNnznDHHXdw11138dVXX5GVlXXBeVlZWTz//PNcccUVeHl50aBBA2677Tb27t3rOMdut/Pvf/+bDh064OXlRb169ejfvz+bNm0CLj0+6c9jnp5//nlsNhvbt2/n7rvvpnbt2lx11VUA/P7779x77700a9YMLy8vgoKCuO+++zhx4sRFf2Zjx44lODgYT09PmjZtyt///ndycnLYt28fNpuNt99++4LXrV27FpvNxhdffFHSH6lIteJmdQEiYp0mTZrQs2dPvvjiCwYMGADA0qVLSU1N5a677uKdd94pdL5hGAwaNIhVq1YxduxYOnfuzA8//MCTTz7JkSNHCn3h3n///Xz22Wfcfffd9OrVi5UrV3LzzTdfUENiYiI9evTAZrMxYcIE6tWrx9KlSxk7dixpaWlMnDjxsj7b3Llz6dOnD0FBQdx11108/fTTfPPNN9xxxx2Oc/Ly8rjllluIiorirrvu4tFHHyU9PZ3ly5fzxx9/0Lx5cwDGjh3LnDlzGDBgAPfffz9nz57ll19+Yf369YSHh19WfXfccQctW7bklVdecYTH5cuXs2/fPsaMGUNQUBDbtm3jo48+Ytu2baxfvx6bzQbA0aNH6d69OykpKTzwwAO0bt2aI0eOsGDBAk6fPk2zZs248sormTt3Lo899tgFPxcfHx9uvfXWy6pbpNowRMTpzJ492wCMjRs3Gu+9957h4+NjnD592jAMw7jjjjuMPn36GIZhGKGhocbNN9/seN3ixYsNwHj55ZcLXW/o0KGGzWYz9uzZYxiGYcTGxhqA8dBDDxU67+677zYAY8qUKY5jY8eONRo0aGAkJycXOveuu+4y/Pz8HHXt37/fAIzZs2f/5edLTEw03NzcjJkzZzqO9erVy7j11lsLnTdr1iwDMN56660LrmG32w3DMIyVK1cagPHII48Uec6lavvz550yZYoBGMOHD7/g3PzPer4vvvjCAIzVq1c7jo0aNcpwcXExNm7cWGRNH374oQEYO3bscDyXk5NjBAQEGKNHj77gdSLORl1gIk7uzjvv5MyZM3z77bekp6fz7bffFtn99f333+Pq6sojjzxS6Pjjjz+OYRgsXbrUcR5wwXl/bs0xDIOFCxcycOBADMMgOTnZcevXrx+pqanExMSU+DPNmzcPFxcXbr/9dsex4cOHs3TpUk6dOuU4tnDhQgICAnj44YcvuEZ+a8vChQux2WxMmTKlyHMux4MPPnjBsRo1ajjuZ2VlkZycTI8ePQAcPwe73c7ixYsZOHDgRVuf8mu688478fLyKjT26YcffiA5OZl77rnnsusWqS4UgEScXL169ejbty+ff/45X331FXl5eQwdOvSi5x48eJDg4GB8fHwKHW/Tpo3j+fw/XVxcHF1I+Vq1alXo8fHjx0lJSeGjjz6iXr16hW5jxowBICkpqcSf6bPPPqN79+6cOHGCPXv2sGfPHrp06UJOTg5ffvml47y9e/fSqlUr3NyKHg2wd+9egoODqVOnTonruJSmTZtecOzkyZM8+uijBAYGUqNGDerVq+c4LzU1FTB/ZmlpabRv3/6S1/f392fgwIGFZvnNnTuXhg0bct1115XhJxGpmjQGSES4++67GTduHAkJCQwYMAB/f/8Ked/8tXnuueceRo8efdFzOnbsWKJr7t69m40bNwLQsmXLC56fO3cuDzzwQAkrvbSiWoLy8vKKfM35rT357rzzTtauXcuTTz5J586dqVWrFna7nf79+1/WOkajRo3iyy+/ZO3atXTo0IElS5bw0EMP4eKi331FFIBEhCFDhvC3v/2N9evXM3/+/CLPCw0NZcWKFaSnpxdqBdq5c6fj+fw/7Xa7o4UlX1xcXKHr5c8Qy8vLo2/fvmXyWebOnYu7uzuffvoprq6uhZ5bs2YN77zzDvHx8TRu3JjmzZvz22+/kZubi7u7+0Wv17x5c3744QdOnjxZZCtQ7dq1AUhJSSl0PL9FrDhOnTpFVFQUL7zwApMnT3Yc3717d6Hz6tWrh6+vL3/88cdfXrN///7Uq1ePuXPnEhERwenTpxk5cmSxaxKpzvRrgIhQq1YtPvjgA55//nkGDhxY5Hk33XQTeXl5vPfee4WOv/3229hsNsdMsvw//zyLbNq0aYUeu7q6cvvtt7Nw4cKLfqEfP368xJ9l7ty5XH311QwbNoyhQ4cWuj355JMAjingt99+O8nJyRd8HsAxM+v222/HMAxeeOGFIs/x9fUlICCA1atXF3r+/fffL3bd+WHN+NNyAn/+mbm4uDB48GC++eYbxzT8i9UE4ObmxvDhw/nf//7HnDlz6NChQ4lb1ESqK7UAiQhAkV1Q5xs4cCB9+vThH//4BwcOHKBTp078+OOPfP3110ycONEx5qdz584MHz6c999/n9TUVHr16kVUVBR79uy54Jqvvvoqq1atIiIignHjxtG2bVtOnjxJTEwMK1as4OTJk8X+DL/99ht79uxhwoQJF32+YcOGdO3alblz5/LUU08xatQoPvnkEyIjI9mwYQNXX301mZmZrFixgoceeohbb72VPn36MHLkSN555x12797t6I765Zdf6NOnj+O97r//fl599VXuv/9+wsPDWb16Nbt27Sp27b6+vvTu3ZvXXnuN3NxcGjZsyI8//sj+/fsvOPeVV17hxx9/5JprruGBBx6gTZs2HDt2jC+//JI1a9YU6sIcNWoU77zzDqtWreJf//pXsesRqfasm4AmIlY5fxr8pfx5GrxhGEZ6errx2GOPGcHBwYa7u7vRsmVL4/XXX3dMv8535swZ45FHHjHq1q1reHt7GwMHDjQOHTp0wbRwwzCnrY8fP94ICQkx3N3djaCgIOP66683PvroI8c5xZkG//DDDxuAsXfv3iLPef755w3A2LJli2EY5tTzf/zjH0bTpk0d7z106NBC1zh79qzx+uuvG61btzY8PDyMevXqGQMGDDCio6Md55w+fdoYO3as4efnZ/j4+Bh33nmnkZSUVOQ0+OPHj19Q2+HDh40hQ4YY/v7+hp+fn3HHHXcYR48evejP7ODBg8aoUaOMevXqGZ6enkazZs2M8ePHG9nZ2Rdct127doaLi4tx+PDhIn8uIs7GZhh/am8VEZFqpUuXLtSpU4eoqCirSxGpNDQGSESkGtu0aROxsbGMGjXK6lJEKhW1AImIVEN//PEH0dHRvPnmmyQnJ7Nv3z68vLysLkuk0lALkIhINbRgwQLGjBlDbm4uX3zxhcKPyJ+oBUhEREScjlqARERExOkoAImIiIjT0UKIF2G32zl69Cg+Pj6l2u1ZREREKo5hGKSnpxMcHPyXe94pAF3E0aNHCQkJsboMERERuQyHDh2iUaNGlzxHAegi8jd5PHToEL6+vhZXIyIiIsWRlpZGSEhIoc2ai6IAdBH53V6+vr4KQCIiIlVMcYavaBC0iIiIOB0FIBEREXE6CkAiIiLidDQGSEREpILY7XZycnKsLqPKcnd3x9XVtUyupQAkIiJSAXJycti/fz92u93qUqo0f39/goKCSr1On+UBaPr06bz++uskJCTQqVMn3n33Xbp3737Rc+fMmcOYMWMKHfP09CQrK8vxOCMjg6effprFixdz4sQJmjZtyiOPPMKDDz5Yrp9DRESkKIZhcOzYMVxdXQkJCfnLRfrkQoZhcPr0aZKSkgBo0KBBqa5naQCaP38+kZGRzJgxg4iICKZNm0a/fv2Ii4ujfv36F32Nr68vcXFxjsd/ToCRkZGsXLmSzz77jCZNmvDjjz/y0EMPERwczKBBg8r184iIiFzM2bNnOX36NMHBwdSsWdPqcqqsGjVqAJCUlET9+vVL1R1maQR96623GDduHGPGjKFt27bMmDGDmjVrMmvWrCJfY7PZCAoKctwCAwMLPb927VpGjx7NtddeS5MmTXjggQfo1KkTGzZsKO+PIyIiclF5eXkAeHh4WFxJ1ZcfIHNzc0t1HcsCUE5ODtHR0fTt27egGBcX+vbty7p164p8XUZGBqGhoYSEhHDrrbeybdu2Qs/36tWLJUuWcOTIEQzDYNWqVezatYsbb7yxyGtmZ2eTlpZW6CYiIlLWtL9k6ZXVz9CyAJScnExeXt4FLTiBgYEkJCRc9DWtWrVi1qxZfP3113z22WfY7XZ69erF4cOHHee8++67tG3blkaNGuHh4UH//v2ZPn06vXv3LrKWqVOn4ufn57hpHzAREZHqrUqNwurZsyejRo2ic+fOXHPNNXz11VfUq1ePDz/80HHOu+++y/r161myZAnR0dG8+eabjB8/nhUrVhR53UmTJpGamuq4HTp0qCI+joiIiNNp0qQJ06ZNs7oM6wZBBwQE4OrqSmJiYqHjiYmJBAUFFesa7u7udOnShT179gBw5swZnnnmGRYtWsTNN98MQMeOHYmNjeWNN94o1N12Pk9PTzw9PUvxaURERKqXv+pqmjJlCs8//3yJr7tx40a8vb0vs6qyY1kLkIeHB2FhYURFRTmO2e12oqKi6NmzZ7GukZeXx9atWx1T4XJzc8nNzb1geqGrq2vlWXchcRukX7yLT0REpLI4duyY4zZt2jR8fX0LHXviiScc5xqGwdmzZ4t13Xr16lWKmXCWdoFFRkYyc+ZMPv74Y3bs2MHf//53MjMzHWv9jBo1ikmTJjnOf/HFF/nxxx/Zt28fMTEx3HPPPRw8eJD7778fMKfIX3PNNTz55JP89NNP7N+/nzlz5vDJJ58wZMgQSz5jIcuegQ96wW8f/vW5IiIiFjp/xrWfn1+hWdg7d+7Ex8eHpUuXEhYWhqenJ2vWrGHv3r3ceuutBAYGUqtWLbp163bBEJQ/d4HZbDb+85//MGTIEGrWrEnLli1ZsmRJuX8+S9cBGjZsGMePH2fy5MkkJCTQuXNnli1b5hgYHR8fX6g159SpU4wbN46EhARq165NWFgYa9eupW3bto5z5s2bx6RJkxgxYgQnT54kNDSUf/7zn5VjIcTGPWD9dNj8KVw7Cdw0HVJExBkZhsGZ3DxL3ruGu2uZzaR6+umneeONN2jWrBm1a9fm0KFD3HTTTfzzn//E09OTTz75hIEDBxIXF0fjxo2LvM4LL7zAa6+9xuuvv867777LiBEjOHjwIHXq1CmTOi/GZhiGUW5Xr6LS0tLw8/MjNTUVX1/fsrtwXi683R4yEmDobGh/W9ldW0REKq2srCz2799P06ZN8fLy4nTOWdpO/sGSWra/2I+aHiVr/5gzZw4TJ04kJSUFgJ9++ok+ffqwePFibr311ku+tn379jz44INMmDABMFuAJk6cyMSJEwGzBejZZ5/lpZdeAiAzM5NatWqxdOlS+vfvf8H1/vyzPF9Jvr+r1CywKs/VHbqONO9vKnqxRxERkaogPDy80OOMjAyeeOIJ2rRpg7+/P7Vq1WLHjh3Ex8df8jodO3Z03Pf29sbX19ex5UV5sXwvMKfTdTT88iYc+AWSd0NAS6srEhGRClbD3ZXtL/az7L3Lyp9ncz3xxBMsX76cN954gxYtWlCjRg2GDh1KTk7OJa/j7u5e6LHNZiv3yUsKQBXNPwRa3gi7lkH0HOj3T6srEhGRCmaz2UrcDVUV/Prrr9x7772OiUcZGRkcOHDA2qKKoC4wK4Sd29E+di7kZl36XBERkSqiZcuWfPXVV8TGxrJlyxbuvvvuyrMMzZ8oAFmh5Q3g2wjOnILtX1tdjYiISJl46623qF27Nr169WLgwIH069ePrl27Wl3WRWkW2EWU2yyw8/38Oqx6GUJ6wFhrZgKIiEjFuNTMJSkZzQKr6rqOBJsrHFpvrg4tIiIiFUYByCo+QdD6JvP+ptnW1iIiIuJkFICsFH6f+efv8yEn09paREREnIgCkJWaXgu1m0J2Gvyx0OpqREREnIYCkJVcXCDsXvO+VoYWERGpMApAVutyD7i4w9HN5k1ERETKnQKQ1bwDoO25jeQ0GFpERKRCKABVBvmDobcugKxUa2sRERFxAgpAlUFoLwhoBbmZ8Pv/rK5GRESk2lMAqgxsNgg/tz/YptmgxblFRETKlQJQZdHpLnDzgqRtcHij1dWIiIiTs9lsl7w9//zzpbr24sWLy6zWy+Fm6btLgRq1od1tsOVzc0p8SHerKxIRESd27Ngxx/358+czefJk4uLiHMdq1aplRVllRi1AlUn+YOhti+D0SWtrERERpxYUFOS4+fn5YbPZCh2bN28ebdq0wcvLi9atW/P+++87XpuTk8OECRNo0KABXl5ehIaGMnXqVACaNGkCwJAhQ7DZbI7HFU0tQJVJo3AI7ACJW2HLPOj5kNUViYhIeTAMyD1tzXu71zTHnpbC3LlzmTx5Mu+99x5dunRh8+bNjBs3Dm9vb0aPHs0777zDkiVL+N///kfjxo05dOgQhw4dAmDjxo3Ur1+f2bNn079/f1xdXcviU5WYAlBlkj8Y+rtIsxusx99L/ZdUREQqodzT8EqwNe/9zFHw8C7VJaZMmcKbb77JbbfdBkDTpk3Zvn07H374IaNHjyY+Pp6WLVty1VVXYbPZCA0Ndby2Xr16APj7+xMUFFSqOkpDXWCVTYc7wN0bTuyGA2usrkZERKSQzMxM9u7dy9ixY6lVq5bj9vLLL7N3714A7r33XmJjY2nVqhWPPPIIP/74o8VVX0gtQJWNly90vAOi50D0bGh6tdUViYhIWXOvabbEWPXepZCRkQHAzJkziYiIKPRcfndW165d2b9/P0uXLmXFihXceeed9O3blwULFpTqvcuSAlBlFH6fGYC2L4GM41CrntUViYhIWbLZSt0NZZXAwECCg4PZt28fI0aMKPI8X19fhg0bxrBhwxg6dCj9+/fn5MmT1KlTB3d3d/Ly8iqw6gspAFVGDTpBcFc4GgOxn8FVj1ldkYiIiMMLL7zAI488gp+fH/379yc7O5tNmzZx6tQpIiMjeeutt2jQoAFdunTBxcWFL7/8kqCgIPz9/QFzJlhUVBRXXnklnp6e1K5du8I/g8YAVVb5U+Kj54DdbmkpIiIi57v//vv5z3/+w+zZs+nQoQPXXHMNc+bMoWnTpgD4+Pjw2muvER4eTrdu3Thw4ADff/89Li5m7HjzzTdZvnw5ISEhdOnSxZLPYDMM7bvwZ2lpafj5+ZGamoqvr681ReRkwpttIDsV7vkKWlxvTR0iIlJqWVlZ7N+/n6ZNm+Ll5WV1OVXapX6WJfn+VgtQZeXhbW6PAeaUeBERESkzCkCVWf4GqXFLIe3Ypc8VERGRYlMAqszqt4HGPcHIg82fWl2NiIhItaEAVNmdPxg676ylpYiIiFQXlgeg6dOn06RJE7y8vIiIiGDDhg1FnjtnzhxsNluh28UGk+3YsYNBgwbh5+eHt7c33bp1Iz4+vjw/RvlpMwhq1IG0I7BnudXViIhIKWjeUemV1c/Q0gA0f/58IiMjmTJlCjExMXTq1Il+/fqRlJRU5Gt8fX05duyY43bw4MFCz+/du5errrqK1q1b89NPP/H777/z3HPPVd1R9+5e0Plu8/6m2dbWIiIilyV/heScnByLK6n6Tp82N5F1d3cv1XUsnQYfERFBt27deO+99wCw2+2EhITw8MMP8/TTT19w/pw5c5g4cSIpKSlFXvOuu+7C3d2dTz+9/DEzlWIa/PmS98B7YYANJv4O/o2trkhERErAMAzi4+PJzc0lODjYsR6OFJ9hGJw+fZqkpCT8/f1p0KDBBeeU5PvbspWgc3JyiI6OZtKkSY5jLi4u9O3bl3Xr1hX5uoyMDEJDQ7Hb7XTt2pVXXnmFdu3aAWaA+u677/i///s/+vXrx+bNm2natCmTJk1i8ODBRV4zOzub7Oxsx+O0tLTSf8CyFNACml4D+3+G6I/h+uesrkhERErAZrPRoEED9u/ff0HPhZRMWe0ib1kASk5OJi8vj8DAwELHAwMD2blz50Vf06pVK2bNmkXHjh1JTU3ljTfeoFevXmzbto1GjRqRlJRERkYGr776Ki+//DL/+te/WLZsGbfddhurVq3immuuueh1p06dygsvvFDmn7FMhY8xA9DmT+Hap8G1dE1/IiJSsTw8PGjZsqW6wUrB3d3d0Z1YWlVqL7CePXvSs2dPx+NevXrRpk0bPvzwQ1566SXs57aMuPXWW3nsMXP/rM6dO7N27VpmzJhRZACaNGkSkZGRjsdpaWmEhISU4ye5DK1uBu/6kJEIcd9D21utrkhERErIxcWl6o5JrWYs64QMCAjA1dWVxMTEQscTExOL3bTl7u5Oly5d2LNnj+Oabm5utG3bttB5bdq0ueQsME9PT3x9fQvdKh03D+g60ryvlaFFRERKxbIA5OHhQVhYGFFRUY5jdrudqKioQq08l5KXl8fWrVsdA6E8PDzo1q0bcXFxhc7btWsXoaGhZVe8VbqOBmyw7yc4sdfqakRERKosS7vAIiMjGT16NOHh4XTv3p1p06aRmZnJmDHmFhCjRo2iYcOGTJ06FYAXX3yRHj160KJFC1JSUnj99dc5ePAg999/v+OaTz75JMOGDaN379706dOHZcuW8c033/DTTz9Z8RHLVu1QaNHXXA8oeg7c+JLVFYmIiFRJlgagYcOGcfz4cSZPnkxCQgKdO3dm2bJljoHR8fHxhaYKnjp1inHjxpGQkEDt2rUJCwtj7dq1hbq8hgwZwowZM5g6dSqPPPIIrVq1YuHChVx11VUV/vnKRfh9ZgDa/Blc9yy4eVpdkYiISJVj6TpAlVWlWwfofHln4d8dzZWhb/sPdLzD6opEREQqhZJ8f2slpqrG1Q26jjLvazC0iIjIZVEAqoq6jgKbK8SvhaSLr5kkIiIiRVMAqop8g6HVAPN+tPYHExERKSkFoKoqzJwpR+wXkHPa2lpERESqGAWgqqr5deamqNmpsG2R1dWIiIhUKQpAVZWLS0ErkAZDi4iIlIgCUFXW5R5wcYcjm+DYFqurERERqTIUgKqyWvWhzS3m/U0aDC0iIlJcCkBVXfh95p9bv4TsdGtrERERqSIUgKq6JldD3RaQk2GGIBEREflLCkBVnc1WeDC0djYRERH5SwpA1UHnu8HVExK2wpEYq6sRERGp9BSAqoOadaDdEPO+psSLiIj8JQWg6iJ/MPQfC+FMiqWliIiIVHZuVhcgZSSkO9RvC0nb4ff5EPE3qysSEal0DMPg1Olcjqac4WjKGY6lZnE09QxHU7I4lnKGzJw8mgV40zKwFi3r+3BFYC2aBHjj7qr2gupGAai6sNnMVqDvnzC7wbo/YB4TEXEiGdlnOZZyhqOpWWbAOXf/WH7IST1DVq79ktfYcSwNthY8dnOx0TTAmysCfWhRvxZXBPrQMrAWTep64+GmYFRVKQBVJx3vhOWT4fhOiF8Hob2srkhEpMxkn80jITXLEWSOpWZx5FzIOXYu8KRlnS3WtQJqeRLs70WwXw0anPdnTQ9X9iZlsisxnd1JGexJyiAj+yy7kzLYnZRR6BpuLjaaBHhzxbnWopaBZjhSMKoaFICqEy8/aH87bP7UbAVSABKRKiLPbnA8Pftcd9QZjqVkFdw/F3qSM7KLdS1fLzeC/WvQwM+LBv41aJh/38+8H+jniaeba5Gvv651wX3DMDiWmmUGosQMdielsyuxIBjtOReSIMHxmvODUYtz3Wgt6/vQNEDBqDKxGYYWjvmztLQ0/Pz8SE1NxdfX1+pySuZIDMzsA64eELkTvOtaXZGIOLnzx93kt9QcTTVDTn7XVGJaFmftf/115OnmQrB/DYL9zUAT7Odlhh1/834D/xrU8iz/3+3zg9HupAx2J6Y7Wox2J5rB6GJcXWw0qVvzXBeaDy3PdacpGJWdknx/KwBdRJUOQAAfXgPHYuGGl+DKR6yuRkSquczssxxLPcORcwOJHeNvUgtacv5q3A2YASHI18vRcuPoojoXcoL9a1C7pju2Sjy+0TAMEtKy2JVoBqPdiRnsSkpnT2IG6cUJRvVrmeEosBZNA7wv2VIlF1IAKqUqH4CiP4ZvHoE6zWBCNLjoNwsRuTzZZ/NITM3+U3fUeS05JRp341HQNXWuO6pBfkuOvxf1fbxwdam84aY08oPR7sSMQt1pu/8iGIXWrckV57rRWgSafyoYFU0BqJSqfADKzoA3W0NOOoz6Gppda3VFIlIJnT/uJr876kjKeV1TqVkcTy/euBsfL7eCAcX53VF+NRzdVYG+Xni560v7zwzDIDEtm13nutH2JBUEpOIEo5aBtRzdac3qKRgpAJVSlQ9AAN89Dhv/A21vhTs/sboaEalghmGQcjq3YI2bc3/md02VZNyNh5vLnwYSm11U+V1TDfy88PFyr4BP5Tzyg1H+oOvd58YY7UpMJ72IFjdXFxuhdWo6ZqO1OG+MkbOETwWgUqoWASjhD5hxJbi4wWPbwCfI6opEpAzlj7s56mi5ySo0Hbwk424CfTzPjbnJb7kpGHPTwM+LOt4elXrcjTMxDIOk9PwWowz2nAtIlwpGLjZoUrdgccf8P5vVq37BSAGolKpFAAL4zw1weANc9yz0ftLqakSkhAzDYO/xTNbtO0FcQtp5LThZpJ7JLdY1Amp50KDQQOKCMTfB/jWoV8sTN61yXOXlByPHGKNz44t2JaYXOUbLxQahdb0ds9GqQzBSACqlahOAYr+AxQ+CX2N4NBZcquZfaBFnYRgG8SdPs27vCdbuPcH6fSdIusQYHB9PN8eYm8JTws3ZU0F+Gnfj7AzDHOe1yxGMCqbtFycYnd+d1rxerUr/90kBqJSqTQDKPWMOhs5Kgbu/hCtutLoiEfmTw6fMwLNu3wnW7z3B0dSsQs97uLkQ1rg2XUP9aehfkwb+Xo7xOBp3I5fr/GBUsLij+WdRrYsuNmhcpyYtA30KrX5dmYKRAlApVZsABLDsGVg/Ha4YAHfPs7oaEaeXmJZlBp5zoSf+5OlCz7u72ugSUpsezevSs1ldujT2rzRfLlL95Qej/AHXBS1GxQtG+d1pLerXokX9ig9GCkClVK0C0PFdML0b2Fxg4lbwa2R1RSJOJTkjm/X7TjhCz77kzELPu7rY6NjIj57N6tKzeV3CQ+tQw0OBRyoXwzA4nmGOMdqdmM6upAz2nFvkMeX0xYORLT8Y5W8Hcq7VqDyDkQJQKVWrAAQw5xY48Atc8xT0ecbqakSqtZTTOazfd5J1e5NZt+8EuxILb6Bps0H7YD96nmvh6da0ToVs3SBSHvKD0Z5CY4yKF4xGRDTmgd7Ny7Seknx/V4r/66ZPn87rr79OQkICnTp14t1336V79+4XPXfOnDmMGTOm0DFPT0+ysrIuev6DDz7Ihx9+yNtvv83EiRPLuvSqIXyMGYCiPzZng7lq3IBIWUnLymXDvpOsO9fKsyMhjT//Wtk6yIeezevSq3kA3ZvWwa+G/h+U6sFms1Hfx1zFu1eLAMdxwzBIzsgptH5RfnfaqdO5HDxxuljLNJQnywPQ/PnziYyMZMaMGURERDBt2jT69etHXFwc9evXv+hrfH19iYuLczwuan2KRYsWsX79eoKDg8ul9iqj9UCoGQAZCbBrGbQZaHVFIlVWZvZZNh446Ri0vPVIKn9eS7Bl/VqOFp6IZnWp4+1hTbEiFrHZbNTz8aSej+fFg1FSOg39a1hYYSUIQG+99Rbjxo1ztOrMmDGD7777jlmzZvH0009f9DU2m42goEsv7HfkyBEefvhhfvjhB26++eYyr7tKcfOALvfAr9Ng02wFIJESyMrNI/rgKceg5S2HUi5YPblpgDc9zo3h6dGsDvV9vCyqVqRyOz8YWc3SAJSTk0N0dDSTJk1yHHNxcaFv376sW7euyNdlZGQQGhqK3W6na9euvPLKK7Rr187xvN1uZ+TIkTz55JOFjhclOzub7OyCtTbS0tIu8xNVYmH3mgFobxSc3A91mlpdkUillH02j9j4FEeX1ub4FHLyCjfVN6pdwzFouWfzujTws/Y3WREpOUsDUHJyMnl5eQQGBhY6HhgYyM6dOy/6mlatWjFr1iw6duxIamoqb7zxBr169WLbtm00amTOcPrXv/6Fm5sbjzzySLHqmDp1Ki+88ELpPkxlV6cpNL/eDEDRc+CGav55RYopN8/O74dTHYOWow+eumBsQpCvlyPs9GxWl5A6NS2qVkTKiuVdYCXVs2dPevbs6Xjcq1cv2rRpw4cffshLL71EdHQ0//73v4mJiSn23jWTJk0iMjLS8TgtLY2QkJAyr91y4WPMALT5M+jzD7NrTMTJ5NkN/jiS6mjh2XjgJKdz8gqdE1DLgx7NzEHLPZvXpUndmtoLS6SasTQABQQE4OrqSmJiYqHjiYmJfznGJ5+7uztdunRhz549APzyyy8kJSXRuHFjxzl5eXk8/vjjTJs2jQMHDlxwDU9PTzw9re+PLHdX9AefBpB+DHZ+A+1vt7oikXJntxvsSEhj3bmtJX7bf/KCTSP9a7oXdGk1q0uL+rUUeESqOUsDkIeHB2FhYURFRTF48GDAHL8TFRXFhAkTinWNvLw8tm7dyk033QTAyJEj6du3b6Fz+vXrx8iRIy+YPu90XN2h6yj4+V/mYGgFIKmGDMNgd1KGY+HB9ftPXLAeiY+XGxFNCwJP6yAfXFwUeEScieVdYJGRkYwePZrw8HC6d+/OtGnTyMzMdISVUaNG0bBhQ6ZOnQrAiy++SI8ePWjRogUpKSm8/vrrHDx4kPvvvx+AunXrUrdu3ULv4e7uTlBQEK1atarYD1cZdR0Fq1831wU6vgvqXWF1RSKlYhgG+5PNHdPX7j3Bb/tOkJyRU+gcbw9XujWt42jlaRfsh6sCj4hTszwADRs2jOPHjzN58mQSEhLo3Lkzy5YtcwyMjo+Px8XFxXH+qVOnGDduHAkJCdSuXZuwsDDWrl1L27ZtrfoIVYtfI2jZD3YtNQdD93/F6opESuyQY8d0c+ByYlrhHdO93F0ID63jGLjcoaEf7q4uRVxNRJyRtsK4iGq3Fcaf7foRPr8DvPzh8Z3grim8UrkdTTnjWIdn3d4THEk5U+h5D1cXuob607OZOWi5U4gfnm7aT0vE2VS5rTCkgrW4HvwaQ2o8bFsMnYdbXZFIIUnpWY5By+v2nuDAicI7pru52Ogc4u8Yw9M1tLZ2TBeRElEAckYurhA2Cla+DNGzFYDEciczcxxhZ+3eZPYeL7xjuosNOjTyP2/H9Np4awNRESkF/QvirLqMhJ9ehUO/QcIfENTe6orEiaSezuW3/eag5fX7TrAzIb3Q8zYbtG3g6wg83ZrWwddLG4iKSNlRAHJWPkHQ+mbY/rXZCnTzm1ZXJNVYelauuYHouXE8245euGN6q0Afx6DliKZ18K+phTpFpPwoADmzsDFmANoyH/q+AJ61rK5IqonTOWfZdOCUY9Dy1iOp5P1pA9Hm9bzPjeEJoEezOtSt5QSLkYpIpaEA5MyaXgN1msHJffDHQggbbXVFUkVl5eYRE3+K9edaeGIPpZCbVzjwhNat6ejS6tGsLoG+2jFdRKyjAOTMXFzMVqDlz8GmWQpAUiJpWbnM33CIlTuTiI4/Rc7ZwhuINvSvQY/zdkxv6K/lFkSk8lAAcnadR8DKl+BYLByJgYZdra5IKrmTmTnM/nU/c9YeKLSnVn0fT3o2r0uvc91aIXVqaD8tEam0FICcnXddaHsrbP3SHAytACRFSEjN4qPV+/hiQzxncs3d01vWr8XInqFc2SKAZgHeCjwiUmUoAAmE32cGoK0L4MaXwcvP6oqkEjl4IpMZP+9lQfRhx7ieDg39GN+nBTe2DdQmoiJSJSkACTTuCfVaw/Gd8Pv/oPs4qyuSSiAuIZ33f9rDN1uOkj+BK6JpHcb3acHVLQPU2iMiVZoCkJirzoWNgWVPmYOhu91vHhOnFHsohemr9rB8e6LjWJ9W9RjfpwXhTepYWJmISNlRABJTp2Gw4nlI2g6HNkDjCKsrkgpkGAbr9p3g/VV7WbMnGTAz8E3tG/D3a5vTvqG6RUWkelEAElON2tD+doj9zGwFUgByCoZhsHJnEtNX7SEmPgUwNxod3KUhD17TnBb1tTimiFRPCkBSIPw+MwBtWwT9p0JNdXdUV3l2g++3HmP6qj2Ofbg83Fy4q1sID/RuRqPaNS2uUESkfCkASYGGXSGoAyRshS1fQM/xVlckZSznrJ3Fm4/wwc972Z9s7rju7eHKPT1DGXtVU+r7aHVmEXEOCkBSwGYzW4G+fczsBuvxkAZDVxNncvKYtzGej1bv41hqFgD+Nd0Z06sp9/Zqgl9N7bQuIs5FAUgK63AH/PgcnNgDB36Bpr2trkhKIS0rl0/XHWTWmv2cyMwBzBWbx13djLsjGuPtqX8CRMQ56V8/KczTxwxB0bPNViAFoCrpZGYOs9bs5+N1BdtVhNSpwYPXNOf2ro3wcne1uEIREWspAMmFwseYAWjHt5CRBLXqW12RFNOx1DPMXL3/gu0qHurTnIEdg3FzdbG4QhGRykEBSC7UoBM0DIcjm2DzZ3B1pNUVyV84kJzJh6u1XYWISHEpAMnFhd9nBqDoOXDlRHBRy0FlpO0qREQujwKQXFy7IbBsEqQchH0roUVfqyuS82i7ChGR0lEAkovzqAmdh8NvM2DTbAWgSiB/u4rpq/bw654TgLarEBG5XApAUrSwMWYAilsKqUfAr6HVFTml/O0q3lu1h83arkJEpEwoAEnR6reGxr0gfi1s/hSufdrqipxKnt3gu63HeF/bVYiIlDkFILm08PvMABTzCVz9BLjqr0x5yzlrZ9Hmw3zw014OnDgNaLsKEZGypm8zubS2g2BZXUg7Art/hNY3WV1RtaXtKkREKo4CkFyamyd0HgFr3zFXhlYAKnParkJEpOLpX1b5a2H3mgFozwo4dRBqh1pdUbVwIiOb2b8e0HYVIiIWUACSv1a3OTS7Fvb9BDEfw/WTra6oStN2FSIi1qsU/9JOnz6dJk2a4OXlRUREBBs2bCjy3Dlz5mCz2QrdvLwKBoXm5uby1FNP0aFDB7y9vQkODmbUqFEcPXq0Ij5K9RV+n/lnzKdwNsfaWqqoA8mZTPrqd3q/topZv+7nTG4eHRr6MeOeMH6Y2JshXRop/IiIVBDLW4Dmz59PZGQkM2bMICIigmnTptGvXz/i4uKoX//im3D6+voSFxfneHz+cv+nT58mJiaG5557jk6dOnHq1CkeffRRBg0axKZNm8r981RbrW6CWoGQkQhx35krRUux7ExI4/1Ve/n294LtKro3rcMEbVchImIZm2EYhpUFRERE0K1bN9577z0A7HY7ISEhPPzwwzz99IXrzsyZM4eJEyeSkpJS7PfYuHEj3bt35+DBgzRu3Pgvz09LS8PPz4/U1FR8fX2L/T7VXtRL8Msb0PQaGL3E6moqvdhDKby3cg8rdhTeruKhPi3opu0qRETKXEm+vy1tAcrJySE6OppJkyY5jrm4uNC3b1/WrVtX5OsyMjIIDQ3FbrfTtWtXXnnlFdq1a1fk+ampqdhsNvz9/S/6fHZ2NtnZ2Y7HaWlpJf8wziBsNPzyJuz/GZL3QEALqyuqdAzDYN3eE0z/SdtViIhUZpYOOEhOTiYvL4/AwMBCxwMDA0lISLjoa1q1asWsWbP4+uuv+eyzz7Db7fTq1YvDhw9f9PysrCyeeuophg8fXmQanDp1Kn5+fo5bSEhI6T5YdeXfGFreaN6Pnm1tLZWMYRis2J7IbR+s5e7//Mave07g5mJjaFgjlj92DdNHdFX4ERGpRCwfA1RSPXv2pGfPno7HvXr1ok2bNnz44Ye89NJLhc7Nzc3lzjvvxDAMPvjggyKvOWnSJCIjIx2P09LSFIKKEj4Gdv8AsZ/Ddc+Bu3OvSqztKkREqiZLA1BAQACurq4kJiYWOp6YmEhQUFCxruHu7k6XLl3Ys2dPoeP54efgwYOsXLnykn2Bnp6eeHp6lvwDOKOWN4JvI0g7DDuWQMc7ra7IEtquQkSkarO0C8zDw4OwsDCioqIcx+x2O1FRUYVaeS4lLy+PrVu30qBBA8ex/PCze/duVqxYQd26dcu8dqfl4mqOBQJzZWgncyYnj9m/7uea11fx1MKtHDhxGv+a7jzW9wrWPn09kwa0UfgREakCLO8Ci4yMZPTo0YSHh9O9e3emTZtGZmYmY8aMAWDUqFE0bNiQqVOnAvDiiy/So0cPWrRoQUpKCq+//joHDx7k/vvvB8zwM3ToUGJiYvj222/Jy8tzjCeqU6cOHh4e1nzQ6qTLSPjpVYhfB0k7oH4bqysqd9quQkSkerH8X+1hw4Zx/PhxJk+eTEJCAp07d2bZsmWOgdHx8fG4uBQ0VJ06dYpx48aRkJBA7dq1CQsLY+3atbRt2xaAI0eOsGSJOUW7c+fOhd5r1apVXHvttRXyuao13wbQagDs/BY2zYabXrO6onKj7SpERKony9cBqoy0DlAx7ImCz24DTz94fAd4eFtdUZk6lnqGj1bv44sN8WTl2gFtVyEiUtlVmXWApApr1gdqN4FTB+CPr6DrSKsrKhMHkjOZ8fNeFsYcJjfP/N2gQ0M/xvdpwY1tA3Fx0arNIiLVgQKQXB4XF3OX+BXPm2sCVfEApO0qRESciwKQXL7O98DKf8KRaDgaC8Gdra6oxDbHn2L6qr3arkJExMkoAMnlq1UP2g6CPxaarUDB/7a6omLRdhUiIqIAJKUTNsYMQFsXwI0vg6eP1RUVyTAMonYkMf2nPWyOTwHAzcXG4C4NefCa5rSoX8vaAkVEpMIoAEnpNLkK6raEE7vh9/9Bt7FWV3QBbVchIiJ/pgAkpWOzQfh98MMkc02g8PvMY5WAtqsQEZGiKABJ6XW6C6JegMStcHgThHSztJyU0zksjDnCf37Zx7HULAD8a7ozpldT7u3VBL+a7pbWJyIi1lMAktKrWQfaDYEtX5iDoS0IQGfz7KzefZwF0YdZsT2JnDxz8UJtVyEiIhejbwQpG+H3mQHoj4XQ759Qo3aFvG1cQjoLYw7zVcwRkjOyHcdbB/lwT49QhoZpuwoREbmQApCUjUbdILA9JP4BW+ZBj7+X21udysxhyZajLIg+zNYjqY7jdbw9uLVzMEPDGtEuWFPZRUSkaApAUjZsNnNl6O+fMAdDRzxYpoOhc/Ps/Bx3nIUxh1mxI9GxTYWbi43rWtdnaFgjrm1VHw837dElIiJ/rcQBqEmTJtx3333ce++9NG7cuDxqkqqq4zBYPgWS4+DgWmhyZakvuTMhjQWbDrM49gjJGTmO420b+DI0rBG3dg6mbi3PUr+PiIg4lxIHoIkTJzJnzhxefPFF+vTpw9ixYxkyZAienvoScnpevtBhKMR8DJtmXXYAOpmZw9exR1gYc5g/jqQ5jtf19mBwl4bc3rURbYMvvcuviIjIpdgMwzAu54UxMTHMmTOHL774gry8PO6++27uu+8+unbtWtY1Vri0tDT8/PxITU3F11dftCVydDN8dC24uMPjO8E7oFgvy82z81PccRZEH2LlziRHF5e7a34XVwjXtqqHu6u6uERE5OJK8v192QEoX25uLu+//z5PPfUUubm5dOjQgUceeYQxY8ZU2R20FYBK6aNrzSB0w4tw5aOXPHX70TQWxhxm8eYjnMgs6OJq39CXoV0bMahzQ+p4e5RzwSIiUh2U5Pv7sgdB5+bmsmjRImbPns3y5cvp0aMHY8eO5fDhwzzzzDOsWLGCzz///HIvL1VZ+H2w5GFzMHTPh8GlcKvNiYxsvo41Z3FtP1bQxRVQy4PBnRtye1gj2jRQ8BQRkfJT4gAUExPD7Nmz+eKLL3BxcWHUqFG8/fbbtG7d2nHOkCFD6NbN2tWAxULtb4cf/gGn9sP+n6D5deTm2Vm1M4kF0YdZuTOJs/aCLq6+bQIZGtaI3leoi0tERCpGiQNQt27duOGGG/jggw8YPHgw7u4XbivQtGlT7rrrrjIpUKogD29zRtjGmaSu+Yhp2wNZEnu0UBdXh4Z+DA1rxKBOwdRWF5eIiFSwEo8BOnjwIKGhoeVVT6WgMUClk5yRzeo1P3Pb+js4a7jQK/tdkqhNQC1PbutqzuJqFeRjdZkiIlLNlOsYoKSkJBISEoiIiCh0/LfffsPV1ZXw8PCSXlKqgZyzdlbuTGJhzGFWneviCvG4gm4uu3i24SZq3TCJ3i3r4aYuLhERqQRK/G00fvx4Dh06dMHxI0eOMH78+DIpSqoGwzD440gqzy/ZRsQrK3jws2iWb0/krN2gUyM/sjuNBmBQ7nKuuyJA4UdERCqNErcAbd++/aJr/XTp0oXt27eXSVFSuR1Pz+br2CMsiD7MzoR0x/H6Pp4M6WLO4roi0Adyw2HPG5B2GPasgCv6WVi1iIhIgRIHIE9PTxITE2nWrFmh48eOHcPNTVuLVVdmF1ciC6IPsyruOHnnZnF5uLpwQztzFtfVLf7UyuPuBZ1HwLr3zJWhFYBERKSSKHFiufHGG5k0aRJff/01fn7mjtspKSk888wz3HDDDWVeoFjH7OJKY0H0IZZsOcqp07mO5zqH+DM0rBEDOwbjV/PCmYAOYfeaAWj3j5ByCPxDyr9wERGRv1DiAPTGG2/Qu3dvQkND6dKlCwCxsbEEBgby6aeflnmBUvGS0rP4erO5UGFcYkEXV6CvJ0O6NGJoWENa1C/mLK6AltDkajjwC8R8Atf9o5yqFhERKb7L2gojMzOTuXPnsmXLFmrUqEHHjh0ZPnz4RdcEqoqccRp89tk8onYksTD6MD/tOq+Ly82Ffu2CGBrWiKtaBODqchnbm/zxFSwYA7WC4LE/wLV6/D0REZHKpdy3wvD29uaBBx64rOKk8jAMg61HUlkQfZivY4+Seqagi6tLY7OL65aOwfjVKGVgaX0LeNeDjASIWwptB5WychERkdK57FHL27dvJz4+npycnELHBw3Sl1tll5SWxaLNR1gYc5hdiRmO40G+XuZChWGNaF6vVtm9oZsHdBkJa94yB0MrAImIiMVKHID27dvHkCFD2Lp1KzabjfwetPyd3/Py8sq2QikTWblmF9eC6EP8vOs453q48Dyvi+vKy+3iKo6w0bDmbdi3Ck7ugzrN/vo1IiIi5aTEK9M9+uijNG3alKSkJGrWrMm2bdtYvXo14eHh/PTTT5dVxPTp02nSpAleXl5ERESwYcOGIs+dM2cONput0M3Ly6vQOYZhMHnyZBo0aECNGjXo27cvu3fvvqzaqjLDMIg9lMKzi7cS8UoU4z+PYVWcGX7CQmsz9bYObHy2L+8M70LvK+qVX/gBqN0EWlxv3o+eU37vIyIiUgwlbgFat24dK1euJCAgABcXF1xcXLjqqquYOnUqjzzyCJs3by7R9ebPn09kZCQzZswgIiKCadOm0a9fP+Li4qhfv/5FX+Pr60tcXJzjcX7rU77XXnuNd955h48//pimTZvy3HPP0a9fP7Zv335BWKqOEs91cS2IPsyepIIurgZ+Xo69uJqVZRdXcYXfZy6IuPkz6PMPcPOs+BpERES4jACUl5eHj485BTogIICjR4/SqlUrQkNDC4WS4nrrrbcYN24cY8aMAWDGjBl89913zJo1i6effvqir7HZbAQFBV30OcMwmDZtGs8++yy33norAJ988gmBgYEsXry42u5Sn5Wbx/LtiSyMOczqP3VxDWgfxNCwEHo2r1u+rTx/pWU/8AmG9KOw4xvoMNS6WkRExKmVOAC1b9+eLVu20LRpUyIiInjttdfw8PDgo48+umB16L+Sk5NDdHQ0kyZNchxzcXGhb9++rFu3rsjXZWRkEBoait1up2vXrrzyyiu0a9cOgP3795OQkEDfvn0d5/v5+REREcG6deuqVQDK7+JaEH2Yb7YcJS3rrOO58NDaDA1rxE0dG+DrVUmmnbu6QddR8POrsGm2ApCIiFimxAHo2WefJTMzE4AXX3yRW265hauvvpq6desyf/78El0rOTmZvLw8AgMDCx0PDAxk586dF31Nq1atmDVrFh07diQ1NZU33niDXr16sW3bNho1akRCQoLjGn++Zv5zf5adnU12drbjcVpaWok+R0VLSM3iq82HWRh9mL3HMx3Hg/28uD2sEbd1bUTTAG8LK7yErqNg9WtwcA0cj4N6rayuSEREnFCJA1C/fgX7ObVo0YKdO3dy8uRJateufcFYnPLQs2dPevbs6Xjcq1cv2rRpw4cffshLL710WdecOnUqL7zwQlmVWC6ycvP4cbu5F9ea3QVdXF7uLgxo34ChYY3o2awuLlZ2cRWHX0O4YgDEfWe2Ag141eqKRETECZUoAOXm5lKjRg1iY2Np376943idOnUu680DAgJwdXUlMTGx0PHExMQix/j8mbu7O126dGHPnj0AjtclJibSoEGDQtfs3LnzRa8xadIkIiMjHY/T0tIICbF+zyrDMIiJN7u4vv39KOnndXF1b1KHoWGNGNAhCJ/K0sVVXOFjzAC05XPoOwXca1hdkYiIOJkSBSB3d3caN25cZmv9eHh4EBYWRlRUFIMHDwbAbrcTFRXFhAkTinWNvLw8tm7dyk033QRA06ZNCQoKIioqyhF40tLS+O233/j73/9+0Wt4enri6Vl5ZiQdSz3DVzFHWBh9mH3JBV1cDf1rcHvXhtzWtRFNKmsXV3E0vw78G0NKPGxbBJ3vtroiERFxMiXuAvvHP/7BM888w6effnrZLT/ni4yMZPTo0YSHh9O9e3emTZtGZmamY1bYqFGjaNiwIVOnTgXMcUc9evSgRYsWpKSk8Prrr3Pw4EHuv/9+wJwhNnHiRF5++WVatmzpmAYfHBzsCFmV0ZmcPH7cnmB2ce1JJn+HthrurgzoYC5U2KNpFejiKg4XV3OX+KgXzZWhFYBERKSClTgAvffee+zZs4fg4GBCQ0Px9i7cEhETE1Oi6w0bNozjx48zefJkEhIS6Ny5M8uWLXMMYo6Pj8fFpWC9xlOnTjFu3DgSEhKoXbs2YWFhrF27lrZt2zrO+b//+z8yMzN54IEHSElJ4aqrrmLZsmWVbg0gs4vrlNnFteUY6dnndXE1Nbu4burQgFqel71jSeXV+R5Y9Qoc3ggJWyGog9UViYiIEynxbvB/NVh4ypQppSqoMijv3eCPpJxhUcxhFsYcYf95XVyNatfg9q6NuL1rIxrXrVnm71vp/G80bF8M4WPhlresrkZERKq4knx/lzgAOYPyCkBROxKZ/esBft1b0MVV08PVMYsrommd6tHFVVz7foZPBoFHLXh8J3j6WF2RiIhUYSX5/q6GfSuV17ajaazZkwxAj2Z1GBoWwoD2QXhXxy6u4mjaG+q2gBN7YOsCc3aYiIhIBSjxN6+Li8sl1/vRbvBFu61rQ+yGwe1dGxFSxwm6uP6KzWYOhv7xWXMwdNi95jEREZFyVuIAtGjRokKPc3Nz2bx5Mx9//HGlX0zQao1q12Ri3yusLqNy6XQ3RL0ECb/D0RhoGGZ1RSIi4gRKHIDyNxg939ChQ2nXrh3z589n7NixZVKYOAnvutBuMPw+32wFUgASEZEK4PLXpxRPjx49iIqKKqvLiTMJOzf254+v4EyKpaWIiIhzKJMAdObMGd555x0aNmxYFpcTZ9O4B9RrA7mn4ff/WV2NiIg4gRJ3gf1501PDMEhPT6dmzZp89tlnZVqcOAmbDcLvg6VPmt1g3cdpMLSIiJSrEgegt99+u1AAcnFxoV69ekRERFC7du0yLU6cSKdhsGIKHN8B8eshtKfVFYmISDVW4gB07733lkMZ4vS8/KD9bbD5M4ierQAkIiLlqsRjgGbPns2XX355wfEvv/ySjz/+uEyKEicVfp/557bFkHnC0lJERKR6K3EAmjp1KgEBARccr1+/Pq+88kqZFCVOKrgrNOgEedmw5XOrqxERkWqsxAEoPj6epk2bXnA8NDSU+Pj4MilKnJTNVjAlftNs0DZ1IiJSTkocgOrXr8/vv/9+wfEtW7ZQt27dMilKnFiHoeDhAyf3wv7VVlcjIiLVVIkD0PDhw3nkkUdYtWoVeXl55OXlsXLlSh599FHuuuuu8qhRnImnD3S807y/aZa1tYiISLVV4llgL730EgcOHOD666/Hzc18ud1uZ9SoURoDJGUjfAxs+i/s/BbSE8En0OqKRESkmrEZxuUNtNi9ezexsbHUqFGDDh06EBoaWta1WSYtLQ0/Pz9SU1Px9fW1uhzn9J++cHgjXD8Zrn7c6mpERKQKKMn3d4lbgPK1bNmSli1bXu7LRS4t/D4zAEXPgSsngour1RWJiEg1UuIxQLfffjv/+te/Ljj+2muvcccdd5RJUSK0G2IujpgSD3tXWl2NiIhUMyUOQKtXr+amm2664PiAAQNYvVqzdqSMuNeATneb9zfNtrYWEXEeuVlWVyAVpMQBKCMjAw8PjwuOu7u7k5aWViZFiQDmYGiAXUsh9Yi1tYhI9ZZzGj4fBq83hwNrrK5GKkCJA1CHDh2YP3/+BcfnzZtH27Zty6QoEQDqtYLQq8CwQ8wnVlcjItVVdgZ8fifsWgY5GfDNo3A22+qqpJyVeBD0c889x2233cbevXu57rrrAIiKiuLzzz9nwYIFZV6gOLnwMXBwDcR8DL2fBNfLHrcvInKh7HSYewfErzMXYXXzhBN74Nd/wzX/Z3V1Uo5K3AI0cOBAFi9ezJ49e3jooYd4/PHHOXLkCCtXrqRFixblUaM4szYDoWZdSD8Gu3+wuhoRqU6yUuHTIWb48fSDUYthwLlJPqvfgJP7LC1PyleJAxDAzTffzK+//kpmZib79u3jzjvv5IknnqBTp05lXZ84OzdP6HKPeV8rQ4tIWTlzCj4ZbC634eUPo7+GRuHQ/nZodq25KfP3T2pPwmrssgIQmLPBRo8eTXBwMG+++SbXXXcd69evL8vaRExh95p/7omCUwesrEREqoPTJ+HjQXA0BmrUgdHfQHAX8zmbDW56E1w9YM8K2P61tbVKuSlRAEpISODVV1+lZcuW3HHHHfj6+pKdnc3ixYt59dVX6datW3nVKc6sTjNo1gcwIPpjq6sRkaos4zjMuQUSfgfvenDvd9CgY+FzAlrAVY+Z95dNMscJSbVT7AA0cOBAWrVqxe+//860adM4evQo7777bnnWJlIg/D7zz82fwtkca2sRkaopPRE+vgWStkGtQDP8BBYxe/mqSKjdFNKPwqqpFVunVIhiB6ClS5cyduxYXnjhBW6++WZcXbU1gVSgVgOgVhBkHjc3SRURKYm0YzDnZji+E3yC4d7vzaU2iuLuBTe/Yd7/bQYkbK2YOqXCFDsArVmzhvT0dMLCwoiIiOC9994jOTm5PGsTKeDqDl1Hmfc1GFpESiL1MMy5CU7sBr8QGPOd2c31V1r0hbaDwciDbx8Du73cS5WKU+wA1KNHD2bOnMmxY8f429/+xrx58wgODsZut7N8+XLS09VHKuWs6yiwucCBXyB5t9XViEhVcOogzL7JnNLu39js9qrTrPiv7z8VPGqZs8ViNAaxOinxLDBvb2/uu+8+1qxZw9atW3n88cd59dVXqV+/PoMGDSpxAdOnT6dJkyZ4eXkRERHBhg0bivW6efPmYbPZGDx4cKHjGRkZTJgwgUaNGlGjRg3atm3LjBkzSlyXVEL+IdDyRvN+9BxLSxGRKuDkPrPbK+WgOZ5nzFKoHVqya/gGQ59/mPdXPG8OopZq4bKnwQO0atWK1157jcOHD/PFF1+U+PXz588nMjKSKVOmEBMTQ6dOnejXrx9JSUmXfN2BAwd44oknuPrqqy94LjIykmXLlvHZZ5+xY8cOJk6cyIQJE1iyZEmJ65NKKH8wdOxcbVooIkU7sRdm3wyph6BuCxjzPfg1urxrdX8AgjpAVgosn1ymZYp1ShWA8rm6ujJ48OASh4y33nqLcePGMWbMGEdLTc2aNZk1q+gxHnl5eYwYMYIXXniBZs0ubMZcu3Yto0eP5tprr6VJkyY88MADdOrUqdgtS1LJtehr9uGfOaX1OUTk4o7Hmd1e6UehXmtzwLNv8OVfz9UNbn4bsMGWz+HAr2VWqlinTALQ5cjJySE6Opq+ffsWFOPiQt++fVm3bl2Rr3vxxRepX78+Y8eOvejzvXr1YsmSJRw5cgTDMFi1ahW7du3ixhtvLPKa2dnZpKWlFbpJJeXiCl1Hm/c1GFpE/ixxu9ntlZEA9dvB6G/BJ7D01w3pBmHn/u35LlLLcVQDlgWg5ORk8vLyCAws/BczMDCQhISEi75mzZo1/Pe//2XmzJlFXvfdd9+lbdu2NGrUCA8PD/r378/06dPp3bt3ka+ZOnUqfn5+jltISMjlfSipGF1Hgs0VDq03/7ETEQFzqvqcm83lMoI6wr3fQq16ZXf966dAzQBzKv366WV3XbGEZQGopNLT0xk5ciQzZ84kICCgyPPeffdd1q9fz5IlS4iOjubNN99k/PjxrFixosjXTJo0idTUVMft0KFD5fERpKz4BEHrm8370bOtrUVEKoejm80Vns+cNLe1GL0EatYp2/eoWQdufNm8//NrkBJftteXCuVm1RsHBATg6upKYmJioeOJiYkEBQVdcP7evXs5cOAAAwcOdByzn1uTwc3Njbi4OIKDg3nmmWdYtGgRN99sfkF27NiR2NhY3njjjULdbefz9PTE09OzrD6aVITwMbBjCWyZB32fBw9vqysSEasc3gSf3gbZqdCoG9yzELz8yue9Ot0Fmz+Dg2tg6VMwvOQTgKRysKwFyMPDg7CwMKKiohzH7HY7UVFR9OzZ84LzW7duzdatW4mNjXXcBg0aRJ8+fYiNjSUkJITc3Fxyc3NxcSn8sVxdXR1hSaqJptea01qz0+CPhVZXIyJWiV9v7uqenQqNe8I9X5Vf+AFzs9Sb3wQXN4j7HnZ+X37vJeXKshYgMKesjx49mvDwcLp37860adPIzMxkzJgxAIwaNYqGDRsydepUvLy8aN++faHX+/v7AziOe3h4cM011/Dkk09So0YNQkND+fnnn/nkk0946623KvSzSTlzcTFbgZZPNgdD568SLSLO48CvMPcOyM2EJlfD8HngWav837d+a+j1MKx5G5b+HzS7Rq3QVZClAWjYsGEcP36cyZMnk5CQQOfOnVm2bJljYHR8fPwFrTl/Zd68eUyaNIkRI0Zw8uRJQkND+ec//8mDDz5YHh9BrNR5BKx82ez7P7rZ7PcXEeew72f44i7IPQ3NroW7vgCPmhX3/r3/D7YuhNR4czzQDS9U3HtLmbAZhmFYXURlk5aWhp+fH6mpqfj6+lpdjlzKgrHwxwJzavygd6yuRkQqwp4omHc3nM0y1wYb9hm416j4OuKWmiHMxQ0eXAP121R8DVJISb6/q8wsMJGLyl8ZeusCyNL6TSLV3q4fzNBxNguuGAB3fW5N+AFoNQBa3Qz2s/Dd46D2hCpFAUiqttBeENDKHAOw9X9WVyMi5WnndzBvBOTlQOtb4M5PwM3iGbwD/gXuNeHgr7BFM8KqEgUgqdpsNnMwNMDGWfoNTKS62v41/G8U2HOh7WC4Yw64eVhdlblJ8zVPmfd/fBZOn7S2Hik2BSCp+jrdBW5ekLQNDm+0uhoRKWtbF8CXY8yupg53wO3/BVd3q6sq0HM81GsDp09AlAZDVxUKQFL11agN7W8372t/MJHqZcs8+GocGHnQ6W4Y8qG5OWll4uoOt5xbaiV6DhzSL2JVgQKQVA9h57rBti1SE7RIdRHzKSx6EAy7udbXrdPNDZEro9Be5tIcAN8+Bnlnra1H/pICkFQPjcIhsIM5M2TLPKurEZHS2jQblkwADAgfC7f821wAtTK74UXw8ofErbDhI6urkb9Qyf82iRTT+YOhN2kwtEiV9ttH8O1E837E389tPVEFvq68AwoWRFz1T0g9Ym09cklV4G+USDF1vBM8asGJ3eaUVBGpetZNh6VPmvd7PQz9p5q/4FQVXUZBo+6QkwE/TLK6GrkEBSCpPjx9oMNQ874GQ4tUPWumwQ/PmPevfhxueKlqhR8wW6pueQtsrubU/d0rrK5IiqAAJNVL/srQ25dAxnFraxGR4vv5dVgxxbx/zdNw3XNVL/zkC+oAEef2n/z+ccg9Y209clEKQFK9NOgEDcPMxdLWvAU5mVZXJCKXYhiw8p+w6mXz8XXPQp9JVTf85OszCXyC4dQB+OUtq6uRi1AAkuonvxVo/fvwWnNz6fzfv9ReYSKVjWGYCweufs18fMOL0PtJa2sqK54+5vglgF+nQfIeS8uRCykASfXT6W6z+bx2Uzh7BnZ+C1/dD683h7l3wua5WitIxGqGYW4dseZt83G/qXDlo9bWVNba3gotbjD3LvsuUrNTKxmbYei/yJ+lpaXh5+dHamoqvr6+Vpcjl8swIPEPcyDi9q8heVfBcy5u0LQ3tBlkbqpYq551dYo4G8OApU/Bhg/Nxze9Ad3HWVtTeTm5H97vYa5Rdvt/CyZqSLkoyfe3AtBFKABVU0k7YccSMwwl/lFw3OYCoVeav621vgV8G1hXo0h1Z7ebA4M3zQJsMHAahN1rcVHl7OfXzTFOtQJhwkbw8rO6ompLAaiUFICcwIm9ZhDasQSObj7vCRuEREDbQWbrkH+IZSWKVDt2O3zzCGz+FLCZW1t0GWF1VeXvbDZ8cKW5Rln3B+Cm162uqNpSAColBSAnc+og7PjGDEOHfiv8XHDXgjBUt7k19YlUB/Y8+Ho8bPnCbHUd8qG5eKmz2PczfDLI/OzjVkJwF6srqpYUgEpJAciJpR2FHd+arUPxa81NGPMFdjC7ydoOgnqtrKtRpKrJOwuL/gZ/LDAXCLx9JrS/3eqqKt7C+2Hrl2b4uT+q8m7sWoUpAJWSApAAkJFkziDbvgT2rwYjr+C5eq3NVqG2t0Jgu6q/ZolIecnLhYVjzV8qXNxg6GzzlwhnlJ4I73WD7NTqPfDbQgpApaQAJBc4fRLivjfD0N6V5kKL+eo0M4NQm0Hmb3YKQyKmszmwYIz5i4SrB9z5CbQaYHVV1towE75/Ajz9zAHRPoFWV1StKACVkgKQXFJWKuz6wfyNds8Kc3prPr/GBWOGGnWrGjtYi5SH3Cz43yjY/QO4esJdc6HlDVZXZT17HvznenPyRYc7ze5AKTMKQKWkACTFlp0Bu380B1Dv+hFyz9t6w6cBtBlotg417qn+fnEeuWfMFdj3RoGbFwz/AppfZ3VVlceRGJh5HWDAqCXQ7BqrK6o2FIBKSQFILkvuGdgTZbYM7VoG2edtveFdz1xjqO0gaHI1uLpbV6dIecrJhC/uMsfNudeEu+ebi45KYd89ARtnQt2W8Pdfwc3T6oqqBQWgUlIAklI7m21Oe93+NcR9B2dOFTxXoza0utlsGWp2jf7hk+ojOwM+vxMO/goetWDElxDay+qqKqczKeaA6MwkcwPY6rIHmsUUgEpJAUjKVF4uHPjFHEC94xs4nVzwnKcvXNH/3J5B14N7DevqFCmNrDSYewccWm/+vR6xABpHWF1V5fb7l+Y+hW5e8NB6qNPU6oqqPAWgUlIAknJjz4P4dedWof4G0o8VPOfuDVfcaA6gbnkjeNayrk6RkjiTAp/dDkc2mds83LMIGoVZXVXlZxjwya2w/2dz09QRX2oWaSkpAJWSApBUCLvd/MLY/rXZOpQaX/Ccmxe06Gu2DF3RT3sHSeV1+iR8OgSOxZrduyMXQ3Bni4uqQpJ3wwe9zB3j7/zE/H9eLpsCUCkpAEmFMwxzWmz+Zq0n9xU85+oBza41/2FsdRPUrGNZmSKFZJ4wWzASt0LNujDqawjqYHVVVc/Kl2H16+ATDBM2gKeP1RVVWQpApaQAJJYyDEjcVrBZ6/GdBc/ZXM0ZNW0HmbPKatW3rk5xbhnHzb2tkraDd30YvQTqt7G6qqop9wy83wNOHYCeE6DfP62uqMoqyfe35au0TZ8+nSZNmuDl5UVERAQbNmwo1uvmzZuHzWZj8ODBFzy3Y8cOBg0ahJ+fH97e3nTr1o34+PgLLyJSGdlsENQervsHjP8Nxm+APs+ae5EZebBvFXz7GLzZCmbfDL99ZO5hJlJR0hNgzs1m+KkVBPd+p/BTGu41zK0xANZ/AAl/WFuPk7C0BWj+/PmMGjWKGTNmEBERwbRp0/jyyy+Ji4ujfv2if7M9cOAAV111Fc2aNaNOnTosXrzY8dzevXvp3r07Y8eOZfjw4fj6+rJt2zZ69OhxyWueTy1AUmmd2GsOnt7+NRyNKfxcSMS5/ckGgX9ja+qT6i/1CHw8EE7uBd+GMPobqNvc6qqqh/+NMv/fbtQd7vtBK8lfhirTBRYREUG3bt147733ALDb7YSEhPDwww/z9NNPX/Q1eXl59O7dm/vuu49ffvmFlJSUQgHorrvuwt3dnU8//fSy61IAkiohJf5cGFpiTj0+X3CXgs1a9eUkZSXlEHx8i9lV4xdihh9N3S47aUfNtYFyMmDgOxA22uqKqpwq0QWWk5NDdHQ0ffv2LSjGxYW+ffuybt26Il/34osvUr9+fcaOHXvBc3a7ne+++44rrriCfv36Ub9+fSIiIgoFJJFqw78x9BwPY3+AyJ1mE3qTq8HmYg6ojnoB3u0KH1wJP78GSTv/+poiRTl1AObcZP7pHwpjvlf4KWu+wdDnGfP+iinmIHMpN5YFoOTkZPLy8ggMLLwTbmBgIAkJCRd9zZo1a/jvf//LzJkX3zwuKSmJjIwMXn31Vfr378+PP/7IkCFDuO222/j555+LrCU7O5u0tLRCN5EqxbcBdB8H934Lj++CW6aZey+5uEHiH7Dqn/B+BLzX3ZxxkrDVHGwtUhwn9prjzVLioU5zGLNU3azlpfvfzPF+Z07B8slWV1OtVZkOxvT0dEaOHMnMmTMJCAi46Dl2ux2AW2+9lccee4zOnTvz9NNPc8sttzBjxowirz116lT8/Pwct5CQkHL5DCIVolY9CB8DIxfBE7vh1vfN1aZdPSA5zpxuO+MqeKcLLJ8CR6IVhqRoybvNAc9phyHgCnPAs19Dq6uqvlzd4Ja3zPuxn8HBontEpHTcrHrjgIAAXF1dSUxMLHQ8MTGRoKCgC87fu3cvBw4cYODAgY5j+YHHzc2NuLg4QkJCcHNzo23btoVe26ZNG9asWVNkLZMmTSIyMtLxOC0tTSFIqoeadaDLCPOWlWruWL99MexZAaf2w6/TzJtfSMEA6kbdNfhSTEk7zQHPmUlQr4051V1LL5S/kO7QdTTEfAzfRcLfVmsD5XJgWQDy8PAgLCyMqKgox1R2u91OVFQUEyZMuOD81q1bs3Xr1kLHnn32WdLT0/n3v/9NSEgIHh4edOvWjbi4uELn7dq1i9DQ0CJr8fT0xNNTG1JKNeflBx3vMG/ZGbBnuTmAetcPkHoI1k83b7WCoM1AcwB1aC9wcbW6crFC4jb4eJC5d11ge3ORQ++Lt75LOej7POz81lxqYP37cOWjVldU7VgWgAAiIyMZPXo04eHhdO/enWnTppGZmcmYMWMAGDVqFA0bNmTq1Kl4eXnRvn37Qq/39/cHKHT8ySefZNiwYfTu3Zs+ffqwbNkyvvnmG3766aeK+lgilZ9nLWg3xLzlnoG9K80wFLcUMhJg40zzVjMA2txitg417a3fQp3FsS3wyWA4cxIadDK3t9AK5BWrZh244SX4+iH46VVodxv4q2eiLFkagIYNG8bx48eZPHkyCQkJdO7cmWXLljkGRsfHx+NSwqb4IUOGMGPGDKZOncojjzxCq1atWLhwIVdddVV5fASRqs+9BrS+2bydzTE3Zty+GHZ+Z/72Hz3HvHn5m+e0GQTN+4CbWk2rpSMx8Olgs8u0YRjc8xXU8Le6KufU+W7Y/BnEr4VlT8Ndc62uqFrRVhgXoXWARIC8XDiwxtyOY8c3kHm84DlPX3OT1ra3QvPrwaOmdXVK2Tm0ET67DbLTzLFg9yzQRrxWS9phTlqwn4Xh86FVf6srqtSqzEKIlZUCkMif2PMgfv25/cm+gfTztt5w9YQmV5pBqEVfqNfK3M5DqpaD62DuHZCTDo17wYj/aVPOymL5ZPj13+bSAw/9pl84LkEBqJQUgEQuwW43p85vX2y2DqX8aZ8934bQ4nozEDW7Vt0nVcH+X+DzYZCbaS6mefd88PC2uirJl5MJ0yPMyQpXRULfKVZXVGkpAJWSApBIMRkGJO+CPVHm1PqDv8LZrILnba7QKNxsGWp+PQR31qyyymbvKvhiOJw9Yy6eOWyuWhgqo53fwby7wcUd/v6r2dIqF1AAKiUFIJHLlHvGDEF7VpqBKLnwkhTUqGN+yba43vzT58I1v6QC7V4B80eYobXljXDnp+DuZXVVUpTP74JdSyH0KnPVd3U1X0ABqJQUgETKSMoh2BtlthDt+8kcXHu+wA5mGGpxPYT0ADcPS8p0SnHL4H8jIS8HWt0Ed8zRzL7K7tRBsyvs7BkYPAM6D7e6okpHAaiUFIBEykFeLhzedC4QrYCjscB5//x41DLHn+QHojrNrKq0+tvxDXw5Buy55rIGt/9X4bOq+OUtc6PjmgEwYaPWZ/oTBaBSUgASqQCZyeb4k/wWosykws/XaVYws6zJVebijVJ62xbBgrFg5JmL6932kRa4rErO5sCHV8PxnRA2BgZOs7qiSkUBqJQUgEQqmN1u7lq/Z4W5KnX8OnPdk3wu7hDasyAQBbbT+IfL8fuXsOgBMOzQcZi5Ua6rpevhyuU4sMbcoBYb3L/CnGgggAJQqSkAiVgsOx32ry6YXZZysPDztYIKusqa9VE3QHHEfg5fjzfDT+d7YNA7mpFXlS16ELZ8AUEdYNxPCrLnKACVkgKQSCViGHBynxmE9kTBgV8g9/R5J9jMLRtanGsdCu6qL4M/i/4YvnkUMCDsXrj5bSjhNkNSyWQch/fCISsF+r8KPf5udUWVggJQKSkAiVRiZ7PNLrI9K8zp9knbCj/v5We2CuUvxujX0Jo6K4uN/4HvHjfvdxsHN72u7sPqYtNs+HYiePiYA6J9G1hdkeUUgEpJAUikCkk7ao4b2rPCHFSdlVL4+XptCrrLGvdyrnVu1s+AZU+Z93s8BP1eUfipTux2mHUjHN4I7YaYSxk4OQWgUlIAEqmi7Hnmbub5U+2PRJtjXvK51TBnlLXoa97qNq++gWDtu/Djs+b9Kx+Fvi9U38/qzI79Dh9dY/49v+crM+g7MQWgUlIAEqkmTp80F2DcE2WGovRjhZ/3b1ywTUfT3uBVTf5//+VNiHrRvN/7SejzD4Wf6mzZJFj/vrl0xN/XOVcr558oAJWSApBINWQYkLS9YGZZ/DpzFeR8Lm4QElEwdiioY9UcKPzTv+CnV8z71z4D1z5lbT1S/rLT4b1uZsC/5mnoM8nqiiyjAFRKCkAiTiAn01xPJT8Qndxb+Hnveuf2Letr/ukdYE2dxWUYsOqfsPp18/H1k+Hqx62tSSrOtkXw5b3g6gEPrTe7d52QAlApKQCJOKGT+wtWpd6/GnIyznvSBg06nRs7dD006la5Vk82DFgxBX79t/n4xpeh18PW1iQVyzDgs9vNv8PN+sDIRU7Z7akAVEoKQCJO7mwOHPqtYDB1wtbCz3v6mmOG8gORf2Nr6gTzi++HZ8wxIAADXoOIv1lXj1jnxF54vyfkZcPQWdD+dqsrqnAKQKWkACQihaQnmlPt81uIzpws/HzAFeftW3YluNeomLrsdlj6f7Bxpvn45reg29iKeW+pnPLHgNUKMtcGqi4D+4tJAaiUFIBEpEj2PDgWay7CuGeFuQaLkVfwvKunGYLyA1G9VuXTFWG3w3ePQfQcwGZubdF1VNm/j1QtZ7PNVqCTeyHiQRjwL6srqlAKQKWkACQixXYmBfb/fG4wdRSkHS78vG/Dgm06ml4DNfxL/572PFjyMMTOBZuLualp5+Glv65UD3tXwaeDzb8b41ZBcGerK6owCkClpAAkIpfFMCB5V8G+ZQd/hbNZBc/bXM0B1PkrUzfoUvKp9nlnYfHfYev/zOsN+RA63lG2n0OqvgX3wR8LzX3yxi53mo1vFYBKSQFIRMpE7hkzBOW3DiXHFX6+Rp1zU+2vN//0Cbr09fJy4asHYNtX5rpFt//H3AJB5M/SE8y1gbLTnGpsmAJQKSkAiUi5SDlUMLNs38/ml9P5AjsUtA6F9AA3j4LnzubAwrGwYwm4uMMds6HNwIqtX6qW3z40B8l7+sHDm6BWfasrKncKQKWkACQi5S4vFw5vOreJaxQc3Vz4eY9a0ORqMww1vcZc5yfue3Ohuzs/hVb9ralbqg57HszsA8e2QMdhcNtHVldU7hSASkkBSEQqXGayOXg1PxBlHr/wHDcvuGuuOaBapDiORMPM6wEDRn9jrl9VjSkAlZICkIhYym6HxK0FY4cOrTen1w//HJpda3V1UtV8Gwmb/muuV/Xgr4W7VqsZBaBSUgASkUolOx3sZ6FGbasrkaroTAq8F262Kl73HPR+wuqKyk1Jvr+r4FbHIiJOxtNH4UcuXw1/6PeKeX/163DqgJXVVBoKQCIiItVdhzvM8T9ns+D7/zPXrHJyCkAiIiLVnc0GN71pLqGw+wfY+a3VFVmuUgSg6dOn06RJE7y8vIiIiGDDhg3Fet28efOw2WwMHjy4yHMefPBBbDYb06ZNK5tiRUREqqJ6V8CVj5r3lz4F2RnW1mMxywPQ/PnziYyMZMqUKcTExNCpUyf69etHUlLSJV934MABnnjiCa6++uoiz1m0aBHr168nODi4rMsWERGpeno/Af6hkHYEfn7V6mosZXkAeuuttxg3bhxjxoyhbdu2zJgxg5o1azJr1qwiX5OXl8eIESN44YUXaNas2UXPOXLkCA8//DBz587F3d29vMoXERGpOtxrwE1vmPfXvQ+J26ytx0KWBqCcnByio6Pp27dgUS8XFxf69u3LunXrinzdiy++SP369Rk79uJ7m9jtdkaOHMmTTz5Ju3bt/rKO7Oxs0tLSCt1ERESqpStuNLdRMfLMNYLsdqsrsoSlASg5OZm8vDwCAwMLHQ8MDCQhIeGir1mzZg3//e9/mTlzZpHX/de//oWbmxuPPPJIseqYOnUqfn5+jltISEjxP4SIiEhV0/9VcPc2F9mMnWt1NZawvAusJNLT0xk5ciQzZ84kICDgoudER0fz73//mzlz5mCz2Yp13UmTJpGamuq4HTp0qCzLFhERqVz8GkGfSeb95ZPh9Elr67GAm5VvHhAQgKurK4mJiYWOJyYmEhQUdMH5e/fu5cCBAwwcWLADsv1c052bmxtxcXH88ssvJCUl0bhxY8c5eXl5PP7440ybNo0DBw5ccF1PT088PT3L6FOJiIhUAREPQuwXkLTNDEG3vmd1RRXK0hYgDw8PwsLCiIqKchyz2+1ERUXRs2fPC85v3bo1W7duJTY21nEbNGgQffr0ITY2lpCQEEaOHMnvv/9e6Jzg4GCefPJJfvjhh4r8eCIiIpWXqzvc8pZ5f/OnEL/e2noqmKUtQACRkZGMHj2a8PBwunfvzrRp08jMzGTMmDEAjBo1ioYNGzJ16lS8vLxo3759odf7+/sDOI7XrVuXunXrFjrH3d2doKAgWrVqVf4fSEREpKpo3AO6jDQD0LeR8LefzWDkBCwPQMOGDeP48eNMnjyZhIQEOnfuzLJlyxwDo+Pj43FxqVJDlURERKqOG16End+ZXWG/zYBeD1tdUYXQbvAXod3gRUTEqcR8CksmmDPDJmwwB0lXQdoNXkRERIqv8wgI6QG5meY2GU5AAUhERMTZubiYA6JtruZGqbuq/6QhBSARERGBwHbQ8yHz/vdPQM5pa+spZwpAIiIiYrrmafBtBCnx8MsbVldTrhSARERExORZCwb8y7z/6ztwPM7aesqRApCIiIgUaH0zXNEf7Lnw3eNQTSeLKwCJiIhIAZsNBrwGbjXgwC/w+/+srqhcKACJiIhIYbVD4Zonzfs//gPOnLK2nnKgACQiIiIX6vkwBLSCzOMQ9ZLV1ZQ5BSARERG5kJsH3PymeX/TLDgcbW09ZUwBSERERC6u6dXQ8S7AgO8eA3ue1RWVGQUgERERKdqNL4OXHxzbAhv/Y3U1ZUYBSERERIpWqx5cP8W8v/JlSE+wtp4yogAkIiIilxY2BhqGQXYa/PCM1dWUCQUgERERuTQXF7jlbbC5wB8LYe9KqysqNQUgERER+WsNOkH3B8z73z0BuVnW1lNKCkAiIiJSPH3+AbWC4ORe+PXfVldTKgpAIiIiUjxevtD/FfP+L2/Cib3W1lMKCkAiIiJSfO1ug2Z9IC8bvn+iym6WqgAkIiIixWezmStEu3qag6G3LbK6osuiACQiIiIlU7c5XPWYeX/ZJMhKs7aey6AAJCIiIiV31WNQpxlkJMCqV6yupsQUgERERKTk3L3gpjfM+xs+NLfKqEIUgEREROTytLjeHBRt2OHbSLDbra6o2BSARERE5PL1ewU8fODIJoiZY3U1xaYAJCIiIpfPtwFc96x5f8XzkHHc0nKKSwFIRERESqfb/RDUEbJSYflzVldTLApAIiIiUjqubnDLNMAGW76AA2usrugvKQCJiIhI6TUKg/Ax5v1vI+FsjrX1/AUFIBERESkb108G73qQHAfr3rO6mktSABIREZGyUaM23Piyef/n1+DUQWvruYRKEYCmT59OkyZN8PLyIiIigg0bNhTrdfPmzcNmszF48GDHsdzcXJ566ik6dOiAt7c3wcHBjBo1iqNHj5ZT9SIiIuLQcRg0uRrOnoGlT1ldTZEsD0Dz588nMjKSKVOmEBMTQ6dOnejXrx9JSUmXfN2BAwd44oknuPrqqwsdP336NDExMTz33HPExMTw1VdfERcXx6BBg8rzY4iIiAgUbJbq4g67lsLO76yu6KJshmHtPvYRERF069aN994z+wrtdjshISE8/PDDPP300xd9TV5eHr179+a+++7jl19+ISUlhcWLFxf5Hhs3bqR79+4cPHiQxo0b/2VNaWlp+Pn5kZqaiq+v72V9LhEREae24gVY8xb4hcD438DDu9zfsiTf35a2AOXk5BAdHU3fvn0dx1xcXOjbty/r1q0r8nUvvvgi9evXZ+zYscV6n9TUVGw2G/7+/hd9Pjs7m7S0tEI3ERERKYXeT4J/Y0g9BD//y+pqLmBpAEpOTiYvL4/AwMBCxwMDA0lISLjoa9asWcN///tfZs6cWaz3yMrK4qmnnmL48OFFpsGpU6fi5+fnuIWEhJTsg4iIiEhhHjVhwOvm/XXTIXG7tfX8ieVjgEoiPT2dkSNHMnPmTAICAv7y/NzcXO68804Mw+CDDz4o8rxJkyaRmprquB06dKgsyxYREXFOrfpD61vAfha+exysHXVTiJuVbx4QEICrqyuJiYmFjicmJhIUFHTB+Xv37uXAgQMMHDjQccx+budZNzc34uLiaN68OVAQfg4ePMjKlSsv2Rfo6emJp6dnWXwkEREROV//V2HvSohfC7GfQ5cRVlcEWNwC5OHhQVhYGFFRUY5jdrudqKgoevbsecH5rVu3ZuvWrcTGxjpugwYNok+fPsTGxjq6rvLDz+7du1mxYgV169atsM8kIiIi5/EPgWvPTWr68Vk4fdLaes6xtAUIIDIyktGjRxMeHk737t2ZNm0amZmZjBljLqc9atQoGjZsyNSpU/Hy8qJ9+/aFXp8/sDn/eG5uLkOHDiUmJoZvv/2WvLw8x3iiOnXq4OHhUXEfTkRERKDHQ7BlHiRthxVTYNC7VldkfQAaNmwYx48fZ/LkySQkJNC5c2eWLVvmGBgdHx+Pi0vxG6qOHDnCkiVLAOjcuXOh51atWsW1115bVqWLiIhIcbi6w81vwez+EPMJdL4HGkdYWpLl6wBVRloHSEREpBx8PR42fwaB7eGBn81d5MtQlVkHSERERJxI3xfN/cIS/4DfZlhaigKQiIiIVAzvunDDi+Y2GWezLC3F8jFAIiIi4kQ63wNNroI6zSwtQy1AIiIiUnFcXCwPP6AAJCIiIk5IAUhEREScjgKQiIiIOB0FIBEREXE6CkAiIiLidBSARERExOkoAImIiIjTUQASERERp6MAJCIiIk5HAUhEREScjgKQiIiIOB0FIBEREXE6CkAiIiLidNysLqAyMgwDgLS0NIsrERERkeLK/97O/x6/FAWgi0hPTwcgJCTE4kpERESkpNLT0/Hz87vkOTajODHJydjtdo4ePYqPjw82m61Mr52WlkZISAiHDh3C19e3TK8tBfRzrhj6OVcM/Zwrhn7OFaM8f86GYZCenk5wcDAuLpce5aMWoItwcXGhUaNG5foevr6++h+sAujnXDH0c64Y+jlXDP2cK0Z5/Zz/quUnnwZBi4iIiNNRABIRERGnowBUwTw9PZkyZQqenp5Wl1Kt6edcMfRzrhj6OVcM/ZwrRmX5OWsQtIiIiDgdtQCJiIiI01EAEhEREaejACQiIiJORwFIREREnI4CUAWaPn06TZo0wcvLi4iICDZs2GB1SdXO6tWrGThwIMHBwdhsNhYvXmx1SdXS1KlT6datGz4+PtSvX5/BgwcTFxdndVnVzgcffEDHjh0dC8b17NmTpUuXWl1Wtffqq69is9mYOHGi1aVUK88//zw2m63QrXXr1pbVowBUQebPn09kZCRTpkwhJiaGTp060a9fP5KSkqwurVrJzMykU6dOTJ8+3epSqrWff/6Z8ePHs379epYvX05ubi433ngjmZmZVpdWrTRq1IhXX32V6OhoNm3axHXXXcett97Ktm3brC6t2tq4cSMffvghHTt2tLqUaqldu3YcO3bMcVuzZo1ltWgafAWJiIigW7duvPfee4C531hISAgPP/wwTz/9tMXVVU82m41FixYxePBgq0up9o4fP079+vX5+eef6d27t9XlVGt16tTh9ddfZ+zYsVaXUu1kZGTQtWtX3n//fV5++WU6d+7MtGnTrC6r2nj++edZvHgxsbGxVpcCqAWoQuTk5BAdHU3fvn0dx1xcXOjbty/r1q2zsDKRspGamgqYX85SPvLy8pg3bx6ZmZn07NnT6nKqpfHjx3PzzTcX+rdaytbu3bsJDg6mWbNmjBgxgvj4eMtq0WaoFSA5OZm8vDwCAwMLHQ8MDGTnzp0WVSVSNux2OxMnTuTKK6+kffv2VpdT7WzdupWePXuSlZVFrVq1WLRoEW3btrW6rGpn3rx5xMTEsHHjRqtLqbYiIiKYM2cOrVq14tixY7zwwgtcffXV/PHHH/j4+FR4PQpAIlIq48eP548//rC0L786a9WqFbGxsaSmprJgwQJGjx7Nzz//rBBUhg4dOsSjjz7K8uXL8fLysrqcamvAgAGO+x07diQiIoLQ0FD+97//WdKlqwBUAQICAnB1dSUxMbHQ8cTERIKCgiyqSqT0JkyYwLfffsvq1atp1KiR1eVUSx4eHrRo0QKAsLAwNm7cyL///W8+/PBDiyurPqKjo0lKSqJr166OY3l5eaxevZr33nuP7OxsXF1dLaywevL39+eKK65gz549lry/xgBVAA8PD8LCwoiKinIcs9vtREVFqS9fqiTDMJgwYQKLFi1i5cqVNG3a1OqSnIbdbic7O9vqMqqV66+/nq1btxIbG+u4hYeHM2LECGJjYxV+yklGRgZ79+6lQYMGlry/WoAqSGRkJKNHjyY8PJzu3bszbdo0MjMzGTNmjNWlVSsZGRmFfpvYv38/sbGx1KlTh8aNG1tYWfUyfvx4Pv/8c77++mt8fHxISEgAwM/Pjxo1alhcXfUxadIkBgwYQOPGjUlPT+fzzz/np59+4ocffrC6tGrFx8fngvFr3t7e1K1bV+PaytATTzzBwIEDCQ0N5ejRo0yZMgVXV1eGDx9uST0KQBVk2LBhHD9+nMmTJ5OQkEDnzp1ZtmzZBQOjpXQ2bdpEnz59HI8jIyMBGD16NHPmzLGoqurngw8+AODaa68tdHz27Nnce++9FV9QNZWUlMSoUaM4duwYfn5+dOzYkR9++IEbbrjB6tJESuzw4cMMHz6cEydOUK9ePa666irWr19PvXr1LKlH6wCJiIiI09EYIBEREXE6CkAiIiLidBSARERExOkoAImIiIjTUQASERERp6MAJCIiIk5HAUhEREScjgKQiEgx2Gw2Fi9ebHUZIlJGFIBEpNK79957sdlsF9z69+9vdWkiUkVpKwwRqRL69+/P7NmzCx3z9PS0qBoRqerUAiQiVYKnpydBQUGFbrVr1wbM7qkPPviAAQMGUKNGDZo1a8aCBQsKvX7r1q1cd9111KhRg7p16/LAAw+QkZFR6JxZs2bRrl07PD09adCgARMmTCj0fHJyMkOGDKFmzZq0bNmSJUuWlO+HFpFyowAkItXCc889x+23386WLVsYMWIEd911Fzt27AAgMzOTfv36Ubt2bTZu3MiXX37JihUrCgWcDz74gPHjx/PAAw+wdetWlixZQosWLQq9xwsvvMCdd97J77//zk033cSIESM4efJkhX5OESkjhohIJTd69GjD1dXV8Pb2LnT75z//aRiGYQDGgw8+WOg1ERERxt///nfDMAzjo48+MmrXrm1kZGQ4nv/uu+8MFxcXIyEhwTAMwwgODjb+8Y9/FFkDYDz77LOOxxkZGQZgLF26tMw+p4hUHI0BEpEqoU+fPnzwwQeFjtWpU8dxv2fPnoWe69mzJ7GxsQDs2LGDTp064e3t7Xj+yiuvxG63ExcXh81m4+jRo1x//fWXrKFjx46O+97e3vj6+pKUlHS5H0lELKQAJCJVgre39wVdUmWlRo0axTrP3d290GObzYbdbi+PkkSknGkMkIhUC+vXr7/gcZs2bQBo06YNW7ZsITMz0/H8r7/+iouLC61atcLHx4cmTZoQFRVVoTWLiHXUAiQiVUJ2djYJCQmFjrm5uREQEADAl19+SXh4OFdddRVz585lw4YN/Pe//wVgxIgRTJkyhdGjR/P8889z/PhxHn74YUaOHElgYCAAzz//PA8++CD169dnwIABpKen8+uvv/Lwww9X7AcVkQqhACQiVcKyZcto0KBBoWOtWrVi586dgDlDa968eTz00EM0aNCAL774grZt2wJQs2ZNfvjhBx599FG6detGzZo1uf3223nrrbcc1xo9ejRZWVm8/fbbPPHEEwQEBDB06NCK+4AiUqFshmEYVhchIlIaNpuNRYsWMXjwYKtLEZEqQmOARERExOkoAImIiIjT0RggEany1JMvIiWlFiARERFxOgpAIiIi4nQUgERERMTpKACJiIiI01EAEhEREaejACQiIiJORwFIREREnI4CkIiIiDgdBSARERFxOv8P5hTkCpHVDEsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b11c5-39fb-4b38-bcce-28127fe485ae",
   "metadata": {},
   "source": [
    "### d. Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9df13e37-8ee4-4395-8a40-4cd9772bf08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy 0.4979\n"
     ]
    }
   ],
   "source": [
    "model_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Model Accuracy {model_mae[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2827e7-2e8a-4f8e-8e25-d059b94fe39c",
   "metadata": {},
   "source": [
    "## 4 Optimising Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53027725-e2d3-42d6-98c0-617df3e67f3c",
   "metadata": {},
   "source": [
    "### a. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47f75f82-8fe8-478b-a6eb-3ac083a5b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
      "Wall time: 8.11 µs\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 1.0806 - accuracy: 0.5213 - val_loss: 0.7726 - val_accuracy: 0.5192 - 541ms/epoch - 42ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 0.7574 - accuracy: 0.5038 - val_loss: 0.8236 - val_accuracy: 0.4727 - 37ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 0.7194 - accuracy: 0.5574 - val_loss: 0.8042 - val_accuracy: 0.4646 - 36ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 0.7013 - accuracy: 0.5475 - val_loss: 0.8665 - val_accuracy: 0.4707 - 34ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 0.6886 - accuracy: 0.5584 - val_loss: 0.7436 - val_accuracy: 0.5010 - 36ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 0.6862 - accuracy: 0.5589 - val_loss: 0.7549 - val_accuracy: 0.4859 - 35ms/epoch - 3ms/step\n",
      "Epoch 7/50\n",
      "13/13 - 0s - loss: 0.6754 - accuracy: 0.5652 - val_loss: 0.7561 - val_accuracy: 0.4778 - 34ms/epoch - 3ms/step\n",
      "Epoch 8/50\n",
      "13/13 - 0s - loss: 0.6711 - accuracy: 0.5525 - val_loss: 0.7857 - val_accuracy: 0.4717 - 33ms/epoch - 3ms/step\n",
      "Epoch 9/50\n",
      "13/13 - 0s - loss: 0.6601 - accuracy: 0.5939 - val_loss: 0.7891 - val_accuracy: 0.5051 - 33ms/epoch - 3ms/step\n",
      "Epoch 10/50\n",
      "13/13 - 0s - loss: 0.6724 - accuracy: 0.5931 - val_loss: 0.7990 - val_accuracy: 0.5010 - 36ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 1.1214 - accuracy: 0.4985 - val_loss: 0.8895 - val_accuracy: 0.5152 - 564ms/epoch - 43ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 0.7442 - accuracy: 0.5178 - val_loss: 1.0395 - val_accuracy: 0.4768 - 35ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 0.7109 - accuracy: 0.5467 - val_loss: 0.8592 - val_accuracy: 0.4919 - 34ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 0.6868 - accuracy: 0.5541 - val_loss: 0.7801 - val_accuracy: 0.5040 - 35ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 0.6974 - accuracy: 0.5683 - val_loss: 0.7546 - val_accuracy: 0.5061 - 34ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 0.6833 - accuracy: 0.5556 - val_loss: 0.7828 - val_accuracy: 0.4970 - 34ms/epoch - 3ms/step\n",
      "Epoch 7/60\n",
      "13/13 - 0s - loss: 0.6846 - accuracy: 0.5602 - val_loss: 0.8360 - val_accuracy: 0.4889 - 34ms/epoch - 3ms/step\n",
      "Epoch 8/60\n",
      "13/13 - 0s - loss: 0.6668 - accuracy: 0.6033 - val_loss: 0.8499 - val_accuracy: 0.5040 - 34ms/epoch - 3ms/step\n",
      "Epoch 9/60\n",
      "13/13 - 0s - loss: 0.6605 - accuracy: 0.5863 - val_loss: 0.8617 - val_accuracy: 0.5111 - 31ms/epoch - 2ms/step\n",
      "Epoch 10/60\n",
      "13/13 - 0s - loss: 0.6528 - accuracy: 0.6127 - val_loss: 0.8029 - val_accuracy: 0.5071 - 33ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 1.1236 - accuracy: 0.4929 - val_loss: 0.7779 - val_accuracy: 0.4970 - 549ms/epoch - 42ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 0.7476 - accuracy: 0.5259 - val_loss: 1.0313 - val_accuracy: 0.4747 - 36ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 0.7107 - accuracy: 0.5297 - val_loss: 0.7116 - val_accuracy: 0.4970 - 35ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 0.6905 - accuracy: 0.5434 - val_loss: 0.7182 - val_accuracy: 0.4737 - 34ms/epoch - 3ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 0.6824 - accuracy: 0.5723 - val_loss: 0.7595 - val_accuracy: 0.4828 - 34ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 0.6871 - accuracy: 0.5558 - val_loss: 0.8009 - val_accuracy: 0.4747 - 35ms/epoch - 3ms/step\n",
      "Epoch 7/70\n",
      "13/13 - 0s - loss: 0.6677 - accuracy: 0.5860 - val_loss: 0.8883 - val_accuracy: 0.4717 - 35ms/epoch - 3ms/step\n",
      "Epoch 8/70\n",
      "13/13 - 0s - loss: 0.6706 - accuracy: 0.5807 - val_loss: 0.8100 - val_accuracy: 0.4879 - 34ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 1.1461 - accuracy: 0.5173 - val_loss: 0.7727 - val_accuracy: 0.4616 - 644ms/epoch - 50ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 0.7217 - accuracy: 0.5183 - val_loss: 0.7462 - val_accuracy: 0.5091 - 36ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 0.7009 - accuracy: 0.5287 - val_loss: 0.7590 - val_accuracy: 0.4758 - 36ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 0.6815 - accuracy: 0.5622 - val_loss: 0.7422 - val_accuracy: 0.5182 - 35ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 0.6736 - accuracy: 0.5855 - val_loss: 0.7603 - val_accuracy: 0.5061 - 34ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 0.6855 - accuracy: 0.5726 - val_loss: 0.8070 - val_accuracy: 0.4838 - 35ms/epoch - 3ms/step\n",
      "Epoch 7/80\n",
      "13/13 - 0s - loss: 0.6723 - accuracy: 0.5820 - val_loss: 0.8386 - val_accuracy: 0.5040 - 36ms/epoch - 3ms/step\n",
      "Epoch 8/80\n",
      "13/13 - 0s - loss: 0.6823 - accuracy: 0.5997 - val_loss: 0.8715 - val_accuracy: 0.5040 - 33ms/epoch - 3ms/step\n",
      "Epoch 9/80\n",
      "13/13 - 0s - loss: 0.7111 - accuracy: 0.5759 - val_loss: 0.8519 - val_accuracy: 0.5071 - 34ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 7.1872 - accuracy: 0.4934 - val_loss: 7.9262 - val_accuracy: 0.4838 - 537ms/epoch - 41ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 7.4225 - accuracy: 0.5147 - val_loss: 7.7022 - val_accuracy: 0.4970 - 35ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 7.6468 - accuracy: 0.4992 - val_loss: 7.7363 - val_accuracy: 0.4939 - 37ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 7.2459 - accuracy: 0.5282 - val_loss: 8.1321 - val_accuracy: 0.4717 - 35ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 7.2203 - accuracy: 0.5312 - val_loss: 8.2778 - val_accuracy: 0.4626 - 35ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 7.4258 - accuracy: 0.5152 - val_loss: 7.3152 - val_accuracy: 0.5212 - 35ms/epoch - 3ms/step\n",
      "Epoch 7/50\n",
      "13/13 - 0s - loss: 7.8855 - accuracy: 0.4840 - val_loss: 7.2616 - val_accuracy: 0.5242 - 36ms/epoch - 3ms/step\n",
      "Epoch 8/50\n",
      "13/13 - 0s - loss: 7.9365 - accuracy: 0.4807 - val_loss: 7.3148 - val_accuracy: 0.5212 - 32ms/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "13/13 - 0s - loss: 7.9196 - accuracy: 0.4817 - val_loss: 7.4687 - val_accuracy: 0.5111 - 33ms/epoch - 3ms/step\n",
      "Epoch 10/50\n",
      "13/13 - 0s - loss: 7.8859 - accuracy: 0.4838 - val_loss: 7.3258 - val_accuracy: 0.5202 - 33ms/epoch - 3ms/step\n",
      "Epoch 11/50\n",
      "13/13 - 0s - loss: 7.8578 - accuracy: 0.4858 - val_loss: 7.3568 - val_accuracy: 0.5182 - 33ms/epoch - 3ms/step\n",
      "Epoch 12/50\n",
      "13/13 - 0s - loss: 7.8621 - accuracy: 0.4853 - val_loss: 7.3106 - val_accuracy: 0.5212 - 34ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_7 (LSTM)               (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 7.5621 - accuracy: 0.4650 - val_loss: 7.1625 - val_accuracy: 0.5303 - 530ms/epoch - 41ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 8.1045 - accuracy: 0.4685 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 8.1045 - accuracy: 0.4685 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 8.1045 - accuracy: 0.4685 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 8.1045 - accuracy: 0.4685 - val_loss: 7.1625 - val_accuracy: 0.5303 - 34ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 8.1045 - accuracy: 0.4685 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 6.7955 - accuracy: 0.5132 - val_loss: 8.1799 - val_accuracy: 0.4697 - 528ms/epoch - 41ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 33ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 34ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 3ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 7.1833 - accuracy: 0.5005 - val_loss: 7.7023 - val_accuracy: 0.4980 - 535ms/epoch - 41ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 7.5901 - accuracy: 0.5061 - val_loss: 7.8797 - val_accuracy: 0.4869 - 33ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 7.3651 - accuracy: 0.5213 - val_loss: 8.0078 - val_accuracy: 0.4798 - 35ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 7.2966 - accuracy: 0.5261 - val_loss: 7.9200 - val_accuracy: 0.4859 - 35ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 7.2945 - accuracy: 0.5266 - val_loss: 7.9832 - val_accuracy: 0.4818 - 34ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 7.3216 - accuracy: 0.5249 - val_loss: 7.9838 - val_accuracy: 0.4818 - 35ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 7.4669 - accuracy: 0.4784 - val_loss: 7.1625 - val_accuracy: 0.5303 - 535ms/epoch - 41ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 34ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 33ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_11 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 7.4662 - accuracy: 0.4731 - val_loss: 7.1957 - val_accuracy: 0.5283 - 531ms/epoch - 41ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 8.0816 - accuracy: 0.4701 - val_loss: 7.2107 - val_accuracy: 0.5273 - 35ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 8.0816 - accuracy: 0.4701 - val_loss: 7.2263 - val_accuracy: 0.5263 - 34ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 8.0420 - accuracy: 0.4723 - val_loss: 7.3446 - val_accuracy: 0.5192 - 34ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 8.0377 - accuracy: 0.4734 - val_loss: 7.2840 - val_accuracy: 0.5232 - 35ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 7.9223 - accuracy: 0.4810 - val_loss: 7.3619 - val_accuracy: 0.5182 - 34ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 7.5691 - accuracy: 0.4657 - val_loss: 7.1625 - val_accuracy: 0.5303 - 686ms/epoch - 53ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 8.1162 - accuracy: 0.4678 - val_loss: 7.1625 - val_accuracy: 0.5303 - 34ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 8.1162 - accuracy: 0.4678 - val_loss: 7.1625 - val_accuracy: 0.5303 - 36ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 8.1162 - accuracy: 0.4678 - val_loss: 7.1625 - val_accuracy: 0.5303 - 34ms/epoch - 3ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 8.1162 - accuracy: 0.4678 - val_loss: 7.1625 - val_accuracy: 0.5303 - 34ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 8.1162 - accuracy: 0.4678 - val_loss: 7.1625 - val_accuracy: 0.5303 - 34ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_13 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 6.7474 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 533ms/epoch - 41ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 36ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 38ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 34ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 34ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_14 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 1.4287 - accuracy: 0.4698 - val_loss: 0.8409 - val_accuracy: 0.5020 - 523ms/epoch - 75ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 0.7636 - accuracy: 0.5310 - val_loss: 0.9648 - val_accuracy: 0.4636 - 27ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 0.7229 - accuracy: 0.5404 - val_loss: 0.8390 - val_accuracy: 0.4798 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 0.7051 - accuracy: 0.5348 - val_loss: 0.8636 - val_accuracy: 0.4838 - 28ms/epoch - 4ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 0.6854 - accuracy: 0.5508 - val_loss: 0.7881 - val_accuracy: 0.4990 - 27ms/epoch - 4ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 0.6964 - accuracy: 0.5543 - val_loss: 0.8377 - val_accuracy: 0.4879 - 29ms/epoch - 4ms/step\n",
      "Epoch 7/50\n",
      "7/7 - 0s - loss: 0.6796 - accuracy: 0.5698 - val_loss: 0.8885 - val_accuracy: 0.4657 - 27ms/epoch - 4ms/step\n",
      "Epoch 8/50\n",
      "7/7 - 0s - loss: 0.6705 - accuracy: 0.5789 - val_loss: 0.8962 - val_accuracy: 0.4576 - 28ms/epoch - 4ms/step\n",
      "Epoch 9/50\n",
      "7/7 - 0s - loss: 0.6804 - accuracy: 0.5581 - val_loss: 0.8432 - val_accuracy: 0.4677 - 28ms/epoch - 4ms/step\n",
      "Epoch 10/50\n",
      "7/7 - 0s - loss: 0.6591 - accuracy: 0.5972 - val_loss: 0.8302 - val_accuracy: 0.4939 - 29ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_15 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 1.7083 - accuracy: 0.4901 - val_loss: 1.0909 - val_accuracy: 0.4788 - 520ms/epoch - 74ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 0.7785 - accuracy: 0.5183 - val_loss: 0.8263 - val_accuracy: 0.4636 - 28ms/epoch - 4ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 0.7227 - accuracy: 0.5373 - val_loss: 0.8953 - val_accuracy: 0.4768 - 27ms/epoch - 4ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 0.7178 - accuracy: 0.5165 - val_loss: 0.8545 - val_accuracy: 0.4586 - 28ms/epoch - 4ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 0.7241 - accuracy: 0.5345 - val_loss: 0.8829 - val_accuracy: 0.4616 - 28ms/epoch - 4ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 0.6806 - accuracy: 0.5685 - val_loss: 0.8037 - val_accuracy: 0.4747 - 27ms/epoch - 4ms/step\n",
      "Epoch 7/60\n",
      "7/7 - 0s - loss: 0.6791 - accuracy: 0.5690 - val_loss: 0.8131 - val_accuracy: 0.4848 - 28ms/epoch - 4ms/step\n",
      "Epoch 8/60\n",
      "7/7 - 0s - loss: 0.6717 - accuracy: 0.5645 - val_loss: 0.8466 - val_accuracy: 0.4929 - 28ms/epoch - 4ms/step\n",
      "Epoch 9/60\n",
      "7/7 - 0s - loss: 0.6520 - accuracy: 0.6003 - val_loss: 0.9014 - val_accuracy: 0.4717 - 28ms/epoch - 4ms/step\n",
      "Epoch 10/60\n",
      "7/7 - 0s - loss: 0.6561 - accuracy: 0.6038 - val_loss: 0.9573 - val_accuracy: 0.4859 - 29ms/epoch - 4ms/step\n",
      "Epoch 11/60\n",
      "7/7 - 0s - loss: 0.6651 - accuracy: 0.5995 - val_loss: 0.9571 - val_accuracy: 0.4717 - 28ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_16 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 1.5310 - accuracy: 0.4952 - val_loss: 1.0617 - val_accuracy: 0.5283 - 521ms/epoch - 74ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 0.8074 - accuracy: 0.5063 - val_loss: 0.9667 - val_accuracy: 0.4636 - 26ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 0.7351 - accuracy: 0.5335 - val_loss: 0.7724 - val_accuracy: 0.5040 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 0.6970 - accuracy: 0.5383 - val_loss: 0.7539 - val_accuracy: 0.4909 - 27ms/epoch - 4ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 0.6919 - accuracy: 0.5393 - val_loss: 0.8189 - val_accuracy: 0.4990 - 29ms/epoch - 4ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 0.6908 - accuracy: 0.5632 - val_loss: 0.7868 - val_accuracy: 0.4899 - 29ms/epoch - 4ms/step\n",
      "Epoch 7/70\n",
      "7/7 - 0s - loss: 0.6734 - accuracy: 0.5916 - val_loss: 0.8413 - val_accuracy: 0.4848 - 27ms/epoch - 4ms/step\n",
      "Epoch 8/70\n",
      "7/7 - 0s - loss: 0.6670 - accuracy: 0.5807 - val_loss: 0.8552 - val_accuracy: 0.4929 - 26ms/epoch - 4ms/step\n",
      "Epoch 9/70\n",
      "7/7 - 0s - loss: 0.6699 - accuracy: 0.5807 - val_loss: 0.9416 - val_accuracy: 0.5162 - 27ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_17 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 1.7640 - accuracy: 0.5071 - val_loss: 0.7180 - val_accuracy: 0.4889 - 525ms/epoch - 75ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 0.7241 - accuracy: 0.5426 - val_loss: 0.7973 - val_accuracy: 0.4909 - 28ms/epoch - 4ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 0.7261 - accuracy: 0.5320 - val_loss: 0.8286 - val_accuracy: 0.4929 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 0.6922 - accuracy: 0.5541 - val_loss: 0.8655 - val_accuracy: 0.4899 - 27ms/epoch - 4ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 0.7319 - accuracy: 0.5543 - val_loss: 0.8412 - val_accuracy: 0.4949 - 29ms/epoch - 4ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 0.6801 - accuracy: 0.5657 - val_loss: 0.8230 - val_accuracy: 0.4838 - 29ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 6.0370 - accuracy: 0.5254 - val_loss: 7.8916 - val_accuracy: 0.4848 - 531ms/epoch - 76ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 7.2469 - accuracy: 0.5294 - val_loss: 8.0330 - val_accuracy: 0.4778 - 29ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 7.2588 - accuracy: 0.5292 - val_loss: 7.9506 - val_accuracy: 0.4838 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 7.2398 - accuracy: 0.5305 - val_loss: 7.9870 - val_accuracy: 0.4808 - 32ms/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 7.2473 - accuracy: 0.5299 - val_loss: 7.9795 - val_accuracy: 0.4818 - 37ms/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 7.2392 - accuracy: 0.5305 - val_loss: 7.9786 - val_accuracy: 0.4818 - 32ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_19 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 6.1626 - accuracy: 0.5190 - val_loss: 8.0934 - val_accuracy: 0.4747 - 524ms/epoch - 75ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 7.1906 - accuracy: 0.5338 - val_loss: 8.1625 - val_accuracy: 0.4697 - 27ms/epoch - 4ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 7.1829 - accuracy: 0.5340 - val_loss: 8.0813 - val_accuracy: 0.4758 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 7.2223 - accuracy: 0.5315 - val_loss: 8.0502 - val_accuracy: 0.4778 - 28ms/epoch - 4ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 7.1892 - accuracy: 0.5335 - val_loss: 8.0599 - val_accuracy: 0.4768 - 27ms/epoch - 4ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 7.2154 - accuracy: 0.5320 - val_loss: 8.0811 - val_accuracy: 0.4758 - 29ms/epoch - 4ms/step\n",
      "Epoch 7/60\n",
      "7/7 - 0s - loss: 7.2052 - accuracy: 0.5325 - val_loss: 8.0981 - val_accuracy: 0.4747 - 28ms/epoch - 4ms/step\n",
      "Epoch 8/60\n",
      "7/7 - 0s - loss: 7.1826 - accuracy: 0.5338 - val_loss: 8.0197 - val_accuracy: 0.4798 - 28ms/epoch - 4ms/step\n",
      "Epoch 9/60\n",
      "7/7 - 0s - loss: 7.1936 - accuracy: 0.5335 - val_loss: 8.0659 - val_accuracy: 0.4768 - 29ms/epoch - 4ms/step\n",
      "Epoch 10/60\n",
      "7/7 - 0s - loss: 7.1943 - accuracy: 0.5335 - val_loss: 8.0292 - val_accuracy: 0.4788 - 27ms/epoch - 4ms/step\n",
      "Epoch 11/60\n",
      "7/7 - 0s - loss: 7.1954 - accuracy: 0.5330 - val_loss: 8.0818 - val_accuracy: 0.4758 - 28ms/epoch - 4ms/step\n",
      "Epoch 12/60\n",
      "7/7 - 0s - loss: 7.2065 - accuracy: 0.5327 - val_loss: 8.0363 - val_accuracy: 0.4788 - 28ms/epoch - 4ms/step\n",
      "Epoch 13/60\n",
      "7/7 - 0s - loss: 7.2227 - accuracy: 0.5317 - val_loss: 8.0993 - val_accuracy: 0.4747 - 27ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_20 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 7.0452 - accuracy: 0.4607 - val_loss: 7.1625 - val_accuracy: 0.5303 - 690ms/epoch - 99ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 28ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 27ms/epoch - 4ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 28ms/epoch - 4ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 30ms/epoch - 4ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 28ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_21 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 6.0868 - accuracy: 0.5277 - val_loss: 8.1799 - val_accuracy: 0.4697 - 526ms/epoch - 75ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 27ms/epoch - 4ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 28ms/epoch - 4ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 28ms/epoch - 4ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 29ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_22 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 6.2743 - accuracy: 0.5061 - val_loss: 8.2440 - val_accuracy: 0.4646 - 522ms/epoch - 75ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 7.3103 - accuracy: 0.5256 - val_loss: 8.2192 - val_accuracy: 0.4667 - 28ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 7.3526 - accuracy: 0.5228 - val_loss: 8.1875 - val_accuracy: 0.4687 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 7.3639 - accuracy: 0.5221 - val_loss: 8.2024 - val_accuracy: 0.4677 - 29ms/epoch - 4ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 7.3404 - accuracy: 0.5236 - val_loss: 8.2024 - val_accuracy: 0.4677 - 28ms/epoch - 4ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 7.3366 - accuracy: 0.5239 - val_loss: 8.2024 - val_accuracy: 0.4677 - 26ms/epoch - 4ms/step\n",
      "Epoch 7/50\n",
      "7/7 - 0s - loss: 7.3366 - accuracy: 0.5239 - val_loss: 8.2024 - val_accuracy: 0.4677 - 27ms/epoch - 4ms/step\n",
      "Epoch 8/50\n",
      "7/7 - 0s - loss: 7.3366 - accuracy: 0.5239 - val_loss: 8.2024 - val_accuracy: 0.4677 - 27ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_23 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 7.1184 - accuracy: 0.4480 - val_loss: 7.1625 - val_accuracy: 0.5303 - 555ms/epoch - 79ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 8.1472 - accuracy: 0.4657 - val_loss: 7.1625 - val_accuracy: 0.5303 - 28ms/epoch - 4ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 8.0010 - accuracy: 0.4756 - val_loss: 7.1625 - val_accuracy: 0.5303 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 8.0102 - accuracy: 0.4749 - val_loss: 7.1625 - val_accuracy: 0.5303 - 29ms/epoch - 4ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 8.0747 - accuracy: 0.4706 - val_loss: 7.1625 - val_accuracy: 0.5303 - 29ms/epoch - 4ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 8.1057 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 27ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_24 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 6.6602 - accuracy: 0.4952 - val_loss: 7.2266 - val_accuracy: 0.5263 - 527ms/epoch - 75ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 7.1953 - accuracy: 0.5294 - val_loss: 7.1960 - val_accuracy: 0.5283 - 28ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 7.2558 - accuracy: 0.5261 - val_loss: 7.2337 - val_accuracy: 0.5263 - 29ms/epoch - 4ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 7.2810 - accuracy: 0.5274 - val_loss: 8.0726 - val_accuracy: 0.4758 - 28ms/epoch - 4ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 7.2442 - accuracy: 0.5302 - val_loss: 8.1349 - val_accuracy: 0.4717 - 29ms/epoch - 4ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 7.2404 - accuracy: 0.5305 - val_loss: 8.1194 - val_accuracy: 0.4727 - 27ms/epoch - 4ms/step\n",
      "Epoch 7/70\n",
      "7/7 - 0s - loss: 7.2283 - accuracy: 0.5310 - val_loss: 7.9932 - val_accuracy: 0.4808 - 27ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_25 (LSTM)              (None, 5, 50)             15600     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 5, 1)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15651 (61.14 KB)\n",
      "Trainable params: 15651 (61.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 6.3723 - accuracy: 0.5297 - val_loss: 8.1792 - val_accuracy: 0.4697 - 556ms/epoch - 79ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 7.2754 - accuracy: 0.5279 - val_loss: 8.2254 - val_accuracy: 0.4667 - 28ms/epoch - 4ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 7.2678 - accuracy: 0.5284 - val_loss: 8.1631 - val_accuracy: 0.4707 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 7.2943 - accuracy: 0.5266 - val_loss: 8.1629 - val_accuracy: 0.4707 - 28ms/epoch - 4ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 7.3123 - accuracy: 0.5254 - val_loss: 8.1787 - val_accuracy: 0.4697 - 26ms/epoch - 4ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 7.3312 - accuracy: 0.5241 - val_loss: 8.2098 - val_accuracy: 0.4677 - 28ms/epoch - 4ms/step\n",
      "Epoch 7/80\n",
      "7/7 - 0s - loss: 7.3302 - accuracy: 0.5241 - val_loss: 8.1636 - val_accuracy: 0.4707 - 28ms/epoch - 4ms/step\n",
      "Epoch 8/80\n",
      "7/7 - 0s - loss: 7.3336 - accuracy: 0.5239 - val_loss: 8.1636 - val_accuracy: 0.4707 - 28ms/epoch - 4ms/step\n",
      "Epoch 9/80\n",
      "7/7 - 0s - loss: 7.3257 - accuracy: 0.5244 - val_loss: 8.1636 - val_accuracy: 0.4707 - 26ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_26 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 1.3466 - accuracy: 0.5056 - val_loss: 0.8661 - val_accuracy: 0.5192 - 557ms/epoch - 43ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 0.7432 - accuracy: 0.5066 - val_loss: 0.7566 - val_accuracy: 0.4828 - 40ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 0.7364 - accuracy: 0.5269 - val_loss: 0.7333 - val_accuracy: 0.4838 - 39ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 0.7104 - accuracy: 0.5246 - val_loss: 0.7355 - val_accuracy: 0.4859 - 39ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 0.6866 - accuracy: 0.5662 - val_loss: 0.8266 - val_accuracy: 0.4808 - 38ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 0.6851 - accuracy: 0.5619 - val_loss: 0.9148 - val_accuracy: 0.4626 - 39ms/epoch - 3ms/step\n",
      "Epoch 7/50\n",
      "13/13 - 0s - loss: 0.6989 - accuracy: 0.5589 - val_loss: 0.7990 - val_accuracy: 0.4929 - 39ms/epoch - 3ms/step\n",
      "Epoch 8/50\n",
      "13/13 - 0s - loss: 0.7034 - accuracy: 0.5419 - val_loss: 0.7211 - val_accuracy: 0.5121 - 37ms/epoch - 3ms/step\n",
      "Epoch 9/50\n",
      "13/13 - 0s - loss: 0.6910 - accuracy: 0.5467 - val_loss: 0.8302 - val_accuracy: 0.4636 - 38ms/epoch - 3ms/step\n",
      "Epoch 10/50\n",
      "13/13 - 0s - loss: 0.6908 - accuracy: 0.5594 - val_loss: 0.8056 - val_accuracy: 0.5020 - 38ms/epoch - 3ms/step\n",
      "Epoch 11/50\n",
      "13/13 - 0s - loss: 0.6756 - accuracy: 0.5807 - val_loss: 0.7854 - val_accuracy: 0.4939 - 36ms/epoch - 3ms/step\n",
      "Epoch 12/50\n",
      "13/13 - 0s - loss: 0.6631 - accuracy: 0.5909 - val_loss: 0.8148 - val_accuracy: 0.4788 - 38ms/epoch - 3ms/step\n",
      "Epoch 13/50\n",
      "13/13 - 0s - loss: 0.6554 - accuracy: 0.6152 - val_loss: 1.0902 - val_accuracy: 0.5051 - 41ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_27 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 1.3442 - accuracy: 0.5254 - val_loss: 0.7586 - val_accuracy: 0.5091 - 561ms/epoch - 43ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 0.7724 - accuracy: 0.5251 - val_loss: 0.8601 - val_accuracy: 0.4970 - 39ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 0.7091 - accuracy: 0.5421 - val_loss: 0.7806 - val_accuracy: 0.5101 - 40ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 0.7084 - accuracy: 0.5525 - val_loss: 0.8908 - val_accuracy: 0.4788 - 40ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 0.6894 - accuracy: 0.5632 - val_loss: 1.2526 - val_accuracy: 0.4747 - 37ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 0.6802 - accuracy: 0.5883 - val_loss: 0.8251 - val_accuracy: 0.4960 - 40ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_28 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 1.3142 - accuracy: 0.5135 - val_loss: 0.7640 - val_accuracy: 0.5333 - 553ms/epoch - 43ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 0.7511 - accuracy: 0.5183 - val_loss: 0.8730 - val_accuracy: 0.4596 - 39ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 0.7137 - accuracy: 0.5650 - val_loss: 0.9296 - val_accuracy: 0.4707 - 40ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 0.7060 - accuracy: 0.5353 - val_loss: 0.8008 - val_accuracy: 0.4747 - 38ms/epoch - 3ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 0.6748 - accuracy: 0.5632 - val_loss: 0.8653 - val_accuracy: 0.4697 - 38ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 0.6669 - accuracy: 0.5888 - val_loss: 0.8916 - val_accuracy: 0.4859 - 38ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_29 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 1.2771 - accuracy: 0.5079 - val_loss: 0.8388 - val_accuracy: 0.5202 - 739ms/epoch - 57ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 0.7802 - accuracy: 0.5211 - val_loss: 0.7250 - val_accuracy: 0.5091 - 39ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 0.7203 - accuracy: 0.5338 - val_loss: 0.8588 - val_accuracy: 0.4475 - 38ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 0.6959 - accuracy: 0.5480 - val_loss: 0.7325 - val_accuracy: 0.5202 - 40ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 0.6692 - accuracy: 0.5711 - val_loss: 0.7664 - val_accuracy: 0.4818 - 40ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 0.6752 - accuracy: 0.5840 - val_loss: 0.7777 - val_accuracy: 0.4778 - 39ms/epoch - 3ms/step\n",
      "Epoch 7/80\n",
      "13/13 - 0s - loss: 0.6832 - accuracy: 0.5761 - val_loss: 0.7867 - val_accuracy: 0.5000 - 40ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_30 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 6.6273 - accuracy: 0.5327 - val_loss: 8.1799 - val_accuracy: 0.4697 - 563ms/epoch - 43ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 7.2269 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 40ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 7.2269 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 40ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 7.2269 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 39ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 7.2269 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 37ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 7.2269 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 38ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_31 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 7.4919 - accuracy: 0.4728 - val_loss: 7.1625 - val_accuracy: 0.5303 - 552ms/epoch - 42ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 8.1063 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 46ms/epoch - 4ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 7.9678 - accuracy: 0.4777 - val_loss: 7.1482 - val_accuracy: 0.5313 - 45ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 8.0009 - accuracy: 0.4756 - val_loss: 7.1174 - val_accuracy: 0.5333 - 40ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 8.0014 - accuracy: 0.4756 - val_loss: 7.1020 - val_accuracy: 0.5343 - 41ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 8.0094 - accuracy: 0.4751 - val_loss: 7.1176 - val_accuracy: 0.5333 - 39ms/epoch - 3ms/step\n",
      "Epoch 7/60\n",
      "13/13 - 0s - loss: 8.0055 - accuracy: 0.4754 - val_loss: 7.1020 - val_accuracy: 0.5343 - 40ms/epoch - 3ms/step\n",
      "Epoch 8/60\n",
      "13/13 - 0s - loss: 7.9938 - accuracy: 0.4761 - val_loss: 7.1020 - val_accuracy: 0.5343 - 40ms/epoch - 3ms/step\n",
      "Epoch 9/60\n",
      "13/13 - 0s - loss: 7.9977 - accuracy: 0.4759 - val_loss: 7.1020 - val_accuracy: 0.5343 - 40ms/epoch - 3ms/step\n",
      "Epoch 10/60\n",
      "13/13 - 0s - loss: 7.9977 - accuracy: 0.4759 - val_loss: 7.1020 - val_accuracy: 0.5343 - 37ms/epoch - 3ms/step\n",
      "Epoch 11/60\n",
      "13/13 - 0s - loss: 7.9977 - accuracy: 0.4759 - val_loss: 7.1020 - val_accuracy: 0.5343 - 39ms/epoch - 3ms/step\n",
      "Epoch 12/60\n",
      "13/13 - 0s - loss: 7.9977 - accuracy: 0.4759 - val_loss: 7.1020 - val_accuracy: 0.5343 - 39ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_32 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 7.6611 - accuracy: 0.4673 - val_loss: 7.1625 - val_accuracy: 0.5303 - 539ms/epoch - 41ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 39ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 39ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 39ms/epoch - 3ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 39ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 37ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_33 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 6.8754 - accuracy: 0.5195 - val_loss: 8.1470 - val_accuracy: 0.4717 - 533ms/epoch - 41ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 7.2699 - accuracy: 0.5277 - val_loss: 8.1944 - val_accuracy: 0.4687 - 39ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 7.2453 - accuracy: 0.5297 - val_loss: 8.1301 - val_accuracy: 0.4727 - 40ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 7.1867 - accuracy: 0.5335 - val_loss: 8.1762 - val_accuracy: 0.4697 - 39ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 7.1750 - accuracy: 0.5343 - val_loss: 8.1450 - val_accuracy: 0.4717 - 39ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 7.1750 - accuracy: 0.5343 - val_loss: 8.1294 - val_accuracy: 0.4727 - 38ms/epoch - 3ms/step\n",
      "Epoch 7/80\n",
      "13/13 - 0s - loss: 7.1750 - accuracy: 0.5343 - val_loss: 8.1294 - val_accuracy: 0.4727 - 37ms/epoch - 3ms/step\n",
      "Epoch 8/80\n",
      "13/13 - 0s - loss: 7.1750 - accuracy: 0.5343 - val_loss: 8.1294 - val_accuracy: 0.4727 - 39ms/epoch - 3ms/step\n",
      "Epoch 9/80\n",
      "13/13 - 0s - loss: 7.1750 - accuracy: 0.5343 - val_loss: 8.1294 - val_accuracy: 0.4727 - 39ms/epoch - 3ms/step\n",
      "Epoch 10/80\n",
      "13/13 - 0s - loss: 7.1750 - accuracy: 0.5343 - val_loss: 8.1294 - val_accuracy: 0.4727 - 40ms/epoch - 3ms/step\n",
      "Epoch 11/80\n",
      "13/13 - 0s - loss: 7.1750 - accuracy: 0.5343 - val_loss: 8.1294 - val_accuracy: 0.4727 - 39ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_34 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 6.8138 - accuracy: 0.5297 - val_loss: 8.1799 - val_accuracy: 0.4697 - 572ms/epoch - 44ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 7.2265 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 40ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 7.2265 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 39ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 7.2265 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 40ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 7.2265 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 38ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 7.2265 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 40ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_35 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 7.5026 - accuracy: 0.4759 - val_loss: 7.1625 - val_accuracy: 0.5303 - 531ms/epoch - 41ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 8.1712 - accuracy: 0.4642 - val_loss: 7.1625 - val_accuracy: 0.5303 - 39ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 8.1282 - accuracy: 0.4680 - val_loss: 7.1625 - val_accuracy: 0.5303 - 40ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 7.5002 - accuracy: 0.5109 - val_loss: 7.0708 - val_accuracy: 0.5364 - 39ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 7.4918 - accuracy: 0.5117 - val_loss: 7.0866 - val_accuracy: 0.5354 - 58ms/epoch - 4ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 7.5007 - accuracy: 0.5112 - val_loss: 7.0867 - val_accuracy: 0.5354 - 41ms/epoch - 3ms/step\n",
      "Epoch 7/60\n",
      "13/13 - 0s - loss: 7.5087 - accuracy: 0.5107 - val_loss: 7.0713 - val_accuracy: 0.5364 - 39ms/epoch - 3ms/step\n",
      "Epoch 8/60\n",
      "13/13 - 0s - loss: 7.5126 - accuracy: 0.5104 - val_loss: 7.0713 - val_accuracy: 0.5364 - 40ms/epoch - 3ms/step\n",
      "Epoch 9/60\n",
      "13/13 - 0s - loss: 7.5165 - accuracy: 0.5102 - val_loss: 7.0715 - val_accuracy: 0.5364 - 37ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_36 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 6.7467 - accuracy: 0.5282 - val_loss: 8.1487 - val_accuracy: 0.4717 - 540ms/epoch - 42ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 7.2096 - accuracy: 0.5322 - val_loss: 8.1084 - val_accuracy: 0.4737 - 40ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 7.2266 - accuracy: 0.5310 - val_loss: 8.1013 - val_accuracy: 0.4737 - 38ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 7.2081 - accuracy: 0.5325 - val_loss: 8.1636 - val_accuracy: 0.4707 - 40ms/epoch - 3ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 7.2478 - accuracy: 0.5299 - val_loss: 8.1636 - val_accuracy: 0.4707 - 40ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 7.2441 - accuracy: 0.5302 - val_loss: 8.1636 - val_accuracy: 0.4707 - 41ms/epoch - 3ms/step\n",
      "Epoch 7/70\n",
      "13/13 - 0s - loss: 7.2441 - accuracy: 0.5302 - val_loss: 8.1636 - val_accuracy: 0.4707 - 41ms/epoch - 3ms/step\n",
      "Epoch 8/70\n",
      "13/13 - 0s - loss: 7.2441 - accuracy: 0.5302 - val_loss: 8.1636 - val_accuracy: 0.4707 - 42ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_37 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 6.6555 - accuracy: 0.5353 - val_loss: 8.1799 - val_accuracy: 0.4697 - 541ms/epoch - 42ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 40ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 39ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 39ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 39ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 39ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_38 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 1.8168 - accuracy: 0.4865 - val_loss: 0.7726 - val_accuracy: 0.5000 - 549ms/epoch - 78ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 0.7540 - accuracy: 0.5175 - val_loss: 0.8634 - val_accuracy: 0.4859 - 30ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 0.7263 - accuracy: 0.5231 - val_loss: 0.8074 - val_accuracy: 0.4808 - 30ms/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 0.7192 - accuracy: 0.5178 - val_loss: 0.8567 - val_accuracy: 0.5040 - 31ms/epoch - 4ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 0.7054 - accuracy: 0.5467 - val_loss: 1.2247 - val_accuracy: 0.5071 - 31ms/epoch - 4ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 0.7211 - accuracy: 0.5551 - val_loss: 0.8668 - val_accuracy: 0.5121 - 31ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_39 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 2.2307 - accuracy: 0.4980 - val_loss: 0.8185 - val_accuracy: 0.5152 - 535ms/epoch - 76ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 0.7687 - accuracy: 0.5289 - val_loss: 0.9379 - val_accuracy: 0.4929 - 29ms/epoch - 4ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 0.7438 - accuracy: 0.5317 - val_loss: 0.7729 - val_accuracy: 0.5263 - 30ms/epoch - 4ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 0.7072 - accuracy: 0.5614 - val_loss: 0.8680 - val_accuracy: 0.4960 - 30ms/epoch - 4ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 0.6743 - accuracy: 0.5924 - val_loss: 0.8093 - val_accuracy: 0.5242 - 31ms/epoch - 4ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 0.6580 - accuracy: 0.5997 - val_loss: 0.8424 - val_accuracy: 0.4737 - 31ms/epoch - 4ms/step\n",
      "Epoch 7/60\n",
      "7/7 - 0s - loss: 0.6707 - accuracy: 0.5914 - val_loss: 0.8048 - val_accuracy: 0.5253 - 31ms/epoch - 4ms/step\n",
      "Epoch 8/60\n",
      "7/7 - 0s - loss: 0.6449 - accuracy: 0.6152 - val_loss: 0.9058 - val_accuracy: 0.5111 - 31ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_40 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 2.1406 - accuracy: 0.5069 - val_loss: 1.3251 - val_accuracy: 0.4677 - 772ms/epoch - 110ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 0.8474 - accuracy: 0.5327 - val_loss: 0.8141 - val_accuracy: 0.5182 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 0.7396 - accuracy: 0.5168 - val_loss: 0.9092 - val_accuracy: 0.4717 - 30ms/epoch - 4ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 0.7351 - accuracy: 0.5322 - val_loss: 0.7700 - val_accuracy: 0.5071 - 30ms/epoch - 4ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 0.7040 - accuracy: 0.5439 - val_loss: 1.0431 - val_accuracy: 0.5273 - 31ms/epoch - 4ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 0.7260 - accuracy: 0.5368 - val_loss: 0.7910 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 7/70\n",
      "7/7 - 0s - loss: 0.6899 - accuracy: 0.5728 - val_loss: 0.7452 - val_accuracy: 0.5040 - 29ms/epoch - 4ms/step\n",
      "Epoch 8/70\n",
      "7/7 - 0s - loss: 0.6662 - accuracy: 0.5962 - val_loss: 0.8178 - val_accuracy: 0.5374 - 31ms/epoch - 4ms/step\n",
      "Epoch 9/70\n",
      "7/7 - 0s - loss: 0.6675 - accuracy: 0.5926 - val_loss: 0.8239 - val_accuracy: 0.4859 - 31ms/epoch - 4ms/step\n",
      "Epoch 10/70\n",
      "7/7 - 0s - loss: 0.6555 - accuracy: 0.6183 - val_loss: 0.8488 - val_accuracy: 0.4980 - 28ms/epoch - 4ms/step\n",
      "Epoch 11/70\n",
      "7/7 - 0s - loss: 0.6305 - accuracy: 0.6307 - val_loss: 0.8624 - val_accuracy: 0.5091 - 32ms/epoch - 5ms/step\n",
      "Epoch 12/70\n",
      "7/7 - 0s - loss: 0.6491 - accuracy: 0.6223 - val_loss: 0.9142 - val_accuracy: 0.5172 - 31ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_41 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 1.7422 - accuracy: 0.4980 - val_loss: 0.7513 - val_accuracy: 0.4838 - 540ms/epoch - 77ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 0.7452 - accuracy: 0.5132 - val_loss: 0.8185 - val_accuracy: 0.4899 - 29ms/epoch - 4ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 0.7293 - accuracy: 0.5307 - val_loss: 0.7744 - val_accuracy: 0.4859 - 28ms/epoch - 4ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 0.7163 - accuracy: 0.5518 - val_loss: 0.9589 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 0.7790 - accuracy: 0.5332 - val_loss: 0.8361 - val_accuracy: 0.4576 - 30ms/epoch - 4ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 0.7061 - accuracy: 0.5485 - val_loss: 0.8975 - val_accuracy: 0.4879 - 32ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_42 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 6.1620 - accuracy: 0.5244 - val_loss: 8.1799 - val_accuracy: 0.4697 - 547ms/epoch - 78ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 29ms/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 32ms/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_43 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 5.9853 - accuracy: 0.5452 - val_loss: 8.1799 - val_accuracy: 0.4697 - 534ms/epoch - 76ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_44 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 7.0471 - accuracy: 0.4541 - val_loss: 7.2406 - val_accuracy: 0.5253 - 549ms/epoch - 78ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 7.8800 - accuracy: 0.4835 - val_loss: 7.3713 - val_accuracy: 0.5162 - 30ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 7.9771 - accuracy: 0.4772 - val_loss: 7.2061 - val_accuracy: 0.5273 - 29ms/epoch - 4ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 7.9640 - accuracy: 0.4782 - val_loss: 7.3479 - val_accuracy: 0.5182 - 30ms/epoch - 4ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 7.9579 - accuracy: 0.4784 - val_loss: 7.2751 - val_accuracy: 0.5232 - 31ms/epoch - 4ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 7.9953 - accuracy: 0.4761 - val_loss: 7.2504 - val_accuracy: 0.5253 - 31ms/epoch - 4ms/step\n",
      "Epoch 7/70\n",
      "7/7 - 0s - loss: 7.9851 - accuracy: 0.4769 - val_loss: 7.3428 - val_accuracy: 0.5192 - 32ms/epoch - 5ms/step\n",
      "Epoch 8/70\n",
      "7/7 - 0s - loss: 7.9684 - accuracy: 0.4779 - val_loss: 7.3426 - val_accuracy: 0.5192 - 32ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_45 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 7.0412 - accuracy: 0.4670 - val_loss: 7.1625 - val_accuracy: 0.5303 - 527ms/epoch - 75ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 30ms/epoch - 4ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 29ms/epoch - 4ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 33ms/epoch - 5ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 31ms/epoch - 4ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 32ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_46 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 6.1330 - accuracy: 0.5368 - val_loss: 8.1799 - val_accuracy: 0.4697 - 554ms/epoch - 79ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 7.2385 - accuracy: 0.5307 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 7.2308 - accuracy: 0.5312 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 7.2308 - accuracy: 0.5312 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 7.2308 - accuracy: 0.5312 - val_loss: 8.1799 - val_accuracy: 0.4697 - 32ms/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 7.2308 - accuracy: 0.5312 - val_loss: 8.1799 - val_accuracy: 0.4697 - 33ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_47 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 6.2024 - accuracy: 0.5218 - val_loss: 8.1799 - val_accuracy: 0.4697 - 539ms/epoch - 77ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 29ms/epoch - 4ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 29ms/epoch - 4ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 29ms/epoch - 4ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 29ms/epoch - 4ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 32ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_48 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 6.3396 - accuracy: 0.4987 - val_loss: 8.1799 - val_accuracy: 0.4697 - 533ms/epoch - 76ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 29ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 32ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_49 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 6.8437 - accuracy: 0.4739 - val_loss: 7.2116 - val_accuracy: 0.5273 - 544ms/epoch - 78ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 7.9256 - accuracy: 0.4807 - val_loss: 7.2121 - val_accuracy: 0.5273 - 30ms/epoch - 4ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 7.9470 - accuracy: 0.4797 - val_loss: 7.1967 - val_accuracy: 0.5283 - 29ms/epoch - 4ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 7.9032 - accuracy: 0.4827 - val_loss: 7.9904 - val_accuracy: 0.4808 - 32ms/epoch - 5ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 7.4975 - accuracy: 0.5127 - val_loss: 7.8700 - val_accuracy: 0.4889 - 31ms/epoch - 4ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 7.3408 - accuracy: 0.5231 - val_loss: 7.9541 - val_accuracy: 0.4828 - 30ms/epoch - 4ms/step\n",
      "Epoch 7/80\n",
      "7/7 - 0s - loss: 7.3451 - accuracy: 0.5228 - val_loss: 8.0575 - val_accuracy: 0.4768 - 31ms/epoch - 4ms/step\n",
      "Epoch 8/80\n",
      "7/7 - 0s - loss: 7.3419 - accuracy: 0.5231 - val_loss: 8.0579 - val_accuracy: 0.4768 - 40ms/epoch - 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_50 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 1.3952 - accuracy: 0.4934 - val_loss: 1.5315 - val_accuracy: 0.4495 - 541ms/epoch - 42ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 0.7345 - accuracy: 0.5398 - val_loss: 0.9460 - val_accuracy: 0.5061 - 41ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 0.7363 - accuracy: 0.5457 - val_loss: 0.8010 - val_accuracy: 0.4919 - 42ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 0.7180 - accuracy: 0.5239 - val_loss: 0.7782 - val_accuracy: 0.4990 - 42ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 0.6784 - accuracy: 0.5906 - val_loss: 0.9599 - val_accuracy: 0.5051 - 44ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 0.6887 - accuracy: 0.5706 - val_loss: 0.7798 - val_accuracy: 0.5141 - 42ms/epoch - 3ms/step\n",
      "Epoch 7/50\n",
      "13/13 - 0s - loss: 0.7067 - accuracy: 0.5766 - val_loss: 0.8591 - val_accuracy: 0.5020 - 43ms/epoch - 3ms/step\n",
      "Epoch 8/50\n",
      "13/13 - 0s - loss: 0.6972 - accuracy: 0.5858 - val_loss: 1.0070 - val_accuracy: 0.4475 - 44ms/epoch - 3ms/step\n",
      "Epoch 9/50\n",
      "13/13 - 0s - loss: 0.7090 - accuracy: 0.5754 - val_loss: 1.0157 - val_accuracy: 0.4949 - 45ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_51 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 1.3488 - accuracy: 0.4970 - val_loss: 1.3685 - val_accuracy: 0.5192 - 560ms/epoch - 43ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 0.8149 - accuracy: 0.5071 - val_loss: 0.7917 - val_accuracy: 0.4990 - 43ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 0.7264 - accuracy: 0.5464 - val_loss: 0.8069 - val_accuracy: 0.4747 - 45ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 0.7070 - accuracy: 0.5236 - val_loss: 0.8037 - val_accuracy: 0.4818 - 45ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 0.6796 - accuracy: 0.5668 - val_loss: 0.7325 - val_accuracy: 0.5283 - 45ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 0.6768 - accuracy: 0.5805 - val_loss: 0.8351 - val_accuracy: 0.4960 - 45ms/epoch - 3ms/step\n",
      "Epoch 7/60\n",
      "13/13 - 0s - loss: 0.6782 - accuracy: 0.5779 - val_loss: 0.7530 - val_accuracy: 0.5343 - 44ms/epoch - 3ms/step\n",
      "Epoch 8/60\n",
      "13/13 - 0s - loss: 0.6726 - accuracy: 0.5939 - val_loss: 1.0065 - val_accuracy: 0.4848 - 39ms/epoch - 3ms/step\n",
      "Epoch 9/60\n",
      "13/13 - 0s - loss: 0.6787 - accuracy: 0.5721 - val_loss: 0.8185 - val_accuracy: 0.5101 - 41ms/epoch - 3ms/step\n",
      "Epoch 10/60\n",
      "13/13 - 0s - loss: 0.7317 - accuracy: 0.5904 - val_loss: 1.4534 - val_accuracy: 0.4606 - 41ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_52 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 1.1569 - accuracy: 0.5074 - val_loss: 0.8086 - val_accuracy: 0.5545 - 536ms/epoch - 41ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 0.7695 - accuracy: 0.5157 - val_loss: 0.7433 - val_accuracy: 0.4970 - 42ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 0.7129 - accuracy: 0.5429 - val_loss: 0.7660 - val_accuracy: 0.4727 - 41ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 0.7062 - accuracy: 0.5503 - val_loss: 0.7563 - val_accuracy: 0.4646 - 44ms/epoch - 3ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 0.6849 - accuracy: 0.5820 - val_loss: 0.7670 - val_accuracy: 0.4657 - 42ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 0.6797 - accuracy: 0.5777 - val_loss: 1.0896 - val_accuracy: 0.5000 - 42ms/epoch - 3ms/step\n",
      "Epoch 7/70\n",
      "13/13 - 0s - loss: 0.6991 - accuracy: 0.5614 - val_loss: 0.7656 - val_accuracy: 0.4859 - 44ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_53 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 1.2551 - accuracy: 0.5135 - val_loss: 0.8557 - val_accuracy: 0.4859 - 850ms/epoch - 65ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 0.7758 - accuracy: 0.5254 - val_loss: 0.8221 - val_accuracy: 0.5192 - 41ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 0.7197 - accuracy: 0.5330 - val_loss: 0.9133 - val_accuracy: 0.4899 - 41ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 0.7200 - accuracy: 0.5325 - val_loss: 0.8070 - val_accuracy: 0.5152 - 42ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 0.6813 - accuracy: 0.5584 - val_loss: 0.8169 - val_accuracy: 0.5000 - 42ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 0.6733 - accuracy: 0.5789 - val_loss: 0.8850 - val_accuracy: 0.5232 - 40ms/epoch - 3ms/step\n",
      "Epoch 7/80\n",
      "13/13 - 0s - loss: 0.6675 - accuracy: 0.5876 - val_loss: 0.9771 - val_accuracy: 0.5182 - 41ms/epoch - 3ms/step\n",
      "Epoch 8/80\n",
      "13/13 - 0s - loss: 0.6948 - accuracy: 0.5629 - val_loss: 1.0380 - val_accuracy: 0.4828 - 42ms/epoch - 3ms/step\n",
      "Epoch 9/80\n",
      "13/13 - 0s - loss: 0.6889 - accuracy: 0.5698 - val_loss: 0.8609 - val_accuracy: 0.4626 - 41ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_54 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 6.7322 - accuracy: 0.5236 - val_loss: 8.1799 - val_accuracy: 0.4697 - 563ms/epoch - 43ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 45ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 46ms/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 45ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 44ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 44ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_55 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 7.7176 - accuracy: 0.4678 - val_loss: 7.1625 - val_accuracy: 0.5303 - 536ms/epoch - 41ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 42ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 41ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 41ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 42ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 41ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_56 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 6.7923 - accuracy: 0.5150 - val_loss: 7.4871 - val_accuracy: 0.5101 - 553ms/epoch - 43ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 7.9932 - accuracy: 0.4766 - val_loss: 7.3488 - val_accuracy: 0.5192 - 45ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 7.9525 - accuracy: 0.4794 - val_loss: 7.3332 - val_accuracy: 0.5202 - 45ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 7.9486 - accuracy: 0.4797 - val_loss: 7.3488 - val_accuracy: 0.5192 - 44ms/epoch - 3ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 7.9487 - accuracy: 0.4797 - val_loss: 7.3488 - val_accuracy: 0.5192 - 43ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 7.9487 - accuracy: 0.4797 - val_loss: 7.3488 - val_accuracy: 0.5192 - 43ms/epoch - 3ms/step\n",
      "Epoch 7/70\n",
      "13/13 - 0s - loss: 7.9487 - accuracy: 0.4797 - val_loss: 7.3488 - val_accuracy: 0.5192 - 42ms/epoch - 3ms/step\n",
      "Epoch 8/70\n",
      "13/13 - 0s - loss: 7.9487 - accuracy: 0.4797 - val_loss: 7.3488 - val_accuracy: 0.5192 - 41ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_57 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 7.4052 - accuracy: 0.4741 - val_loss: 7.4112 - val_accuracy: 0.5152 - 568ms/epoch - 44ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 7.9223 - accuracy: 0.4810 - val_loss: 7.4127 - val_accuracy: 0.5152 - 41ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 7.9031 - accuracy: 0.4822 - val_loss: 7.4287 - val_accuracy: 0.5141 - 43ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 7.8727 - accuracy: 0.4840 - val_loss: 7.3370 - val_accuracy: 0.5202 - 42ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 7.8633 - accuracy: 0.4850 - val_loss: 7.3379 - val_accuracy: 0.5202 - 41ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 7.8980 - accuracy: 0.4827 - val_loss: 7.2909 - val_accuracy: 0.5232 - 42ms/epoch - 3ms/step\n",
      "Epoch 7/80\n",
      "13/13 - 0s - loss: 7.8614 - accuracy: 0.4850 - val_loss: 7.4370 - val_accuracy: 0.5131 - 42ms/epoch - 3ms/step\n",
      "Epoch 8/80\n",
      "13/13 - 0s - loss: 7.8760 - accuracy: 0.4840 - val_loss: 7.4752 - val_accuracy: 0.5111 - 40ms/epoch - 3ms/step\n",
      "Epoch 9/80\n",
      "13/13 - 0s - loss: 7.9041 - accuracy: 0.4820 - val_loss: 7.4000 - val_accuracy: 0.5162 - 40ms/epoch - 3ms/step\n",
      "Epoch 10/80\n",
      "13/13 - 0s - loss: 7.8929 - accuracy: 0.4830 - val_loss: 7.3426 - val_accuracy: 0.5202 - 41ms/epoch - 3ms/step\n",
      "Epoch 11/80\n",
      "13/13 - 0s - loss: 7.9406 - accuracy: 0.4799 - val_loss: 7.3745 - val_accuracy: 0.5182 - 43ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_58 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 7.5464 - accuracy: 0.4685 - val_loss: 7.1625 - val_accuracy: 0.5303 - 559ms/epoch - 43ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 45ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 45ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 45ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 44ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 44ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_59 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 6.8294 - accuracy: 0.5201 - val_loss: 8.1799 - val_accuracy: 0.4697 - 554ms/epoch - 43ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 42ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 43ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 41ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 42ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 41ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_60 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 6.6538 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 538ms/epoch - 41ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 42ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 41ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 44ms/epoch - 3ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 43ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 47ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_61 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 7.4335 - accuracy: 0.4820 - val_loss: 7.2280 - val_accuracy: 0.5263 - 576ms/epoch - 44ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 7.9336 - accuracy: 0.4810 - val_loss: 7.1257 - val_accuracy: 0.5333 - 42ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 8.0567 - accuracy: 0.4739 - val_loss: 7.3472 - val_accuracy: 0.5192 - 42ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 8.0134 - accuracy: 0.4769 - val_loss: 7.4890 - val_accuracy: 0.5101 - 42ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 8.0220 - accuracy: 0.4764 - val_loss: 7.4120 - val_accuracy: 0.5152 - 42ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 7.9911 - accuracy: 0.4784 - val_loss: 7.3812 - val_accuracy: 0.5172 - 42ms/epoch - 3ms/step\n",
      "Epoch 7/80\n",
      "13/13 - 0s - loss: 7.9951 - accuracy: 0.4782 - val_loss: 7.3968 - val_accuracy: 0.5162 - 41ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_62 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 1.7850 - accuracy: 0.4962 - val_loss: 0.9406 - val_accuracy: 0.4636 - 533ms/epoch - 76ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 0.8137 - accuracy: 0.5109 - val_loss: 0.7974 - val_accuracy: 0.5061 - 32ms/epoch - 5ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 0.7563 - accuracy: 0.5178 - val_loss: 0.8134 - val_accuracy: 0.4879 - 32ms/epoch - 5ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 0.7135 - accuracy: 0.5294 - val_loss: 0.7634 - val_accuracy: 0.5192 - 34ms/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 0.6801 - accuracy: 0.5713 - val_loss: 0.7965 - val_accuracy: 0.4929 - 36ms/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 0.6717 - accuracy: 0.5642 - val_loss: 0.7677 - val_accuracy: 0.5091 - 36ms/epoch - 5ms/step\n",
      "Epoch 7/50\n",
      "7/7 - 0s - loss: 0.6600 - accuracy: 0.6076 - val_loss: 0.8042 - val_accuracy: 0.5141 - 36ms/epoch - 5ms/step\n",
      "Epoch 8/50\n",
      "7/7 - 0s - loss: 0.6446 - accuracy: 0.6094 - val_loss: 0.9322 - val_accuracy: 0.4707 - 35ms/epoch - 5ms/step\n",
      "Epoch 9/50\n",
      "7/7 - 0s - loss: 0.6609 - accuracy: 0.6033 - val_loss: 0.8495 - val_accuracy: 0.5020 - 37ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_63 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 1.7580 - accuracy: 0.4957 - val_loss: 0.9215 - val_accuracy: 0.5182 - 555ms/epoch - 79ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 0.8614 - accuracy: 0.4888 - val_loss: 1.4638 - val_accuracy: 0.5374 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 0.8024 - accuracy: 0.5150 - val_loss: 0.8588 - val_accuracy: 0.4606 - 32ms/epoch - 5ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 0.7246 - accuracy: 0.5345 - val_loss: 0.9111 - val_accuracy: 0.4758 - 35ms/epoch - 5ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 0.7045 - accuracy: 0.5548 - val_loss: 0.8137 - val_accuracy: 0.5030 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 0.6871 - accuracy: 0.5822 - val_loss: 0.8377 - val_accuracy: 0.4737 - 36ms/epoch - 5ms/step\n",
      "Epoch 7/60\n",
      "7/7 - 0s - loss: 0.6770 - accuracy: 0.5799 - val_loss: 0.7622 - val_accuracy: 0.4707 - 33ms/epoch - 5ms/step\n",
      "Epoch 8/60\n",
      "7/7 - 0s - loss: 0.6574 - accuracy: 0.6053 - val_loss: 0.7658 - val_accuracy: 0.5051 - 32ms/epoch - 5ms/step\n",
      "Epoch 9/60\n",
      "7/7 - 0s - loss: 0.6591 - accuracy: 0.6069 - val_loss: 0.8433 - val_accuracy: 0.4747 - 34ms/epoch - 5ms/step\n",
      "Epoch 10/60\n",
      "7/7 - 0s - loss: 0.6519 - accuracy: 0.6147 - val_loss: 0.9302 - val_accuracy: 0.4768 - 33ms/epoch - 5ms/step\n",
      "Epoch 11/60\n",
      "7/7 - 0s - loss: 0.6469 - accuracy: 0.6239 - val_loss: 0.8913 - val_accuracy: 0.4788 - 34ms/epoch - 5ms/step\n",
      "Epoch 12/60\n",
      "7/7 - 0s - loss: 0.6447 - accuracy: 0.6023 - val_loss: 0.9273 - val_accuracy: 0.4838 - 35ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_64 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 2.0845 - accuracy: 0.5109 - val_loss: 2.9626 - val_accuracy: 0.4727 - 551ms/epoch - 79ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 0.9435 - accuracy: 0.5188 - val_loss: 1.4948 - val_accuracy: 0.5394 - 30ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 0.8030 - accuracy: 0.5228 - val_loss: 0.8309 - val_accuracy: 0.5101 - 33ms/epoch - 5ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 0.7614 - accuracy: 0.5381 - val_loss: 0.8110 - val_accuracy: 0.5040 - 34ms/epoch - 5ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 0.6995 - accuracy: 0.5599 - val_loss: 0.7806 - val_accuracy: 0.5172 - 37ms/epoch - 5ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 0.6693 - accuracy: 0.5784 - val_loss: 0.8279 - val_accuracy: 0.4879 - 38ms/epoch - 5ms/step\n",
      "Epoch 7/70\n",
      "7/7 - 0s - loss: 0.6649 - accuracy: 0.5825 - val_loss: 0.7757 - val_accuracy: 0.5040 - 37ms/epoch - 5ms/step\n",
      "Epoch 8/70\n",
      "7/7 - 0s - loss: 0.6445 - accuracy: 0.6221 - val_loss: 0.8736 - val_accuracy: 0.4677 - 36ms/epoch - 5ms/step\n",
      "Epoch 9/70\n",
      "7/7 - 0s - loss: 0.6429 - accuracy: 0.6155 - val_loss: 0.8402 - val_accuracy: 0.5081 - 38ms/epoch - 5ms/step\n",
      "Epoch 10/70\n",
      "7/7 - 0s - loss: 0.6346 - accuracy: 0.6239 - val_loss: 0.8115 - val_accuracy: 0.5192 - 36ms/epoch - 5ms/step\n",
      "Epoch 11/70\n",
      "7/7 - 0s - loss: 0.6547 - accuracy: 0.6048 - val_loss: 0.8489 - val_accuracy: 0.5394 - 36ms/epoch - 5ms/step\n",
      "Epoch 12/70\n",
      "7/7 - 0s - loss: 0.6321 - accuracy: 0.6454 - val_loss: 1.0182 - val_accuracy: 0.5222 - 36ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_65 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 1.5996 - accuracy: 0.5137 - val_loss: 0.7873 - val_accuracy: 0.5101 - 530ms/epoch - 76ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 0.7616 - accuracy: 0.4980 - val_loss: 0.8769 - val_accuracy: 0.4970 - 32ms/epoch - 5ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 0.7575 - accuracy: 0.5277 - val_loss: 0.7860 - val_accuracy: 0.4788 - 33ms/epoch - 5ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 0.6967 - accuracy: 0.5571 - val_loss: 0.7546 - val_accuracy: 0.4859 - 36ms/epoch - 5ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 0.7075 - accuracy: 0.5505 - val_loss: 0.8892 - val_accuracy: 0.4778 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 0.7128 - accuracy: 0.5612 - val_loss: 0.9151 - val_accuracy: 0.4636 - 36ms/epoch - 5ms/step\n",
      "Epoch 7/80\n",
      "7/7 - 0s - loss: 0.6881 - accuracy: 0.5843 - val_loss: 0.8142 - val_accuracy: 0.4909 - 42ms/epoch - 6ms/step\n",
      "Epoch 8/80\n",
      "7/7 - 0s - loss: 0.6609 - accuracy: 0.6084 - val_loss: 0.8367 - val_accuracy: 0.4889 - 46ms/epoch - 7ms/step\n",
      "Epoch 9/80\n",
      "7/7 - 0s - loss: 0.6644 - accuracy: 0.6051 - val_loss: 0.7855 - val_accuracy: 0.4808 - 37ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_66 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 6.6877 - accuracy: 0.4909 - val_loss: 7.6686 - val_accuracy: 0.5010 - 548ms/epoch - 78ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 7.2443 - accuracy: 0.5294 - val_loss: 7.9077 - val_accuracy: 0.4869 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 7.2665 - accuracy: 0.5284 - val_loss: 7.8920 - val_accuracy: 0.4879 - 32ms/epoch - 5ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 7.2705 - accuracy: 0.5282 - val_loss: 7.8764 - val_accuracy: 0.4889 - 35ms/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 7.2744 - accuracy: 0.5279 - val_loss: 7.8764 - val_accuracy: 0.4889 - 34ms/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 7.2706 - accuracy: 0.5282 - val_loss: 7.8764 - val_accuracy: 0.4889 - 34ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_67 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 6.3105 - accuracy: 0.5327 - val_loss: 8.0669 - val_accuracy: 0.4768 - 534ms/epoch - 76ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 7.2060 - accuracy: 0.5327 - val_loss: 7.9568 - val_accuracy: 0.4838 - 34ms/epoch - 5ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 7.2066 - accuracy: 0.5325 - val_loss: 8.0193 - val_accuracy: 0.4798 - 33ms/epoch - 5ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 7.2010 - accuracy: 0.5330 - val_loss: 8.0879 - val_accuracy: 0.4747 - 37ms/epoch - 5ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 7.1992 - accuracy: 0.5332 - val_loss: 8.1464 - val_accuracy: 0.4717 - 36ms/epoch - 5ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 7.1877 - accuracy: 0.5340 - val_loss: 8.1322 - val_accuracy: 0.4727 - 35ms/epoch - 5ms/step\n",
      "Epoch 7/60\n",
      "7/7 - 0s - loss: 7.1880 - accuracy: 0.5338 - val_loss: 8.0729 - val_accuracy: 0.4758 - 36ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_68 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 6.3023 - accuracy: 0.5081 - val_loss: 8.1799 - val_accuracy: 0.4697 - 925ms/epoch - 132ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 32ms/epoch - 5ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 34ms/epoch - 5ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 32ms/epoch - 5ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 34ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_69 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 7.1025 - accuracy: 0.4594 - val_loss: 7.1625 - val_accuracy: 0.5303 - 561ms/epoch - 80ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 33ms/epoch - 5ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 33ms/epoch - 5ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 5ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 36ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_70 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 6.2806 - accuracy: 0.5183 - val_loss: 8.1799 - val_accuracy: 0.4697 - 530ms/epoch - 76ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 32ms/epoch - 5ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_71 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 6.2011 - accuracy: 0.5246 - val_loss: 8.1799 - val_accuracy: 0.4697 - 561ms/epoch - 80ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 32ms/epoch - 5ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 5ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 34ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_72 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 6.2217 - accuracy: 0.5147 - val_loss: 8.1799 - val_accuracy: 0.4697 - 538ms/epoch - 77ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 30ms/epoch - 4ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 34ms/epoch - 5ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 34ms/epoch - 5ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 34ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_73 (LSTM)              (None, 5, 70)             27440     \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 5, 1)              71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27511 (107.46 KB)\n",
      "Trainable params: 27511 (107.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 7.1769 - accuracy: 0.4647 - val_loss: 7.1937 - val_accuracy: 0.5283 - 535ms/epoch - 76ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 8.0852 - accuracy: 0.4698 - val_loss: 7.1937 - val_accuracy: 0.5283 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 8.0892 - accuracy: 0.4695 - val_loss: 7.1937 - val_accuracy: 0.5283 - 31ms/epoch - 4ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 8.0853 - accuracy: 0.4698 - val_loss: 7.1781 - val_accuracy: 0.5293 - 35ms/epoch - 5ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 8.0892 - accuracy: 0.4695 - val_loss: 7.1781 - val_accuracy: 0.5293 - 34ms/epoch - 5ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 8.0892 - accuracy: 0.4695 - val_loss: 7.1781 - val_accuracy: 0.5293 - 34ms/epoch - 5ms/step\n",
      "Epoch 7/80\n",
      "7/7 - 0s - loss: 8.0892 - accuracy: 0.4695 - val_loss: 7.1781 - val_accuracy: 0.5293 - 31ms/epoch - 4ms/step\n",
      "Epoch 8/80\n",
      "7/7 - 0s - loss: 8.0892 - accuracy: 0.4695 - val_loss: 7.1781 - val_accuracy: 0.5293 - 34ms/epoch - 5ms/step\n",
      "Epoch 9/80\n",
      "7/7 - 0s - loss: 8.0931 - accuracy: 0.4693 - val_loss: 7.1781 - val_accuracy: 0.5293 - 34ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_74 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 1.5281 - accuracy: 0.5127 - val_loss: 0.7952 - val_accuracy: 0.5253 - 549ms/epoch - 42ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 0.7508 - accuracy: 0.5330 - val_loss: 0.7444 - val_accuracy: 0.5172 - 44ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 0.7280 - accuracy: 0.5266 - val_loss: 0.7857 - val_accuracy: 0.4838 - 43ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 0.7099 - accuracy: 0.5612 - val_loss: 0.9078 - val_accuracy: 0.5030 - 44ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 0.7103 - accuracy: 0.5470 - val_loss: 0.8149 - val_accuracy: 0.5192 - 43ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 0.6706 - accuracy: 0.5924 - val_loss: 0.7974 - val_accuracy: 0.5020 - 44ms/epoch - 3ms/step\n",
      "Epoch 7/50\n",
      "13/13 - 0s - loss: 0.6886 - accuracy: 0.5812 - val_loss: 0.8584 - val_accuracy: 0.4949 - 46ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_75 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 1.5296 - accuracy: 0.5096 - val_loss: 0.9075 - val_accuracy: 0.4899 - 553ms/epoch - 43ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 0.7770 - accuracy: 0.5231 - val_loss: 0.7667 - val_accuracy: 0.5020 - 44ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 0.7124 - accuracy: 0.5162 - val_loss: 0.7411 - val_accuracy: 0.4828 - 45ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 0.7011 - accuracy: 0.5528 - val_loss: 0.8377 - val_accuracy: 0.4717 - 45ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 0.7045 - accuracy: 0.5470 - val_loss: 0.7815 - val_accuracy: 0.4586 - 46ms/epoch - 4ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 0.6891 - accuracy: 0.5731 - val_loss: 0.9065 - val_accuracy: 0.4747 - 47ms/epoch - 4ms/step\n",
      "Epoch 7/60\n",
      "13/13 - 0s - loss: 0.6992 - accuracy: 0.5670 - val_loss: 0.9259 - val_accuracy: 0.4737 - 44ms/epoch - 3ms/step\n",
      "Epoch 8/60\n",
      "13/13 - 0s - loss: 0.6965 - accuracy: 0.5647 - val_loss: 0.8904 - val_accuracy: 0.4828 - 59ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_76 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 2.4559 - accuracy: 0.5058 - val_loss: 0.8563 - val_accuracy: 0.4687 - 545ms/epoch - 42ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 0.8368 - accuracy: 0.5041 - val_loss: 0.8967 - val_accuracy: 0.5303 - 43ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 0.7945 - accuracy: 0.5157 - val_loss: 0.7713 - val_accuracy: 0.5091 - 45ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 0.7031 - accuracy: 0.5353 - val_loss: 0.8372 - val_accuracy: 0.5101 - 46ms/epoch - 4ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 0.6829 - accuracy: 0.5736 - val_loss: 0.8172 - val_accuracy: 0.5000 - 45ms/epoch - 3ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 0.6697 - accuracy: 0.5878 - val_loss: 0.9019 - val_accuracy: 0.5232 - 43ms/epoch - 3ms/step\n",
      "Epoch 7/70\n",
      "13/13 - 0s - loss: 0.6937 - accuracy: 0.5728 - val_loss: 0.7900 - val_accuracy: 0.4939 - 44ms/epoch - 3ms/step\n",
      "Epoch 8/70\n",
      "13/13 - 0s - loss: 0.6831 - accuracy: 0.5518 - val_loss: 0.7740 - val_accuracy: 0.5020 - 42ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_77 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 1.2668 - accuracy: 0.5348 - val_loss: 0.9634 - val_accuracy: 0.5020 - 537ms/epoch - 41ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 0.7910 - accuracy: 0.5119 - val_loss: 0.9212 - val_accuracy: 0.5535 - 44ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 0.7234 - accuracy: 0.5411 - val_loss: 0.7933 - val_accuracy: 0.4929 - 44ms/epoch - 3ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 0.6864 - accuracy: 0.5599 - val_loss: 0.7726 - val_accuracy: 0.5010 - 42ms/epoch - 3ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 0.6770 - accuracy: 0.5685 - val_loss: 0.7998 - val_accuracy: 0.4939 - 44ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 0.6671 - accuracy: 0.5774 - val_loss: 1.2510 - val_accuracy: 0.4566 - 44ms/epoch - 3ms/step\n",
      "Epoch 7/80\n",
      "13/13 - 0s - loss: 0.6982 - accuracy: 0.5698 - val_loss: 0.8302 - val_accuracy: 0.4869 - 45ms/epoch - 3ms/step\n",
      "Epoch 8/80\n",
      "13/13 - 0s - loss: 0.6837 - accuracy: 0.5970 - val_loss: 0.9697 - val_accuracy: 0.4990 - 41ms/epoch - 3ms/step\n",
      "Epoch 9/80\n",
      "13/13 - 0s - loss: 0.6884 - accuracy: 0.5878 - val_loss: 1.0054 - val_accuracy: 0.4768 - 44ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_78 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 6.7712 - accuracy: 0.5244 - val_loss: 8.1799 - val_accuracy: 0.4697 - 561ms/epoch - 43ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 45ms/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 44ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 45ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 45ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 44ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_79 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 7.6983 - accuracy: 0.4556 - val_loss: 7.1625 - val_accuracy: 0.5303 - 539ms/epoch - 41ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 44ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 43ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 44ms/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 44ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 45ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_80 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 6.6726 - accuracy: 0.5345 - val_loss: 8.1799 - val_accuracy: 0.4697 - 564ms/epoch - 43ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 44ms/epoch - 3ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 45ms/epoch - 3ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 47ms/epoch - 4ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 46ms/epoch - 4ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 46ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_81 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 6.7324 - accuracy: 0.5231 - val_loss: 8.1799 - val_accuracy: 0.4697 - 558ms/epoch - 43ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 44ms/epoch - 3ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 58ms/epoch - 4ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 50ms/epoch - 4ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 45ms/epoch - 3ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 46ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_82 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "13/13 - 1s - loss: 6.6538 - accuracy: 0.5312 - val_loss: 8.1799 - val_accuracy: 0.4697 - 553ms/epoch - 43ms/step\n",
      "Epoch 2/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 46ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 48ms/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 47ms/epoch - 4ms/step\n",
      "Epoch 5/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 48ms/epoch - 4ms/step\n",
      "Epoch 6/50\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 50ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_83 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "13/13 - 1s - loss: 6.6761 - accuracy: 0.5320 - val_loss: 8.1799 - val_accuracy: 0.4697 - 569ms/epoch - 44ms/step\n",
      "Epoch 2/60\n",
      "13/13 - 0s - loss: 7.2270 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 47ms/epoch - 4ms/step\n",
      "Epoch 3/60\n",
      "13/13 - 0s - loss: 7.2270 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 47ms/epoch - 4ms/step\n",
      "Epoch 4/60\n",
      "13/13 - 0s - loss: 7.2270 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 47ms/epoch - 4ms/step\n",
      "Epoch 5/60\n",
      "13/13 - 0s - loss: 7.2270 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 43ms/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "13/13 - 0s - loss: 7.2270 - accuracy: 0.5315 - val_loss: 8.1799 - val_accuracy: 0.4697 - 46ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_84 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "13/13 - 1s - loss: 6.7125 - accuracy: 0.5272 - val_loss: 8.1799 - val_accuracy: 0.4697 - 572ms/epoch - 44ms/step\n",
      "Epoch 2/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 47ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 48ms/epoch - 4ms/step\n",
      "Epoch 4/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 48ms/epoch - 4ms/step\n",
      "Epoch 5/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 48ms/epoch - 4ms/step\n",
      "Epoch 6/70\n",
      "13/13 - 0s - loss: 7.2231 - accuracy: 0.5317 - val_loss: 8.1799 - val_accuracy: 0.4697 - 48ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_85 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "13/13 - 1s - loss: 7.6058 - accuracy: 0.4632 - val_loss: 7.3168 - val_accuracy: 0.5212 - 1s/epoch - 87ms/step\n",
      "Epoch 2/80\n",
      "13/13 - 0s - loss: 8.0708 - accuracy: 0.4711 - val_loss: 7.3798 - val_accuracy: 0.5172 - 48ms/epoch - 4ms/step\n",
      "Epoch 3/80\n",
      "13/13 - 0s - loss: 8.0285 - accuracy: 0.4736 - val_loss: 7.3648 - val_accuracy: 0.5182 - 47ms/epoch - 4ms/step\n",
      "Epoch 4/80\n",
      "13/13 - 0s - loss: 8.0055 - accuracy: 0.4754 - val_loss: 7.3501 - val_accuracy: 0.5192 - 47ms/epoch - 4ms/step\n",
      "Epoch 5/80\n",
      "13/13 - 0s - loss: 7.9906 - accuracy: 0.4764 - val_loss: 7.3347 - val_accuracy: 0.5202 - 47ms/epoch - 4ms/step\n",
      "Epoch 6/80\n",
      "13/13 - 0s - loss: 7.9833 - accuracy: 0.4769 - val_loss: 7.4443 - val_accuracy: 0.5131 - 47ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_86 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 2.2443 - accuracy: 0.5170 - val_loss: 0.8694 - val_accuracy: 0.4616 - 565ms/epoch - 81ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 0.8415 - accuracy: 0.5165 - val_loss: 0.8519 - val_accuracy: 0.4798 - 34ms/epoch - 5ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 0.7552 - accuracy: 0.5107 - val_loss: 0.8680 - val_accuracy: 0.4828 - 33ms/epoch - 5ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 0.7123 - accuracy: 0.5383 - val_loss: 0.8183 - val_accuracy: 0.4859 - 36ms/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 0.7086 - accuracy: 0.5589 - val_loss: 0.8937 - val_accuracy: 0.4727 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 0.6765 - accuracy: 0.5901 - val_loss: 0.9491 - val_accuracy: 0.5152 - 37ms/epoch - 5ms/step\n",
      "Epoch 7/50\n",
      "7/7 - 0s - loss: 0.6621 - accuracy: 0.5843 - val_loss: 0.9407 - val_accuracy: 0.4727 - 35ms/epoch - 5ms/step\n",
      "Epoch 8/50\n",
      "7/7 - 0s - loss: 0.6412 - accuracy: 0.6279 - val_loss: 0.9595 - val_accuracy: 0.5061 - 36ms/epoch - 5ms/step\n",
      "Epoch 9/50\n",
      "7/7 - 0s - loss: 0.6625 - accuracy: 0.6033 - val_loss: 1.1944 - val_accuracy: 0.4747 - 35ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_87 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 1.6456 - accuracy: 0.4916 - val_loss: 0.8106 - val_accuracy: 0.4818 - 546ms/epoch - 78ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 0.8086 - accuracy: 0.5180 - val_loss: 1.0486 - val_accuracy: 0.5222 - 33ms/epoch - 5ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 0.7862 - accuracy: 0.5086 - val_loss: 0.8351 - val_accuracy: 0.4939 - 33ms/epoch - 5ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 0.7018 - accuracy: 0.5543 - val_loss: 0.9403 - val_accuracy: 0.4697 - 36ms/epoch - 5ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 0.6878 - accuracy: 0.5739 - val_loss: 0.8136 - val_accuracy: 0.4697 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 0.6593 - accuracy: 0.5906 - val_loss: 0.8096 - val_accuracy: 0.4798 - 38ms/epoch - 5ms/step\n",
      "Epoch 7/60\n",
      "7/7 - 0s - loss: 0.6378 - accuracy: 0.6203 - val_loss: 0.8255 - val_accuracy: 0.4869 - 37ms/epoch - 5ms/step\n",
      "Epoch 8/60\n",
      "7/7 - 0s - loss: 0.6245 - accuracy: 0.6266 - val_loss: 0.9566 - val_accuracy: 0.4657 - 35ms/epoch - 5ms/step\n",
      "Epoch 9/60\n",
      "7/7 - 0s - loss: 0.6327 - accuracy: 0.6170 - val_loss: 0.9124 - val_accuracy: 0.4909 - 36ms/epoch - 5ms/step\n",
      "Epoch 10/60\n",
      "7/7 - 0s - loss: 0.6101 - accuracy: 0.6289 - val_loss: 1.0150 - val_accuracy: 0.4889 - 37ms/epoch - 5ms/step\n",
      "Epoch 11/60\n",
      "7/7 - 0s - loss: 0.6225 - accuracy: 0.6378 - val_loss: 0.9308 - val_accuracy: 0.4697 - 37ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_88 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 2.0908 - accuracy: 0.5081 - val_loss: 0.8821 - val_accuracy: 0.4899 - 559ms/epoch - 80ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 0.7881 - accuracy: 0.5251 - val_loss: 1.0374 - val_accuracy: 0.4808 - 32ms/epoch - 5ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 0.7900 - accuracy: 0.5256 - val_loss: 0.8780 - val_accuracy: 0.4909 - 34ms/epoch - 5ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 0.7211 - accuracy: 0.5401 - val_loss: 0.8052 - val_accuracy: 0.4838 - 37ms/epoch - 5ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 0.6995 - accuracy: 0.5520 - val_loss: 0.7899 - val_accuracy: 0.5071 - 38ms/epoch - 5ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 0.6888 - accuracy: 0.5721 - val_loss: 0.8251 - val_accuracy: 0.4970 - 36ms/epoch - 5ms/step\n",
      "Epoch 7/70\n",
      "7/7 - 0s - loss: 0.6682 - accuracy: 0.5919 - val_loss: 0.7858 - val_accuracy: 0.5010 - 37ms/epoch - 5ms/step\n",
      "Epoch 8/70\n",
      "7/7 - 0s - loss: 0.6540 - accuracy: 0.6005 - val_loss: 0.8320 - val_accuracy: 0.5071 - 37ms/epoch - 5ms/step\n",
      "Epoch 9/70\n",
      "7/7 - 0s - loss: 0.6478 - accuracy: 0.6216 - val_loss: 1.0231 - val_accuracy: 0.4687 - 35ms/epoch - 5ms/step\n",
      "Epoch 10/70\n",
      "7/7 - 0s - loss: 0.6435 - accuracy: 0.6228 - val_loss: 0.9871 - val_accuracy: 0.4828 - 36ms/epoch - 5ms/step\n",
      "Epoch 11/70\n",
      "7/7 - 0s - loss: 0.6334 - accuracy: 0.6322 - val_loss: 1.3151 - val_accuracy: 0.5081 - 36ms/epoch - 5ms/step\n",
      "Epoch 12/70\n",
      "7/7 - 0s - loss: 0.6451 - accuracy: 0.6414 - val_loss: 1.0361 - val_accuracy: 0.5242 - 38ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_89 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 1.9426 - accuracy: 0.5010 - val_loss: 1.5241 - val_accuracy: 0.5162 - 544ms/epoch - 78ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 0.8156 - accuracy: 0.5234 - val_loss: 1.0775 - val_accuracy: 0.5040 - 34ms/epoch - 5ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 0.7736 - accuracy: 0.5360 - val_loss: 0.9196 - val_accuracy: 0.4909 - 34ms/epoch - 5ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 0.7002 - accuracy: 0.5523 - val_loss: 0.8798 - val_accuracy: 0.5010 - 37ms/epoch - 5ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 0.7009 - accuracy: 0.5642 - val_loss: 0.9439 - val_accuracy: 0.5010 - 37ms/epoch - 5ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 0.6733 - accuracy: 0.5942 - val_loss: 0.9106 - val_accuracy: 0.4778 - 36ms/epoch - 5ms/step\n",
      "Epoch 7/80\n",
      "7/7 - 0s - loss: 0.6785 - accuracy: 0.6058 - val_loss: 1.0341 - val_accuracy: 0.4788 - 37ms/epoch - 5ms/step\n",
      "Epoch 8/80\n",
      "7/7 - 0s - loss: 0.6960 - accuracy: 0.5944 - val_loss: 0.9830 - val_accuracy: 0.4778 - 36ms/epoch - 5ms/step\n",
      "Epoch 9/80\n",
      "7/7 - 0s - loss: 0.6643 - accuracy: 0.6137 - val_loss: 1.0157 - val_accuracy: 0.4606 - 37ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_90 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "7/7 - 1s - loss: 6.7495 - accuracy: 0.4772 - val_loss: 7.4791 - val_accuracy: 0.5111 - 538ms/epoch - 77ms/step\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 7.8989 - accuracy: 0.4838 - val_loss: 7.4836 - val_accuracy: 0.5111 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 7.7160 - accuracy: 0.4962 - val_loss: 7.7935 - val_accuracy: 0.4919 - 32ms/epoch - 5ms/step\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 7.6144 - accuracy: 0.5033 - val_loss: 8.0474 - val_accuracy: 0.4758 - 35ms/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 7.6363 - accuracy: 0.5020 - val_loss: 7.9103 - val_accuracy: 0.4848 - 36ms/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 7.6173 - accuracy: 0.5033 - val_loss: 8.0198 - val_accuracy: 0.4778 - 36ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_91 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "7/7 - 1s - loss: 6.2322 - accuracy: 0.5249 - val_loss: 8.0108 - val_accuracy: 0.4798 - 546ms/epoch - 78ms/step\n",
      "Epoch 2/60\n",
      "7/7 - 0s - loss: 7.3837 - accuracy: 0.5206 - val_loss: 7.9339 - val_accuracy: 0.4848 - 34ms/epoch - 5ms/step\n",
      "Epoch 3/60\n",
      "7/7 - 0s - loss: 7.4778 - accuracy: 0.5145 - val_loss: 7.9341 - val_accuracy: 0.4848 - 33ms/epoch - 5ms/step\n",
      "Epoch 4/60\n",
      "7/7 - 0s - loss: 7.3852 - accuracy: 0.5206 - val_loss: 8.1572 - val_accuracy: 0.4707 - 37ms/epoch - 5ms/step\n",
      "Epoch 5/60\n",
      "7/7 - 0s - loss: 7.3178 - accuracy: 0.5251 - val_loss: 8.1898 - val_accuracy: 0.4687 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/60\n",
      "7/7 - 0s - loss: 7.2937 - accuracy: 0.5266 - val_loss: 8.1751 - val_accuracy: 0.4697 - 38ms/epoch - 5ms/step\n",
      "Epoch 7/60\n",
      "7/7 - 0s - loss: 7.2754 - accuracy: 0.5279 - val_loss: 8.1907 - val_accuracy: 0.4687 - 39ms/epoch - 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_92 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/70\n",
      "7/7 - 1s - loss: 6.1711 - accuracy: 0.5211 - val_loss: 8.1799 - val_accuracy: 0.4697 - 551ms/epoch - 79ms/step\n",
      "Epoch 2/70\n",
      "7/7 - 0s - loss: 7.2298 - accuracy: 0.5310 - val_loss: 8.1799 - val_accuracy: 0.4697 - 31ms/epoch - 4ms/step\n",
      "Epoch 3/70\n",
      "7/7 - 0s - loss: 7.2186 - accuracy: 0.5320 - val_loss: 8.1799 - val_accuracy: 0.4697 - 33ms/epoch - 5ms/step\n",
      "Epoch 4/70\n",
      "7/7 - 0s - loss: 7.2438 - accuracy: 0.5302 - val_loss: 8.1799 - val_accuracy: 0.4697 - 36ms/epoch - 5ms/step\n",
      "Epoch 5/70\n",
      "7/7 - 0s - loss: 7.2001 - accuracy: 0.5330 - val_loss: 8.1799 - val_accuracy: 0.4697 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/70\n",
      "7/7 - 0s - loss: 7.2553 - accuracy: 0.5289 - val_loss: 8.1799 - val_accuracy: 0.4697 - 36ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_93 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/80\n",
      "7/7 - 1s - loss: 7.0406 - accuracy: 0.4632 - val_loss: 7.1625 - val_accuracy: 0.5303 - 563ms/epoch - 80ms/step\n",
      "Epoch 2/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 32ms/epoch - 5ms/step\n",
      "Epoch 3/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 32ms/epoch - 5ms/step\n",
      "Epoch 4/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 5ms/step\n",
      "Epoch 5/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 35ms/epoch - 5ms/step\n",
      "Epoch 6/80\n",
      "7/7 - 0s - loss: 8.1084 - accuracy: 0.4683 - val_loss: 7.1625 - val_accuracy: 0.5303 - 36ms/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_94 (LSTM)              (None, 5, 80)             34560     \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 5, 1)              81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34641 (135.32 KB)\n",
      "Trainable params: 34641 (135.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1546\u001b[0m, in \u001b[0;36mOperation.get_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m c_api_util\u001b[38;5;241m.\u001b[39mtf_buffer() \u001b[38;5;28;01mas\u001b[39;00m buf:   \u001b[38;5;66;03m# pytype: disable=wrong-arg-count\u001b[39;00m\n\u001b[0;32m-> 1546\u001b[0m   \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_OperationGetAttrValueProto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1547\u001b[0m   data \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(buf)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'gradient_tape/sequential_95/lstm_94/while/Placeholder_22' has no attr named '_read_only_resource_inputs'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     46\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 47\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m val_performance \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Check if this configuration is the best so far\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:905\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m    910\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[1;32m    911\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:598\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 598\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/l4/q_79lrcx3ps_z7hltvr9nl4c0000gn/T/__autograph_generated_filexo2yzds3.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:460\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/keras/src/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1380\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m   1381\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m     )\n\u001b[1;32m   1383\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1384\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1386\u001b[0m     outputs,\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1388\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1389\u001b[0m )\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:1681\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1677\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   1680\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1681\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3271\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3269\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   3270\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4069\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4067\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   4068\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/keras/src/engine/training.py:1373\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1373\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/keras/src/engine/training.py:1154\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m-> 1154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:543\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, var_list, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    523\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \n\u001b[1;32m    525\u001b[0m \u001b[38;5;124;03m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m      None\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:276\u001b[0m, in \u001b[0;36m_BaseOptimizer.compute_gradients\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(var_list):\n\u001b[1;32m    274\u001b[0m             var_list \u001b[38;5;241m=\u001b[39m var_list()\n\u001b[0;32m--> 276\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1062\u001b[0m           output_gradients))\n\u001b[1;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:148\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    146\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/ops/while_v2.py:422\u001b[0m, in \u001b[0;36m_WhileGrad\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m    419\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m counter \u001b[38;5;241m<\u001b[39m forward_loop_iters\n\u001b[1;32m    421\u001b[0m grad_cond_name \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39munique_grad_fn_name(op\u001b[38;5;241m.\u001b[39mget_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcond\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 422\u001b[0m cond_grad_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_cond_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWhileCondFuncGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_cond_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m _check_num_inputs_outputs(cond_grad_graph, body_grad_graph, \u001b[38;5;28mlen\u001b[39m(loop_vars))\n\u001b[1;32m    428\u001b[0m outputs \u001b[38;5;241m=\u001b[39m _build_while_op(\n\u001b[1;32m    429\u001b[0m     loop_vars,\n\u001b[1;32m    430\u001b[0m     cond_grad_graph,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m while_op\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    435\u001b[0m     num_original_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(body_grad_graph\u001b[38;5;241m.\u001b[39moutputs))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:987\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    985\u001b[0m   deps_control_manager \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mNullContextmanager()\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default(), deps_control_manager \u001b[38;5;28;01mas\u001b[39;00m deps_ctx:\n\u001b[1;32m    988\u001b[0m   current_scope \u001b[38;5;241m=\u001b[39m variable_scope\u001b[38;5;241m.\u001b[39mget_variable_scope()\n\u001b[1;32m    989\u001b[0m   default_use_resource \u001b[38;5;241m=\u001b[39m current_scope\u001b[38;5;241m.\u001b[39muse_resource\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/framework/auto_control_deps.py:459\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    456\u001b[0m resource_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# Check for any resource inputs. If we find any, we update control_inputs\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# and last_write_to_resource.\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inp, resource_type \u001b[38;5;129;01min\u001b[39;00m _get_resource_inputs(op):\n\u001b[1;32m    460\u001b[0m   is_read \u001b[38;5;241m=\u001b[39m resource_type \u001b[38;5;241m==\u001b[39m ResourceType\u001b[38;5;241m.\u001b[39mREAD_ONLY\n\u001b[1;32m    461\u001b[0m   input_id \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mtensor_id(inp)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/framework/auto_control_deps.py:608\u001b[0m, in \u001b[0;36m_get_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_resource_inputs\u001b[39m(op):\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m   reads, writes \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_read_write_resource_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m   saturated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    610\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saturated:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/framework/auto_control_deps_utils.py:105\u001b[0m, in \u001b[0;36mget_read_write_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (reads, writes)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m   read_only_input_indices \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREAD_ONLY_RESOURCE_INPUTS_ATTR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   writes\u001b[38;5;241m.\u001b[39mupdate(t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m op\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mresource)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Financial_Trend_Forecasting/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1546\u001b[0m, in \u001b[0;36mOperation.get_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1545\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m c_api_util\u001b[38;5;241m.\u001b[39mtf_buffer() \u001b[38;5;28;01mas\u001b[39;00m buf:   \u001b[38;5;66;03m# pytype: disable=wrong-arg-count\u001b[39;00m\n\u001b[0;32m-> 1546\u001b[0m     \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_OperationGetAttrValueProto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1547\u001b[0m     data \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(buf)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1549\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# Model hyperparameters\n",
    "num_layers = [1, 2, 3] # 2, 2, 2, 2\n",
    "layer_sizes = [50, 60, 70, 80] # 50, 60, 80, 70\n",
    "batch_sizes = [64, 128] # 128, 64, 64, 128\n",
    "learning_rates = [0.1, 1.0, 2.0] # 0.1, 1, 2, 0.1\n",
    "epochs = [50, 60, 70, 80] # 50, 60, 50, 80\n",
    "activ_func = 'relu'\n",
    "\n",
    "best_model = None\n",
    "best_performance = 0.0\n",
    "best_weights = 0.0\n",
    "best_hyperparameters = {}\n",
    "count = 0\n",
    "\n",
    "# Iterate over hyperparameter combinations\n",
    "for num_layer in num_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for batch_size in batch_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                for epoch in epochs:\n",
    "                    count += 1\n",
    "                    # Build the LSTM model\n",
    "                    model = Sequential()\n",
    "                    \n",
    "                    # Add the specified number of LSTM layers\n",
    "                    for _ in range(num_layer):\n",
    "                        model.add(LSTM(layer_size, activation=activ_func, input_shape=(5, X_train.shape[-1]), return_sequences=True))\n",
    "                    \n",
    "                    # Flatten if there are multiple LSTM layers\n",
    "                    if num_layer > 1:\n",
    "                        model.add(Flatten())\n",
    "                    \n",
    "                    # Output layer\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "                    \n",
    "                    # Compile the model\n",
    "                    optimizer = Adam(learning_rate=learning_rate)\n",
    "                    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    # Print model summary\n",
    "                    print(model.summary())\n",
    "                    \n",
    "                    # Train the model\n",
    "                    es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "                    history = model.fit(X_train, y_train, epochs=epoch, batch_size=batch_size, validation_split=0.2, callbacks=[es], verbose=2)\n",
    "                    \n",
    "                    val_performance = history.history['val_accuracy'][-1]\n",
    "                    \n",
    "                    # Check if this configuration is the best so far\n",
    "                    if val_performance > best_performance:\n",
    "                        best_performance = val_performance\n",
    "                        best_model = model\n",
    "                        best_hyperparameters = {\n",
    "                            'num_layer': num_layer,\n",
    "                            'layer_size': layer_size,\n",
    "                            'batch_size': batch_size,\n",
    "                            'learning_rate': learning_rate,\n",
    "                            'epochs': epoch\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aea0d838-fff0-47ee-b965-c4fb3b0691f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_35 (LSTM)              (None, 5, 60)             21120     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 5, 1)              61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21181 (82.74 KB)\n",
      "Trainable params: 21181 (82.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Best Model Accuracy: 0.5363636016845703\n",
      "Best Model Hyperparameters: {'num_layer': 1, 'layer_size': 60, 'batch_size': 64, 'learning_rate': 2.0, 'epochs': 60}\n",
      "Number of Model Iterations: 93\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()\n",
    "print(f'Best Model Accuracy: {best_performance}')\n",
    "print(f'Best Model Hyperparameters: {best_hyperparameters}')\n",
    "print(f'Number of Model Iterations: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d8cf633-9996-4ac9-90b7-f5a9635c2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 7.0749 - accuracy: 0.5378\n",
      "Model Accuracy 0.5378\n"
     ]
    }
   ],
   "source": [
    "model_acc = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f'Model Accuracy {model_acc[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ccc20-854d-4725-871e-5393f469d671",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### b. Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c661f-a99e-4580-bc05-1ae3824b1a6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5 Ideas of model variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24058139-5324-4c70-b9fb-1d08f92f6fb0",
   "metadata": {},
   "source": [
    "GRU, Bidirectional, Transformers, data time dimention, feature selection, overfitting, hyperperameter selection, sentimental analysis, \n",
    "merging features\n",
    " - Regularization such as dropout layer\n",
    " - decay rate\n",
    " - weight initialisation\n",
    " - momentum\n",
    " - layer normalization\n",
    " - Feed-forward layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
